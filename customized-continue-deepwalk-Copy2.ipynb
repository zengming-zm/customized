{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from multiprocessing import cpu_count\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from numpy import genfromtxt\n",
    "import itertools\n",
    "\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'blogcatalog'\n",
    "\n",
    "embedding_size = 128\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 0.1\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.x\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.y\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.tx\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.ty\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.graph\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    f = \"/hdd2/graph_embedding/dataset/blogcatalog/trans.{}.{}\".format(DATASET, NAMES[i])\n",
    "    print(f)\n",
    "    OBJECTS.append(cPickle.load(open(f)))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031, 10312)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_iter(iter):\n",
    "    \"\"\"an auxiliary function used for computing the number of iterations given the argument iter.\n",
    "    iter can either be an int or a float.\n",
    "    \"\"\"\n",
    "    if iter >= 1:\n",
    "        return iter\n",
    "    return 1 if random.random() < iter else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, x.shape[1]], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, y.shape[1]], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [None, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [None], name = 'ind')\n",
    "    \n",
    "    path_sym = tf.placeholder(tf.int32, shape = [batch_size, path_size+1], name = 'path')\n",
    "    path_id_sym = tf.placeholder(tf.int32, shape = [batch_size, ], name = 'path_id')\n",
    "    w_path2pair_sym = tf.placeholder(tf.float32, shape = [batch_size, None], name = 'w_path2pair_sym')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list = ['/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0']\n",
    "dataset = genfromtxt(file_list[0], delimiter=' ')\n",
    "\n",
    "def get_num_vacabulary(dataset):\n",
    "    count = 0\n",
    "    for d in dataset:\n",
    "        count = max(count, max(d))\n",
    "    return int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_sampled = 64\n",
    "# num_ver = max(graph.keys()) + 1\n",
    "# vocabulary_size = num_ver\n",
    "vocabulary_size = get_num_vacabulary(dataset) + 1\n",
    "n_hidden = 32\n",
    "n_steps = path_size+1 #path length\n",
    "n_input = 128 # embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10312"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "\n",
    "cgraph = tf.Graph()\n",
    "\n",
    "with cgraph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym = create_placeholders()\n",
    "\n",
    "    # word embedding\n",
    "    tf.random_normal_initializer(seed = 1)\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "#     softmax_weights = tf.Variable(\n",
    "#         tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_weights = tf.Variable(\n",
    "          tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "   \n",
    "    l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "                                   inputs = l_emd_f, labels = gy_sym, \n",
    "                                   num_sampled = 5, \n",
    "                                   num_classes = vocabulary_size))    \n",
    "    g_optimizer = tf.train.AdamOptimizer(0.025).minimize(g_loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_textfiles(file_list):\n",
    "    count = -1\n",
    "    num_path = 0\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                num_path += 1\n",
    "                for l in line.split():\n",
    "                    count = max(count, int(l))\n",
    "    return count, num_path\n",
    "\n",
    "def get_word_pair(line):\n",
    "    g = []\n",
    "    list_path = []\n",
    "    path = [int(t) for t in line.split()]\n",
    "    list_path.append(path)\n",
    "    for l in range(len(path)):\n",
    "        for m in range(l - window_size, l + window_size + 1):\n",
    "            if m < 0 or m >= len(path) or m == l: continue\n",
    "            g.append([path[l], path[m]])\n",
    "            gy.append(1.0)\n",
    "    return g, list_path\n",
    "\n",
    "def gen_graph_from_path_collection(data):\n",
    "    g = []\n",
    "    list_path = []\n",
    "    \n",
    "    for line in data:\n",
    "        for l in xrange(len(line)):\n",
    "            for m in xrange(1 - window_size, l + window_size + 1):\n",
    "                if (m < 0 or m >= len(line) or m == l): \n",
    "                    continue\n",
    "                g.append([int(line[l]), int(line[m])])\n",
    "    return np.array(g, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "representation_size = 128\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unsupervised word2vec\n",
    "init_iter_deepwalk = 10\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "tot = 0\n",
    "batch_path = 10000\n",
    "with tf.Session(graph = cgraph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "\n",
    "    for i in xrange(init_iter_deepwalk):\n",
    "        for batch_id in xrange(len(dataset) / batch_path + 1):\n",
    "            start_idx = batch_path * batch_id\n",
    "            end_idx = min(len(dataset), batch_path * (batch_id + 1))\n",
    "            data_batch = dataset[start_idx: end_idx, :]\n",
    "            gx = gen_graph_from_path_collection(data_batch)\n",
    "            feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "            _, l  = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "            \n",
    "            print('iter: %d/%d, loss: %.4f' %(batch_id, len(dataset) / batch_path + 1, l))\n",
    "    #             feed_dict={g_sym: np.array(range(0,v+1)), gy_sym: np.array(range(0,v+1)).reshape(v+1,1)}\n",
    "    #             res_l_emd_f = session.run(l_emd_f, feed_dict=feed_dict)\n",
    "#                 final_embeddings = normalized_embeddings.eval()\n",
    "#                 embedding_filename_tmp = '/hdd2/graph_embedding/customized/blog_embeddings.%d.txt' %(tot)\n",
    "    #             np.savetxt(embedding_filename_tmp, res_l_emd_f)\n",
    "        tmp_embeddings = normalized_embeddings.eval()\n",
    "        embedding_filename_tmp = '/hdd2/graph_embedding/customized/blog_embeddings.iter%d.txt' %(i)\n",
    "        np.savetxt(embedding_filename_tmp, tmp_embeddings)\n",
    "        \n",
    "    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(embedding_filename, final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0539942"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0/83, loss: 1.0540\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Skipgram(Word2Vec):\n",
    "    \"\"\"A subclass to allow more customization of the Word2Vec internals.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_counts=None, **kwargs):\n",
    "\n",
    "        self.vocabulary_counts = None\n",
    "\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "        kwargs[\"workers\"] = kwargs.get(\"workers\", cpu_count())\n",
    "        kwargs[\"size\"] = kwargs.get(\"size\", 128)\n",
    "        kwargs[\"sentences\"] = kwargs.get(\"sentences\", None)\n",
    "        kwargs[\"window\"] = kwargs.get(\"window\", 10)\n",
    "        kwargs[\"sg\"] = 1\n",
    "        kwargs[\"hs\"] = 1\n",
    "\n",
    "        if vocabulary_counts != None:\n",
    "            self.vocabulary_counts = vocabulary_counts\n",
    "\n",
    "        super(Skipgram, self).__init__(**kwargs)\n",
    "        \n",
    "class WalksCorpus(object):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "    def __iter__(self):\n",
    "        for file in self.file_list:\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "walks_corpus = WalksCorpus(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824960\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in walks_corpus:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = Skipgram(sentences=walks_corpus, vocabulary_counts=vocabulary_size, size=128,\n",
    "#                  window=10, min_count=0, trim_rule=None, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "    embeddings = np.loadtxt(embeddings_file)\n",
    "    \n",
    "    ## for original deepwalk\n",
    "    #model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    \n",
    "    ## for external word2vec lib\n",
    "#     model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "    features_matrix = embeddings\n",
    "\n",
    "    # original code\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "\n",
    "    # use other word2vec lib\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "#     rand_list = range(len(model.vocab)-1)\n",
    "# #     random.shuffle(rand_list)\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in rand_list])\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix, random_state = 1))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Results, using embeddings of dimensionality', 128)\n",
      "-------------------\n",
      "('Train percent:', 0.1)\n",
      "('Shuffle #1:   ', {'micro': 0.33136457774027767, 'macro': 0.13864051144299749})\n",
      "('Average score:', {'micro': 0.33136457774027767, 'macro': 0.13864051144299749})\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings_iter1030000.txt'\n",
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "embedding_filename_other_lib = '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\n",
    "\n",
    "scoring(embedding_filename, matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "model = KeyedVectors.load_word2vec_format(embedding_filename_original, binary=False)\n",
    "                                          \n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embeddings = np.loadtxt(embedding_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 5, 1, 4]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random\n",
    "skshuffle([1,2,3,4,5], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seeded_vector(seed_string, vector_size):\n",
    "    \"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\n",
    "    # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "    once = np.random.RandomState(hash(seed_string) & 0xffffffff)\n",
    "    return (once.rand(vector_size) - 0.5) / vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.95574675e-04,  -3.94128153e-04,  -2.05971695e-03,\n",
       "        -2.45944998e-03,  -3.06828540e-03,   1.92283277e-03,\n",
       "         2.03048989e-03,  -3.41190604e-03,   2.90534158e-03,\n",
       "        -1.09142098e-03,   1.64901254e-04,  -1.78094929e-03,\n",
       "        -3.15096133e-03,  -2.76827583e-03,  -3.57192140e-05,\n",
       "         4.72280366e-05,  -2.08880128e-03,   2.57089862e-03,\n",
       "        -2.64681217e-03,  -5.08174197e-04,   3.75565917e-04,\n",
       "         1.63093191e-03,  -1.25181891e-03,   1.02226609e-03,\n",
       "        -2.39152526e-03,   2.50722041e-03,   2.40178287e-03,\n",
       "        -7.74683212e-05,  -2.01244911e-03,   3.67147635e-03,\n",
       "         1.79926162e-03,   3.77559747e-03,   1.52344183e-03,\n",
       "         3.42383257e-03,   9.45710198e-04,   2.16900250e-03,\n",
       "        -2.05456782e-03,  -3.46826397e-03,   1.86096827e-03,\n",
       "         1.70899574e-04,   9.43227422e-04,  -1.39875542e-03,\n",
       "        -3.68329350e-03,   3.75635902e-03,  -2.42840464e-03,\n",
       "        -3.75283464e-03,   3.51557492e-03,   1.85909036e-04,\n",
       "        -2.26960811e-03,  -3.02782387e-03,  -2.19190946e-03,\n",
       "         2.80045993e-03,   1.25567055e-03,  -1.64917176e-03,\n",
       "        -3.52057184e-03,   1.60727043e-03,   1.02602397e-04,\n",
       "         1.63254152e-03,   1.77075627e-03,   9.39519393e-04,\n",
       "        -1.79438739e-04,   3.61062545e-03,  -1.75022301e-03,\n",
       "         3.11756315e-03,  -5.11314827e-04,  -2.91414463e-03,\n",
       "        -2.58582906e-03,  -2.18634045e-03,   9.56227351e-05,\n",
       "        -9.88668566e-04,   6.46895432e-05,   2.78061851e-03,\n",
       "        -1.64144336e-03,  -9.44797431e-04,  -1.79603212e-03,\n",
       "        -9.41184743e-04,   2.17828528e-03,   3.00570426e-03,\n",
       "        -6.73176611e-04,   2.87406555e-03,   3.32925673e-03,\n",
       "         2.60145438e-04,  -3.22884829e-03,   9.80284775e-04,\n",
       "        -1.37886600e-03,   1.04532133e-03,  -1.49073851e-03,\n",
       "         4.78396851e-04,   3.80935365e-03,  -1.44289475e-04,\n",
       "        -1.76272589e-03,   1.06624167e-03,   3.04035795e-05,\n",
       "         1.19503293e-03,   1.59014810e-03,   2.47943443e-03,\n",
       "        -3.57011896e-03,  -9.54921479e-04,   7.41190638e-04,\n",
       "        -2.41738443e-03,  -8.42971460e-04,   3.72244965e-03,\n",
       "         2.04731790e-04,  -3.17696813e-03,  -1.89878916e-03,\n",
       "         5.29873729e-04,  -3.07059507e-03,   3.47938001e-03,\n",
       "        -3.33357414e-03,   3.08492661e-03,  -1.27674147e-03,\n",
       "        -2.50183520e-04,   5.14773231e-06,   2.06913994e-03,\n",
       "         1.98786954e-03,  -1.11061548e-05,   3.72874786e-03,\n",
       "        -1.85286414e-03,  -9.81214217e-05,  -1.11147342e-03,\n",
       "        -2.34516723e-03,  -1.93087485e-03,  -2.08790638e-03,\n",
       "        -2.23813742e-03,  -1.46143410e-03,   1.14113292e-03,\n",
       "         9.36538601e-04,  -2.25696154e-03])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeded_vector('4059' + str(1), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sio.loadmat(matfile)\n",
    "A = mat['network']\n",
    "labels_matrix = mat['group']\n",
    "labels_count = labels_matrix.shape[1]\n",
    "mlb = MultiLabelBinarizer(range(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MultiLabelBinarizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fd4e0751c0ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MultiLabelBinarizer' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-13 21:07:11,853 : INFO : collecting all words and their counts\n",
      "2018-03-13 21:07:11,855 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-13 21:07:11,857 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-03-13 21:07:11,858 : INFO : Loading a fresh vocabulary\n",
      "2018-03-13 21:07:11,860 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-03-13 21:07:11,863 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-03-13 21:07:11,865 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-03-13 21:07:11,868 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-03-13 21:07:11,870 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-03-13 21:07:11,872 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-03-13 21:07:11,874 : INFO : resetting layer weights\n",
      "2018-03-13 21:07:11,876 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-03-13 21:07:11,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,883 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,892 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,900 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 183 effective words/s\n",
      "2018-03-13 21:07:11,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,907 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,916 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,917 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 25 effective words/s\n",
      "2018-03-13 21:07:11,918 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 5\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00497301], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8\n",
      "Words processed: 17000K     Vocab size: 4399K  \n",
      "Vocab size (unigrams + bigrams): 2419827\n",
      "Words in train file: 17005206\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('/hdd2/graph_embedding/tmp/text8', '/hdd2/graph_embedding/tmp/text8-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8-phrases\n",
      "Vocab size: 98331\n",
      "Words in train file: 15857306\n",
      "Alpha: 0.000002  Progress: 100.04%  Words/thread/sec: 467.25k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('/hdd2/graph_embedding/tmp/text8-phrases', '/hdd2/graph_embedding/tmp/text8.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0\n",
      "Vocab size: 10313\n",
      "Words in train file: 33823360\n",
      "Alpha: 0.000002  Progress: 100.00%  Words/thread/sec: 409.50k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0',\n",
    "                  '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin', \n",
    "                  size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03300432, -0.07565996, -0.16463131, -0.10145742,  0.00168844,\n",
       "       -0.09007662, -0.03259023, -0.22714335, -0.00282227, -0.05083675,\n",
       "       -0.07032426, -0.15832779, -0.13998848,  0.07574239,  0.01739285,\n",
       "        0.05170207, -0.01704509, -0.04309488,  0.13984987, -0.07098495,\n",
       "       -0.08131668, -0.06108802,  0.03238546,  0.03199002, -0.14047897,\n",
       "        0.1192169 , -0.11983237,  0.17141342, -0.04238614, -0.07682881,\n",
       "       -0.09226497, -0.08641661, -0.05119943,  0.04880232,  0.05202007,\n",
       "        0.11911941,  0.02494194,  0.0460702 , -0.02544389,  0.02227429,\n",
       "       -0.02918422,  0.08963519, -0.01449389,  0.13147458,  0.04316   ,\n",
       "        0.05062373,  0.01030556,  0.14064927,  0.05300813,  0.09054667,\n",
       "        0.16458763,  0.00496632,  0.09853401, -0.09157445,  0.03466847,\n",
       "       -0.192109  , -0.04286476,  0.0337339 ,  0.09414463,  0.02883872,\n",
       "       -0.12935385,  0.05580147,  0.04444493,  0.12305645, -0.06843153,\n",
       "       -0.05650309, -0.06930542, -0.0933516 , -0.08945961,  0.0355445 ,\n",
       "        0.05217047, -0.09187157,  0.0095493 , -0.16591462,  0.02874937,\n",
       "       -0.0271637 , -0.1071742 , -0.07731676,  0.17057821, -0.08997511,\n",
       "       -0.15251309,  0.0335523 ,  0.08237864,  0.05653743,  0.05472478,\n",
       "       -0.16204691,  0.09632239, -0.0988512 , -0.12040844, -0.13067709,\n",
       "       -0.03478862, -0.07479616,  0.07388658,  0.08031294, -0.12074082,\n",
       "       -0.04398282, -0.04163745,  0.09059986,  0.1189674 ,  0.05980834,\n",
       "       -0.03435113, -0.12926504,  0.06568849, -0.04446449, -0.093913  ,\n",
       "       -0.02966127, -0.00469951, -0.15404062,  0.07520716, -0.02498432,\n",
       "        0.12759562, -0.08096215, -0.07254944,  0.04480873, -0.19941515,\n",
       "       -0.0785991 ,  0.00944981, -0.00052774,  0.031948  ,  0.05943104,\n",
       "       -0.03682529,  0.02468612, -0.00828248,  0.08574598, -0.08077673,\n",
       "        0.09135576,  0.03673941,  0.01044743])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
