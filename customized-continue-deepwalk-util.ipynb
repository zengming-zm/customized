{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from multiprocessing import cpu_count\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from numpy import genfromtxt\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "def scoring(emb_filename, matfile, mode = None, use_feature = False):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    if (mode == 'deepwalk-reproduce' or mode == 'semi' or mode == 'icml'):\n",
    "        # 1. Load Embeddings\n",
    "        embeddings = np.loadtxt(embeddings_file)\n",
    "    elif (mode == 'deepwalk-original'):\n",
    "        ## for original deepwalk\n",
    "        model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    elif (mode == 'deepwalk-word2veclib'):\n",
    "        ## for external word2vec lib\n",
    "        model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "\n",
    "    # 2. Load labels\n",
    "    if (mode == 'icml'):\n",
    "        NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "        OBJECTS = []\n",
    "        print(DATASET)\n",
    "        for i in range(len(NAMES)):\n",
    "            f = file_path + 'trans.{}.{}'.format(DATASET, NAMES[i])\n",
    "            OBJECTS.append(cPickle.load(open(f)))\n",
    "        x, y, tx, ty, graph = tuple(OBJECTS)\n",
    "        labels_matrix = np.concatenate([y, ty])\n",
    "        labels_count = labels_matrix.shape[1]\n",
    "        mlb = MultiLabelBinarizer(range(labels_count))\n",
    "    else:\n",
    "        mat = sio.loadmat(matfile)\n",
    "        A = mat['network']\n",
    "        graph = sparse2graph(A)\n",
    "        labels_matrix = mat['group']\n",
    "        labels_count = labels_matrix.shape[1]\n",
    "        mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    if (mode == 'deepwalk-reproduce' or mode == 'semi' or mode == 'icml'):\n",
    "        # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "        features_matrix = embeddings\n",
    "    elif (mode == 'deepwalk-original'):\n",
    "        # original code\n",
    "        features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "    elif (mode == 'deepwalk-word2veclib'):\n",
    "        # use other word2vec lib\n",
    "        features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "        rand_list = range(len(model.vocab)-1)\n",
    "        features_matrix = np.asarray([model[str(node)] for node in rand_list])\n",
    "        \n",
    "    \n",
    "    if (mode == 'icml'):\n",
    "        all_results = defaultdict(list)\n",
    "        if (use_feature):\n",
    "            features_matrix = np.concatenate([x.toarray(), tx.toarray()])\n",
    "            X, y = features_matrix, labels_matrix\n",
    "        else:\n",
    "            X, y = features_matrix, labels_matrix\n",
    "        \n",
    "        trn_x_id = range(0, x.shape[0])\n",
    "        trn_y = y\n",
    "        tst_x_id = range(x.shape[0], x.shape[0] + tx.shape[0])\n",
    "        tst_y = tx\n",
    "            \n",
    "        X_train = X[trn_x_id, :]\n",
    "        y_train = y[trn_x_id]\n",
    "\n",
    "        X_test = X[tst_x_id, :]\n",
    "        y_test = y[tst_x_id]\n",
    "\n",
    "        clf = TopKRanker(LogisticRegression())\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # find out how many labels should be predicted\n",
    "        top_k_list = [len(l) for l in y_test]\n",
    "        preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "        results = {}\n",
    "        averages = [\"micro\", \"macro\"]\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "        all_results['icml'].append(results)\n",
    "    elif (mode == 'semi'):\n",
    "        # read from trn and tst file\n",
    "        ## Read the data, the trn_x is the position in the embedding matrix\n",
    "        all_results = defaultdict(list)\n",
    "        X, y = features_matrix, labels_matrix\n",
    "        \n",
    "\n",
    "        with open(data_splited_filename, 'rb') as handle:\n",
    "            unserialized_data = pickle.load(handle)\n",
    "        trn_x_id, trn_y, tst_x_id, tst_y = (unserialized_data['trn_x'], unserialized_data['trn_y'],\\\n",
    "                                            unserialized_data['tst_x'], unserialized_data['tst_y'])\n",
    "            \n",
    "        X_train = X[trn_x_id, :]\n",
    "        y_train_ = y[trn_x_id]\n",
    "\n",
    "        y_train = [[] for x in range(y_train_.shape[0])]\n",
    "        \n",
    "        cy =  y_train_.tocoo()\n",
    "        for i, j in zip(cy.row, cy.col):\n",
    "            y_train[i].append(j)\n",
    "\n",
    "        assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "        X_test = X[tst_x_id, :]\n",
    "        y_test_ = y[tst_x_id]\n",
    "\n",
    "        y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "        cy =  y_test_.tocoo()\n",
    "        for i, j in zip(cy.row, cy.col):\n",
    "            y_test[i].append(j)\n",
    "\n",
    "        clf = TopKRanker(LogisticRegression())\n",
    "        clf.fit(X_train, y_train_)\n",
    "\n",
    "        # find out how many labels should be predicted\n",
    "        top_k_list = [len(l) for l in y_test]\n",
    "        preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "        results = {}\n",
    "        averages = [\"micro\", \"macro\"]\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "#             print('=============y_pred==========')\n",
    "#             print(preds)\n",
    "#             print('=============y_test==========')\n",
    "#             print(y_test)\n",
    "        all_results['semi'].append(results)\n",
    "    else:\n",
    "        # 2. Shuffle, to create train/test groups\n",
    "        shuffles = []\n",
    "        for x in range(1):\n",
    "            shuffles.append(skshuffle(features_matrix, labels_matrix, random_state = 1))\n",
    "\n",
    "        # 3. to score each train/test group\n",
    "        all_results = defaultdict(list)\n",
    "\n",
    "    #     if args.all:\n",
    "    #         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "    #     else:\n",
    "    #         training_percents = [0.1, 0.5, 0.9]\n",
    "        training_percents = [0.1]\n",
    "        for train_percent in training_percents:\n",
    "            for shuf in shuffles:\n",
    "                \n",
    "                X, y = shuf\n",
    "\n",
    "                training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "                X_train = X[:training_size, :]\n",
    "                y_train_ = y[:training_size]\n",
    "\n",
    "                y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "                cy =  y_train_.tocoo()\n",
    "                for i, j in zip(cy.row, cy.col):\n",
    "                    y_train[i].append(j)\n",
    "\n",
    "                assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "                X_test = X[training_size:, :]\n",
    "                y_test_ = y[training_size:]\n",
    "\n",
    "                y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "                cy =  y_test_.tocoo()\n",
    "                for i, j in zip(cy.row, cy.col):\n",
    "                    y_test[i].append(j)\n",
    "\n",
    "                clf = TopKRanker(LogisticRegression())\n",
    "                clf.fit(X_train, y_train_)\n",
    "\n",
    "                # find out how many labels should be predicted\n",
    "                top_k_list = [len(l) for l in y_test]\n",
    "                preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "                results = {}\n",
    "                averages = [\"micro\", \"macro\"]\n",
    "                for average in averages:\n",
    "                    results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "                all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citeseer\n",
      "('Results, using embeddings of dimensionality', 128)\n",
      "-------------------\n",
      "('Train percent:', 'icml')\n",
      "('Shuffle #1:   ', {'micro': 0.5, 'macro': 0.33333333333333331})\n",
      "('Average score:', {'micro': 0.5, 'macro': 0.33333333333333331})\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'citeseer'\n",
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/results/exp_citeseer_unsupervised/citeseer_embeddings_iter0.txt'\n",
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "embedding_filename_other_lib = '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\n",
    "\n",
    "data_splited_filename = '/hdd2/graph_embedding/customized/blogcatalog_splited_p90.pickle'\n",
    "\n",
    "scoring(embedding_filename, matfile, 'icml', use_feature = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "model = KeyedVectors.load_word2vec_format(embedding_filename_original, binary=False)\n",
    "                                          \n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embeddings = np.loadtxt(embedding_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def write_walks_to_disk(G, filebase, num_paths, path_length, alpha=0, rand=random.Random(0), num_workers=cpu_count(),\n",
    "#                         always_rebuild=True):\n",
    "#     global __current_graph\n",
    "#     __current_graph = G\n",
    "#     files_list = [\"{}.{}\".format(filebase, str(x)) for x in list(range(num_paths))]\n",
    "#     expected_size = len(G)\n",
    "#     args_list = []\n",
    "#     files = []\n",
    "\n",
    "#     if num_paths <= num_workers:\n",
    "#         paths_per_worker = [1 for x in range(num_paths)]\n",
    "#     else:\n",
    "#         paths_per_worker = [len(list(filter(lambda z: z!= None, [y for y in x])))\n",
    "#                             for x in graph.grouper(int(num_paths / num_workers)+1, range(1, num_paths+1))]\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "#         for size, file_, ppw in zip(executor.map(count_lines, files_list), files_list, paths_per_worker):\n",
    "#             if always_rebuild or size != (ppw*expected_size):\n",
    "#                 args_list.append((ppw, path_length, alpha, random.Random(rand.randint(0, 2**31)), file_))\n",
    "#             else:\n",
    "#                 files.append(file_)\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "#         for file_ in executor.map(_write_walks_to_disk, args_list):\n",
    "#             files.append(file_)\n",
    "\n",
    "#     return files\n",
    "\n",
    "\n",
    "\n",
    "def random_walk(graph, num_paths, path_length, rand = random.Random(0)):\n",
    "    vocabulary_size = max(graph.keys()) + 1\n",
    "    i = 0\n",
    "    list_path = []\n",
    "    tot_path = 0\n",
    "    nodes = range(vocabulary_size)\n",
    "    for cnt in range(num_paths):\n",
    "        rand.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            path = [node]\n",
    "            for _ in xrange(path_length-1):\n",
    "                path.append(rand.choice(graph[path[-1]]))\n",
    "            list_path.append(path)\n",
    "    return list_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.x\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.y\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.tx\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.ty\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.graph\n",
      "Walking...\n"
     ]
    }
   ],
   "source": [
    "## adapt the icml paper to deepwalk\n",
    "DATASET = 'citeseer'\n",
    "number_walks = 50\n",
    "walk_length = 40\n",
    "seed = 1 \n",
    "workers = 1\n",
    "output_filename = '/hdd2/graph_embedding/customized/tmp/%s.embeddings' %(DATASET)\n",
    "def generate_data_icml(file_path, DATASET):\n",
    "    NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "    OBJECTS = []\n",
    "    for i in range(len(NAMES)):\n",
    "        f = file_path + 'trans.{}.{}'.format(DATASET, NAMES[i])\n",
    "        print(f)\n",
    "        OBJECTS.append(cPickle.load(open(f)))\n",
    "    x, y, tx, ty, graph = tuple(OBJECTS)\n",
    "    \n",
    "    num_walks = len(graph) * number_walks\n",
    "    \n",
    "    print(\"Walking...\")\n",
    "    walk = random_walk(graph, number_walks, walk_length)\n",
    "\n",
    "    return walk\n",
    "file_path = '/hdd2/graph_embedding/planetoid/data/'\n",
    "walks_filebase = output_filename + \".walks\"\n",
    "walk = generate_data_icml(file_path, DATASET)\n",
    "np.savetxt(walks_filebase, walk, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.x\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.y\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.tx\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.ty\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.graph\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'citeseer'\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    f = file_path + 'trans.{}.{}'.format(DATASET, NAMES[i])\n",
    "    print(f)\n",
    "    OBJECTS.append(cPickle.load(open(f)))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 3703)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "fx = x.toarray()\n",
    "y_true = [np.nonzero(e)[0][0] for e in y]\n",
    "ftx = tx.toarray()\n",
    "ty_true = [np.nonzero(e)[0][0] for e in ty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename = '/hdd2/graph_embedding/customized/results/exp_citeseer_semi/citeseer_embeddings_iter26.txt'\n",
    "embeddings = np.loadtxt(embedding_filename)\n",
    "emb_x = embeddings[:x.shape[0] , :]\n",
    "emb_tx = embeddings[x.shape[0]:x.shape[0] + ftx.shape[0], :]\n",
    "\n",
    "xx = np.concatenate([fx, emb_x], axis = 1)\n",
    "txx = np.concatenate([ftx, emb_tx], axis = 1)\n",
    "# xx = fx\n",
    "# txx = ftx\n",
    "\n",
    "# xx = emb_x\n",
    "# txx = emb_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.65800000\n",
      "macro f1: 0.62749644\n",
      "micro f1: 0.65800000\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(xx, y_true)\n",
    "ty_pred = logreg.predict(txx)\n",
    "print('acc: %.8f' %(accuracy_score(ty_true, ty_pred)))\n",
    "print('macro f1: %.8f' %f1_score(ty_true, ty_pred, average='macro'))\n",
    "print('micro f1: %.8f' %f1_score(ty_true, ty_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 128)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 3831)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3703 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 58 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
