{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'citeseer'\n",
    "\n",
    "embedding_size = 50\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 0.1\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    OBJECTS.append(cPickle.load(open(\"/hdd2/graph_embedding/planetoid/data/trans.{}.{}\".format(DATASET, NAMES[i]))))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_iter(iter):\n",
    "    \"\"\"an auxiliary function used for computing the number of iterations given the argument iter.\n",
    "    iter can either be an int or a float.\n",
    "    \"\"\"\n",
    "    if iter >= 1:\n",
    "        return iter\n",
    "    return 1 if random.random() < iter else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38732192  0.12907903  0.46030581]\n",
      " [ 0.05552411  0.5345757   1.12406611]\n",
      " [ 0.40012899  0.12259372 -0.88986868]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters([3,3], [3,4], [4,2])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(str(parameters['l_emd_f_W'].eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, x.shape[1]], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, y.shape[1]], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [None, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [None], name = 'ind')\n",
    "    \n",
    "    path_sym = tf.placeholder(tf.int32, shape = [200, path_size+1], name = 'path')\n",
    "    path_id_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'path_id')\n",
    "    w_path2pair_sym = tf.placeholder(tf.float32, shape = [13000, 200], name = 'w_path2pair_sym')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# softmax_weights = tf.Variab`le(\n",
    "#     tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "#                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "# softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "# l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_sampled = 64\n",
    "num_ver = max(graph.keys()) + 1\n",
    "vocabulary_size = num_ver\n",
    "n_hidden = 32\n",
    "n_steps = path_size+1 #path length\n",
    "n_input = 50 # dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weight2, _bias2):\n",
    "# #     _X = tf.transpose(_X, [1, 0, 2]) \n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     print('after: tf.reshape(_X, [-1, n_input]) ')\n",
    "#     print(_X.shape)\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    keep_prob = tf.constant(.8)\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple=True)\n",
    "    seqlen = np.ones(batch_size) * (path_size+1)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "#     print(rnn_outputs[:,-1,:].shape)\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "#     print(outputs)\n",
    "    \n",
    "#     lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "#     lstm_cells = lstm_cell_1\n",
    "    # Get LSTM cell output\n",
    "#     outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    \n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = rnn_outputs[:,-1,:]\n",
    "#     scale_output = tf.layers.dense(inputs=lstm_last_output, units=1, activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(lstm_last_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, rnn_outputs[:,-1,:], lstm_last_output\n",
    "#     return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dnn_paths(_X, _weight2, _bias2):\n",
    "# #     _X = tf.transpose(_X, [1, 0, 2]) \n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     print('after: tf.reshape(_X, [-1, n_input]) ')\n",
    "#     print(_X.shape)\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    path_dnn_output = tf.layers.dense(inputs = _X, units = 1, \n",
    "                    activation = tf.nn.softmax, \n",
    "                    kernel_initializer=glorot_uniform_initializer())\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_dnn_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, rnn_outputs[:,-1,:], lstm_last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average_Paths(_X, _weight2, _bias2):\n",
    "    path_avg_output = tf.reduce_mean(_X, axis=1)\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_avg_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, path_avg_output[-1], path_avg_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "\n",
    "cgraph = tf.Graph()\n",
    "\n",
    "with cgraph.as_default(), tf.device('/gpu:0'):\n",
    "\n",
    "    x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym = create_placeholders()\n",
    "\n",
    "    # word embedding\n",
    "    tf.random_normal_initializer(seed = 1)\n",
    "    tf.set_random_seed(1)\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "    ##\n",
    "    \n",
    "    path_weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size], mean=1.0))\n",
    "    }\n",
    "    path_biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([vocabulary_size]))\n",
    "    }\n",
    "    \n",
    "    s_weight_avg = tf.Variable(tf.random_normal([1, embedding_size]))\n",
    "    s_biase_avg = tf.Variable(tf.random_normal([1]))\n",
    "    \n",
    "#     print('path_sym.shape:')\n",
    "#     print(path_sym.shape)\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, path_sym)\n",
    "#     print('rnn_inputs.shape:')\n",
    "#     print(rnn_inputs.shape)\n",
    "    \n",
    "    if (use_reweight):\n",
    "        reweight, cg_outputs, cg_last_output = Average_Paths(\n",
    "            rnn_inputs, s_weight_avg, s_biase_avg)    \n",
    "        reweight = tf.reshape(reweight, [-1, 1])\n",
    "        reweight_id = tf.matmul(w_path2pair_sym, reweight)\n",
    "    else:\n",
    "        reweight = tf.ones(shape=[w_path2pair_sym.shape[0], 1])\n",
    "        reweight_id = tf.ones(shape=[path_sym.shape[0], 1])\n",
    "    \n",
    "\n",
    "    l_x_hid = tf.layers.dense(inputs = x_sym, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    if use_feature:\n",
    "        l_emd_z = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "        l_f = tf.concat([l_x_hid, l_emd_z], axis = 1)\n",
    "        l_y = tf.layers.dense(inputs = l_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    else:\n",
    "        l_y = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "\n",
    "    l_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_y, labels = y_sym))\n",
    "\n",
    "    if layer_loss and use_feature:\n",
    "        l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_x_hid, labels = y_sym))\n",
    "        l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_emd_z, labels = y_sym))\n",
    "        \n",
    "    # l_rnn = lstm(emb_f, units = xx)\n",
    "    # l_w = tf.layer(l_rnn, units = , activation = softmax)\n",
    "    # g_loss = weighted...\n",
    "\n",
    "    # if neg_samp == 0:\n",
    "    #     pass\n",
    "    # else:\n",
    "#     gw2v_loss = word2vec(l_emd_f, gy_sym, softmax_wecg_last_outputights, softmax_biases)\n",
    "    \n",
    "    g_loss = tf.reduce_mean(reweight_id *\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "                                   inputs = l_emd_f, labels = gy_sym, \n",
    "                                   num_sampled = vocabulary_size, \n",
    "                                   num_classes = vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(l_loss)\n",
    "    \n",
    "    g_optimizer = tf.train.GradientDescentOptimizer(g_learning_rate).minimize(g_loss)\n",
    "    \n",
    "    gl_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "                                   inputs = l_emd_f, labels = gy_sym,\n",
    "                                   num_sampled = vocabulary_size, \n",
    "                                   num_classes = vocabulary_size))\n",
    "    gl_optimizer = tf.train.GradientDescentOptimizer(gl_learning_rate).minimize(gl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_inst():\n",
    "    \"\"\"generator for batches for classification loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    while True:\n",
    "        ind = np.array(np.random.permutation(x.shape[0]), dtype = np.int32)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            j = min(ind.shape[0], i + batch_size)\n",
    "            yield x[ind[i: j]], y[ind[i: j]], ind[i: j]\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label_graph():\n",
    "    \"\"\"generator for batches for label context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    labels, label2inst, not_label = [], dd(list), dd(list)\n",
    "    for i in range(x.shape[0]):\n",
    "        flag = False\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i, j] == 1 and not flag:\n",
    "                labels.append(j)\n",
    "                label2inst[j].append(i)\n",
    "                flag = True\n",
    "            elif y[i, j] == 0:\n",
    "                not_label[j].append(i)\n",
    "\n",
    "    while True:\n",
    "        g, gy = [], []\n",
    "        for _ in range(g_sample_size):\n",
    "            x1 = random.randint(0, x.shape[0] - 1)\n",
    "            label = labels[x1]\n",
    "            if len(label2inst) == 1: continue\n",
    "            x2 = random.choice(label2inst[label])\n",
    "            g.append([x1, x2])\n",
    "            gy.append(1.0)\n",
    "#             for _ in range(neg_samp):\n",
    "#                 g.append([x1, random.choice(not_label[label])])\n",
    "#                 gy.append( - 1.0)\n",
    "        yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_graph():\n",
    "    \"\"\"generator for batches for graph context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_ver = max(graph.keys()) + 1\n",
    "\n",
    "    while True:\n",
    "        ind = np.random.permutation(num_ver)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            g, gy = [], []\n",
    "            list_path = []\n",
    "            list_path_id = []\n",
    "            count_path = 0\n",
    "            row_w_path2pair = 0\n",
    "            j = min(ind.shape[0], i + g_batch_size)\n",
    "            w_path2pair = np.zeros((13000, g_batch_size))\n",
    "            for k in ind[i: j]:\n",
    "                if len(graph[k]) == 0: continue\n",
    "                path = [k]\n",
    "                for _ in range(path_size):\n",
    "                    path.append(random.choice(graph[path[-1]]))\n",
    "                list_path.append(path)\n",
    "                for l in range(len(path)):\n",
    "                    for m in range(l - window_size, l + window_size + 1):\n",
    "                        if m < 0 or m >= len(path): continue\n",
    "                        g.append([path[l], path[m]])\n",
    "                        gy.append(1.0)\n",
    "                        list_path_id.append(count_path)\n",
    "                        w_path2pair[row_w_path2pair, count_path] = 1\n",
    "                        row_w_path2pair += 1\n",
    "                count_path += 1 \n",
    "#                         for _ in range(neg_samp):\n",
    "#                             # if the random number euqals to path[m], the it creates noise!\n",
    "#                             g.append([path[l], random.randint(0, num_ver - 1)])\n",
    "#                             gy.append(- 1.0)\n",
    "            yield (np.array(g, dtype = np.int32), \n",
    "                   np.array(gy, dtype = np.float32), \n",
    "                   np.array(list_path, np.float32), \n",
    "                   np.array(list_path_id, np.float32),\n",
    "                   w_path2pair)\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 2\n",
    "iter_label = 0\n",
    "use_reweight = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_iter_label\n",
      "iter label 0 8.11036\n",
      "iter label 1 8.10921\n",
      "init_iter_graph\n",
      "iter graph 0 0.0248184\n",
      "iter graph 1 0.0248184\n",
      "iter graph 2 0.0248184\n",
      "iter graph 3 0.0248184\n",
      "iter graph 4 0.0248184\n",
      "iter graph 5 0.0248184\n",
      "iter graph 6 0.0248184\n",
      "init_iter_label\n",
      "iter label 0 0.0405572\n",
      "iter label 1 0.0405603\n",
      "iter: 1, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 2, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 3, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 4, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 5, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 6, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 7, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 8, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 9, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 10, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 11, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 12, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 13, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 14, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 15, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 16, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 17, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 18, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 19, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 20, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 21, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 22, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 23, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 24, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 25, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 26, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 27, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 28, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 29, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 30, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 31, Tst Acc: 0.2310, Trn Acc: 0.1667\n",
      "iter: 32, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 33, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 34, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 35, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 36, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 37, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 38, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 39, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 40, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 41, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 42, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 43, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 44, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 45, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 46, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 47, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 48, Tst Acc: 0.2300, Trn Acc: 0.1583\n",
      "iter: 49, Tst Acc: 0.2300, Trn Acc: 0.1583\n",
      "iter: 50, Tst Acc: 0.2300, Trn Acc: 0.1583\n",
      "iter: 51, Tst Acc: 0.2300, Trn Acc: 0.1583\n",
      "iter: 52, Tst Acc: 0.2300, Trn Acc: 0.1583\n",
      "iter: 53, Tst Acc: 0.2300, Trn Acc: 0.1583\n",
      "iter: 54, Tst Acc: 0.2300, Trn Acc: 0.1667\n",
      "iter: 55, Tst Acc: 0.2300, Trn Acc: 0.1417\n",
      "iter: 56, Tst Acc: 0.2290, Trn Acc: 0.1417\n",
      "iter: 57, Tst Acc: 0.2290, Trn Acc: 0.1500\n",
      "iter: 58, Tst Acc: 0.2290, Trn Acc: 0.1500\n",
      "iter: 59, Tst Acc: 0.2290, Trn Acc: 0.1500\n",
      "iter: 60, Tst Acc: 0.2290, Trn Acc: 0.1417\n",
      "iter: 61, Tst Acc: 0.2290, Trn Acc: 0.1583\n",
      "iter: 62, Tst Acc: 0.2290, Trn Acc: 0.1583\n",
      "iter: 63, Tst Acc: 0.2290, Trn Acc: 0.1667\n",
      "iter: 64, Tst Acc: 0.2290, Trn Acc: 0.1750\n",
      "iter: 65, Tst Acc: 0.2300, Trn Acc: 0.1750\n",
      "iter: 66, Tst Acc: 0.2320, Trn Acc: 0.1917\n",
      "iter: 67, Tst Acc: 0.2330, Trn Acc: 0.1917\n",
      "iter: 68, Tst Acc: 0.2320, Trn Acc: 0.1833\n",
      "iter: 69, Tst Acc: 0.2320, Trn Acc: 0.1917\n",
      "iter: 70, Tst Acc: 0.2320, Trn Acc: 0.2000\n",
      "iter: 71, Tst Acc: 0.2310, Trn Acc: 0.2000\n",
      "iter: 72, Tst Acc: 0.2310, Trn Acc: 0.1917\n",
      "iter: 73, Tst Acc: 0.2310, Trn Acc: 0.2000\n",
      "iter: 74, Tst Acc: 0.2310, Trn Acc: 0.1917\n",
      "iter: 75, Tst Acc: 0.2310, Trn Acc: 0.2000\n",
      "iter: 76, Tst Acc: 0.2300, Trn Acc: 0.2000\n",
      "iter: 77, Tst Acc: 0.2310, Trn Acc: 0.2250\n",
      "iter: 78, Tst Acc: 0.2350, Trn Acc: 0.2250\n",
      "iter: 79, Tst Acc: 0.2360, Trn Acc: 0.2250\n",
      "iter: 80, Tst Acc: 0.2370, Trn Acc: 0.2333\n",
      "iter: 81, Tst Acc: 0.2370, Trn Acc: 0.2333\n",
      "iter: 82, Tst Acc: 0.2380, Trn Acc: 0.2417\n",
      "iter: 83, Tst Acc: 0.2380, Trn Acc: 0.2500\n",
      "iter: 84, Tst Acc: 0.2380, Trn Acc: 0.2500\n",
      "iter: 85, Tst Acc: 0.2390, Trn Acc: 0.2500\n",
      "iter: 86, Tst Acc: 0.2400, Trn Acc: 0.2500\n",
      "iter: 87, Tst Acc: 0.2430, Trn Acc: 0.2500\n",
      "iter: 88, Tst Acc: 0.2440, Trn Acc: 0.2500\n",
      "iter: 89, Tst Acc: 0.2440, Trn Acc: 0.2500\n",
      "iter: 90, Tst Acc: 0.2470, Trn Acc: 0.2500\n",
      "iter: 91, Tst Acc: 0.2500, Trn Acc: 0.2500\n",
      "iter: 92, Tst Acc: 0.2510, Trn Acc: 0.2500\n",
      "iter: 93, Tst Acc: 0.2540, Trn Acc: 0.2583\n",
      "iter: 94, Tst Acc: 0.2540, Trn Acc: 0.2500\n",
      "iter: 95, Tst Acc: 0.2560, Trn Acc: 0.2417\n",
      "iter: 96, Tst Acc: 0.2550, Trn Acc: 0.2417\n",
      "iter: 97, Tst Acc: 0.2550, Trn Acc: 0.2417\n",
      "iter: 98, Tst Acc: 0.2540, Trn Acc: 0.2417\n",
      "iter: 99, Tst Acc: 0.2550, Trn Acc: 0.2417\n",
      "iter: 100, Tst Acc: 0.2580, Trn Acc: 0.2417\n",
      "iter: 101, Tst Acc: 0.2600, Trn Acc: 0.2417\n",
      "iter: 102, Tst Acc: 0.2620, Trn Acc: 0.2417\n",
      "iter: 103, Tst Acc: 0.2630, Trn Acc: 0.2417\n",
      "iter: 104, Tst Acc: 0.2650, Trn Acc: 0.2417\n",
      "iter: 105, Tst Acc: 0.2680, Trn Acc: 0.2417\n",
      "iter: 106, Tst Acc: 0.2680, Trn Acc: 0.2417\n",
      "iter: 107, Tst Acc: 0.2700, Trn Acc: 0.2417\n",
      "iter: 108, Tst Acc: 0.2700, Trn Acc: 0.2417\n",
      "iter: 109, Tst Acc: 0.2700, Trn Acc: 0.2417\n",
      "iter: 110, Tst Acc: 0.2710, Trn Acc: 0.2417\n",
      "iter: 111, Tst Acc: 0.2720, Trn Acc: 0.2417\n",
      "iter: 112, Tst Acc: 0.2710, Trn Acc: 0.2417\n",
      "iter: 113, Tst Acc: 0.2710, Trn Acc: 0.2417\n",
      "iter: 114, Tst Acc: 0.2710, Trn Acc: 0.2333\n",
      "iter: 115, Tst Acc: 0.2730, Trn Acc: 0.2333\n",
      "iter: 116, Tst Acc: 0.2740, Trn Acc: 0.2333\n",
      "iter: 117, Tst Acc: 0.2750, Trn Acc: 0.2333\n",
      "iter: 118, Tst Acc: 0.2790, Trn Acc: 0.2333\n",
      "iter: 119, Tst Acc: 0.2820, Trn Acc: 0.2333\n",
      "iter: 120, Tst Acc: 0.2840, Trn Acc: 0.2333\n",
      "iter: 121, Tst Acc: 0.2890, Trn Acc: 0.2333\n",
      "iter: 122, Tst Acc: 0.2860, Trn Acc: 0.2333\n",
      "iter: 123, Tst Acc: 0.2890, Trn Acc: 0.2333\n",
      "iter: 124, Tst Acc: 0.2890, Trn Acc: 0.2333\n",
      "iter: 125, Tst Acc: 0.2910, Trn Acc: 0.2333\n",
      "iter: 126, Tst Acc: 0.2910, Trn Acc: 0.2333\n",
      "iter: 127, Tst Acc: 0.2910, Trn Acc: 0.2333\n",
      "iter: 128, Tst Acc: 0.2930, Trn Acc: 0.2333\n",
      "iter: 129, Tst Acc: 0.2940, Trn Acc: 0.2333\n",
      "iter: 130, Tst Acc: 0.2980, Trn Acc: 0.2333\n",
      "iter: 131, Tst Acc: 0.2990, Trn Acc: 0.2417\n",
      "iter: 132, Tst Acc: 0.3000, Trn Acc: 0.2417\n",
      "iter: 133, Tst Acc: 0.3010, Trn Acc: 0.2417\n",
      "iter: 134, Tst Acc: 0.3010, Trn Acc: 0.2417\n",
      "iter: 135, Tst Acc: 0.2990, Trn Acc: 0.2417\n",
      "iter: 136, Tst Acc: 0.2990, Trn Acc: 0.2417\n",
      "iter: 137, Tst Acc: 0.3000, Trn Acc: 0.2500\n",
      "iter: 138, Tst Acc: 0.3030, Trn Acc: 0.2500\n",
      "iter: 139, Tst Acc: 0.3050, Trn Acc: 0.2583\n",
      "iter: 140, Tst Acc: 0.3050, Trn Acc: 0.2667\n",
      "iter: 141, Tst Acc: 0.3100, Trn Acc: 0.2667\n",
      "iter: 142, Tst Acc: 0.3080, Trn Acc: 0.2667\n",
      "iter: 143, Tst Acc: 0.3100, Trn Acc: 0.2667\n",
      "iter: 144, Tst Acc: 0.3120, Trn Acc: 0.2750\n",
      "iter: 145, Tst Acc: 0.3110, Trn Acc: 0.2833\n",
      "iter: 146, Tst Acc: 0.3120, Trn Acc: 0.2833\n",
      "iter: 147, Tst Acc: 0.3140, Trn Acc: 0.2833\n",
      "iter: 148, Tst Acc: 0.3110, Trn Acc: 0.2833\n",
      "iter: 149, Tst Acc: 0.3100, Trn Acc: 0.2917\n",
      "iter: 150, Tst Acc: 0.3110, Trn Acc: 0.3000\n",
      "iter: 151, Tst Acc: 0.3100, Trn Acc: 0.3000\n",
      "iter: 152, Tst Acc: 0.3100, Trn Acc: 0.3000\n",
      "iter: 153, Tst Acc: 0.3130, Trn Acc: 0.3000\n",
      "iter: 154, Tst Acc: 0.3200, Trn Acc: 0.3083\n",
      "iter: 155, Tst Acc: 0.3210, Trn Acc: 0.3167\n",
      "iter: 156, Tst Acc: 0.3270, Trn Acc: 0.3333\n",
      "iter: 157, Tst Acc: 0.3280, Trn Acc: 0.3750\n",
      "iter: 158, Tst Acc: 0.3280, Trn Acc: 0.3833\n",
      "iter: 159, Tst Acc: 0.3310, Trn Acc: 0.4250\n",
      "iter: 160, Tst Acc: 0.3310, Trn Acc: 0.4333\n",
      "iter: 161, Tst Acc: 0.3310, Trn Acc: 0.4500\n",
      "iter: 162, Tst Acc: 0.3310, Trn Acc: 0.4500\n",
      "iter: 163, Tst Acc: 0.3350, Trn Acc: 0.4833\n",
      "iter: 164, Tst Acc: 0.3380, Trn Acc: 0.5000\n",
      "iter: 165, Tst Acc: 0.3400, Trn Acc: 0.5000\n",
      "iter: 166, Tst Acc: 0.3470, Trn Acc: 0.5000\n",
      "iter: 167, Tst Acc: 0.3480, Trn Acc: 0.5333\n",
      "iter: 168, Tst Acc: 0.3520, Trn Acc: 0.5750\n",
      "iter: 169, Tst Acc: 0.3500, Trn Acc: 0.6000\n",
      "iter: 170, Tst Acc: 0.3550, Trn Acc: 0.6083\n",
      "iter: 171, Tst Acc: 0.3580, Trn Acc: 0.6333\n",
      "iter: 172, Tst Acc: 0.3630, Trn Acc: 0.6500\n",
      "iter: 173, Tst Acc: 0.3640, Trn Acc: 0.6667\n",
      "iter: 174, Tst Acc: 0.3690, Trn Acc: 0.6667\n",
      "iter: 175, Tst Acc: 0.3720, Trn Acc: 0.6667\n",
      "iter: 176, Tst Acc: 0.3750, Trn Acc: 0.6667\n",
      "iter: 177, Tst Acc: 0.3800, Trn Acc: 0.6667\n",
      "iter: 178, Tst Acc: 0.3870, Trn Acc: 0.6667\n",
      "iter: 179, Tst Acc: 0.3900, Trn Acc: 0.6667\n",
      "iter: 180, Tst Acc: 0.3920, Trn Acc: 0.6667\n",
      "iter: 181, Tst Acc: 0.3930, Trn Acc: 0.6667\n",
      "iter: 182, Tst Acc: 0.3940, Trn Acc: 0.6667\n",
      "iter: 183, Tst Acc: 0.3980, Trn Acc: 0.6667\n",
      "iter: 184, Tst Acc: 0.4030, Trn Acc: 0.6667\n",
      "iter: 185, Tst Acc: 0.4010, Trn Acc: 0.6667\n",
      "iter: 186, Tst Acc: 0.4060, Trn Acc: 0.6667\n",
      "iter: 187, Tst Acc: 0.4050, Trn Acc: 0.6667\n",
      "iter: 188, Tst Acc: 0.4090, Trn Acc: 0.6667\n",
      "iter: 189, Tst Acc: 0.4100, Trn Acc: 0.6667\n",
      "iter: 190, Tst Acc: 0.4130, Trn Acc: 0.6667\n",
      "iter: 191, Tst Acc: 0.4150, Trn Acc: 0.6667\n",
      "iter: 192, Tst Acc: 0.4190, Trn Acc: 0.6667\n",
      "iter: 193, Tst Acc: 0.4200, Trn Acc: 0.6667\n",
      "iter: 194, Tst Acc: 0.4200, Trn Acc: 0.6667\n",
      "iter: 195, Tst Acc: 0.4220, Trn Acc: 0.6667\n",
      "iter: 196, Tst Acc: 0.4270, Trn Acc: 0.6667\n",
      "iter: 197, Tst Acc: 0.4300, Trn Acc: 0.6667\n",
      "iter: 198, Tst Acc: 0.4320, Trn Acc: 0.6667\n",
      "iter: 199, Tst Acc: 0.4280, Trn Acc: 0.6667\n",
      "iter: 200, Tst Acc: 0.4330, Trn Acc: 0.6667\n",
      "iter: 201, Tst Acc: 0.4340, Trn Acc: 0.6667\n",
      "iter: 202, Tst Acc: 0.4360, Trn Acc: 0.6667\n",
      "iter: 203, Tst Acc: 0.4370, Trn Acc: 0.6667\n",
      "iter: 204, Tst Acc: 0.4380, Trn Acc: 0.6667\n",
      "iter: 205, Tst Acc: 0.4400, Trn Acc: 0.6667\n",
      "iter: 206, Tst Acc: 0.4400, Trn Acc: 0.6667\n",
      "iter: 207, Tst Acc: 0.4430, Trn Acc: 0.6667\n",
      "iter: 208, Tst Acc: 0.4440, Trn Acc: 0.6667\n",
      "iter: 209, Tst Acc: 0.4470, Trn Acc: 0.6667\n",
      "iter: 210, Tst Acc: 0.4470, Trn Acc: 0.6667\n",
      "iter: 211, Tst Acc: 0.4460, Trn Acc: 0.6667\n",
      "iter: 212, Tst Acc: 0.4460, Trn Acc: 0.6667\n",
      "iter: 213, Tst Acc: 0.4460, Trn Acc: 0.6667\n",
      "iter: 214, Tst Acc: 0.4460, Trn Acc: 0.6667\n",
      "iter: 215, Tst Acc: 0.4510, Trn Acc: 0.6667\n",
      "iter: 216, Tst Acc: 0.4530, Trn Acc: 0.6667\n",
      "iter: 217, Tst Acc: 0.4550, Trn Acc: 0.6667\n",
      "iter: 218, Tst Acc: 0.4560, Trn Acc: 0.6667\n",
      "iter: 219, Tst Acc: 0.4590, Trn Acc: 0.6667\n",
      "iter: 220, Tst Acc: 0.4600, Trn Acc: 0.6667\n",
      "iter: 221, Tst Acc: 0.4590, Trn Acc: 0.6667\n",
      "iter: 222, Tst Acc: 0.4590, Trn Acc: 0.6667\n",
      "iter: 223, Tst Acc: 0.4590, Trn Acc: 0.6667\n",
      "iter: 224, Tst Acc: 0.4590, Trn Acc: 0.6667\n",
      "iter: 225, Tst Acc: 0.4620, Trn Acc: 0.6667\n",
      "iter: 226, Tst Acc: 0.4640, Trn Acc: 0.6667\n",
      "iter: 227, Tst Acc: 0.4650, Trn Acc: 0.6667\n",
      "iter: 228, Tst Acc: 0.4680, Trn Acc: 0.6667\n",
      "iter: 229, Tst Acc: 0.4690, Trn Acc: 0.6667\n",
      "iter: 230, Tst Acc: 0.4690, Trn Acc: 0.6667\n",
      "iter: 231, Tst Acc: 0.4700, Trn Acc: 0.6667\n",
      "iter: 232, Tst Acc: 0.4700, Trn Acc: 0.6667\n",
      "iter: 233, Tst Acc: 0.4710, Trn Acc: 0.6667\n",
      "iter: 234, Tst Acc: 0.4710, Trn Acc: 0.6667\n",
      "iter: 235, Tst Acc: 0.4720, Trn Acc: 0.6667\n",
      "iter: 236, Tst Acc: 0.4730, Trn Acc: 0.6667\n",
      "iter: 237, Tst Acc: 0.4730, Trn Acc: 0.6667\n",
      "iter: 238, Tst Acc: 0.4750, Trn Acc: 0.6667\n",
      "iter: 239, Tst Acc: 0.4760, Trn Acc: 0.6667\n",
      "iter: 240, Tst Acc: 0.4780, Trn Acc: 0.6667\n",
      "iter: 241, Tst Acc: 0.4790, Trn Acc: 0.6667\n",
      "iter: 242, Tst Acc: 0.4770, Trn Acc: 0.6667\n",
      "iter: 243, Tst Acc: 0.4780, Trn Acc: 0.6667\n",
      "iter: 244, Tst Acc: 0.4810, Trn Acc: 0.6667\n",
      "iter: 245, Tst Acc: 0.4820, Trn Acc: 0.6667\n",
      "iter: 246, Tst Acc: 0.4820, Trn Acc: 0.6750\n",
      "iter: 247, Tst Acc: 0.4820, Trn Acc: 0.6750\n",
      "iter: 248, Tst Acc: 0.4850, Trn Acc: 0.6750\n",
      "iter: 249, Tst Acc: 0.4860, Trn Acc: 0.6750\n",
      "iter: 250, Tst Acc: 0.4890, Trn Acc: 0.6750\n",
      "iter: 251, Tst Acc: 0.4890, Trn Acc: 0.6917\n",
      "iter: 252, Tst Acc: 0.4890, Trn Acc: 0.6917\n",
      "iter: 253, Tst Acc: 0.4900, Trn Acc: 0.6917\n",
      "iter: 254, Tst Acc: 0.4910, Trn Acc: 0.7000\n",
      "iter: 255, Tst Acc: 0.4920, Trn Acc: 0.7250\n",
      "iter: 256, Tst Acc: 0.4920, Trn Acc: 0.7500\n",
      "iter: 257, Tst Acc: 0.4910, Trn Acc: 0.7583\n",
      "iter: 258, Tst Acc: 0.4950, Trn Acc: 0.7833\n",
      "iter: 259, Tst Acc: 0.4950, Trn Acc: 0.7917\n",
      "iter: 260, Tst Acc: 0.4940, Trn Acc: 0.7917\n",
      "iter: 261, Tst Acc: 0.4940, Trn Acc: 0.8000\n",
      "iter: 262, Tst Acc: 0.4940, Trn Acc: 0.8083\n",
      "iter: 263, Tst Acc: 0.4940, Trn Acc: 0.8167\n",
      "iter: 264, Tst Acc: 0.4950, Trn Acc: 0.8167\n",
      "iter: 265, Tst Acc: 0.4970, Trn Acc: 0.8167\n",
      "iter: 266, Tst Acc: 0.4970, Trn Acc: 0.8167\n",
      "iter: 267, Tst Acc: 0.4980, Trn Acc: 0.8167\n",
      "iter: 268, Tst Acc: 0.5000, Trn Acc: 0.8167\n",
      "iter: 269, Tst Acc: 0.4980, Trn Acc: 0.8167\n",
      "iter: 270, Tst Acc: 0.4980, Trn Acc: 0.8167\n",
      "iter: 271, Tst Acc: 0.5000, Trn Acc: 0.8167\n",
      "iter: 272, Tst Acc: 0.5000, Trn Acc: 0.8167\n",
      "iter: 273, Tst Acc: 0.4990, Trn Acc: 0.8167\n",
      "iter: 274, Tst Acc: 0.5010, Trn Acc: 0.8167\n",
      "iter: 275, Tst Acc: 0.5020, Trn Acc: 0.8167\n",
      "iter: 276, Tst Acc: 0.5030, Trn Acc: 0.8250\n",
      "iter: 277, Tst Acc: 0.5040, Trn Acc: 0.8250\n",
      "iter: 278, Tst Acc: 0.5030, Trn Acc: 0.8250\n",
      "iter: 279, Tst Acc: 0.5070, Trn Acc: 0.8250\n",
      "iter: 280, Tst Acc: 0.5080, Trn Acc: 0.8250\n",
      "iter: 281, Tst Acc: 0.5080, Trn Acc: 0.8333\n",
      "iter: 282, Tst Acc: 0.5080, Trn Acc: 0.8333\n",
      "iter: 283, Tst Acc: 0.5090, Trn Acc: 0.8333\n",
      "iter: 284, Tst Acc: 0.5100, Trn Acc: 0.8333\n",
      "iter: 285, Tst Acc: 0.5110, Trn Acc: 0.8333\n",
      "iter: 286, Tst Acc: 0.5110, Trn Acc: 0.8333\n",
      "iter: 287, Tst Acc: 0.5130, Trn Acc: 0.8333\n",
      "iter: 288, Tst Acc: 0.5150, Trn Acc: 0.8333\n",
      "iter: 289, Tst Acc: 0.5170, Trn Acc: 0.8333\n",
      "iter: 290, Tst Acc: 0.5170, Trn Acc: 0.8333\n",
      "iter: 291, Tst Acc: 0.5180, Trn Acc: 0.8333\n",
      "iter: 292, Tst Acc: 0.5210, Trn Acc: 0.8333\n",
      "iter: 293, Tst Acc: 0.5230, Trn Acc: 0.8333\n",
      "iter: 294, Tst Acc: 0.5240, Trn Acc: 0.8333\n",
      "iter: 295, Tst Acc: 0.5270, Trn Acc: 0.8333\n",
      "iter: 296, Tst Acc: 0.5280, Trn Acc: 0.8333\n",
      "iter: 297, Tst Acc: 0.5280, Trn Acc: 0.8333\n",
      "iter: 298, Tst Acc: 0.5300, Trn Acc: 0.8333\n",
      "iter: 299, Tst Acc: 0.5300, Trn Acc: 0.8333\n",
      "iter: 300, Tst Acc: 0.5300, Trn Acc: 0.8333\n",
      "iter: 301, Tst Acc: 0.5290, Trn Acc: 0.8333\n",
      "iter: 302, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 303, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 304, Tst Acc: 0.5330, Trn Acc: 0.8333\n",
      "iter: 305, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 306, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 307, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 308, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 309, Tst Acc: 0.5320, Trn Acc: 0.8333\n",
      "iter: 310, Tst Acc: 0.5330, Trn Acc: 0.8333\n",
      "iter: 311, Tst Acc: 0.5360, Trn Acc: 0.8333\n",
      "iter: 312, Tst Acc: 0.5360, Trn Acc: 0.8333\n",
      "iter: 313, Tst Acc: 0.5390, Trn Acc: 0.8333\n",
      "iter: 314, Tst Acc: 0.5420, Trn Acc: 0.8333\n",
      "iter: 315, Tst Acc: 0.5440, Trn Acc: 0.8333\n",
      "iter: 316, Tst Acc: 0.5450, Trn Acc: 0.8333\n",
      "iter: 317, Tst Acc: 0.5450, Trn Acc: 0.8333\n",
      "iter: 318, Tst Acc: 0.5450, Trn Acc: 0.8333\n",
      "iter: 319, Tst Acc: 0.5480, Trn Acc: 0.8333\n",
      "iter: 320, Tst Acc: 0.5510, Trn Acc: 0.8333\n",
      "iter: 321, Tst Acc: 0.5510, Trn Acc: 0.8333\n",
      "iter: 322, Tst Acc: 0.5530, Trn Acc: 0.8333\n",
      "iter: 323, Tst Acc: 0.5540, Trn Acc: 0.8333\n",
      "iter: 324, Tst Acc: 0.5550, Trn Acc: 0.8333\n",
      "iter: 325, Tst Acc: 0.5570, Trn Acc: 0.8333\n",
      "iter: 326, Tst Acc: 0.5560, Trn Acc: 0.8333\n",
      "iter: 327, Tst Acc: 0.5560, Trn Acc: 0.8333\n",
      "iter: 328, Tst Acc: 0.5550, Trn Acc: 0.8333\n",
      "iter: 329, Tst Acc: 0.5560, Trn Acc: 0.8333\n",
      "iter: 330, Tst Acc: 0.5560, Trn Acc: 0.8333\n",
      "iter: 331, Tst Acc: 0.5580, Trn Acc: 0.8333\n",
      "iter: 332, Tst Acc: 0.5600, Trn Acc: 0.8333\n",
      "iter: 333, Tst Acc: 0.5610, Trn Acc: 0.8333\n",
      "iter: 334, Tst Acc: 0.5630, Trn Acc: 0.8333\n",
      "iter: 335, Tst Acc: 0.5620, Trn Acc: 0.8333\n",
      "iter: 336, Tst Acc: 0.5610, Trn Acc: 0.8333\n",
      "iter: 337, Tst Acc: 0.5630, Trn Acc: 0.8333\n",
      "iter: 338, Tst Acc: 0.5640, Trn Acc: 0.8333\n",
      "iter: 339, Tst Acc: 0.5660, Trn Acc: 0.8333\n",
      "iter: 340, Tst Acc: 0.5670, Trn Acc: 0.8333\n",
      "iter: 341, Tst Acc: 0.5690, Trn Acc: 0.8333\n",
      "iter: 342, Tst Acc: 0.5690, Trn Acc: 0.8333\n",
      "iter: 343, Tst Acc: 0.5690, Trn Acc: 0.8333\n",
      "iter: 344, Tst Acc: 0.5710, Trn Acc: 0.8333\n",
      "iter: 345, Tst Acc: 0.5710, Trn Acc: 0.8333\n",
      "iter: 346, Tst Acc: 0.5710, Trn Acc: 0.8333\n",
      "iter: 347, Tst Acc: 0.5730, Trn Acc: 0.8333\n",
      "iter: 348, Tst Acc: 0.5730, Trn Acc: 0.8333\n",
      "iter: 349, Tst Acc: 0.5740, Trn Acc: 0.8333\n",
      "iter: 350, Tst Acc: 0.5760, Trn Acc: 0.8333\n",
      "iter: 351, Tst Acc: 0.5770, Trn Acc: 0.8333\n",
      "iter: 352, Tst Acc: 0.5770, Trn Acc: 0.8333\n",
      "iter: 353, Tst Acc: 0.5800, Trn Acc: 0.8333\n",
      "iter: 354, Tst Acc: 0.5800, Trn Acc: 0.8333\n",
      "iter: 355, Tst Acc: 0.5830, Trn Acc: 0.8333\n",
      "iter: 356, Tst Acc: 0.5820, Trn Acc: 0.8333\n",
      "iter: 357, Tst Acc: 0.5820, Trn Acc: 0.8333\n",
      "iter: 358, Tst Acc: 0.5830, Trn Acc: 0.8333\n",
      "iter: 359, Tst Acc: 0.5830, Trn Acc: 0.8333\n",
      "iter: 360, Tst Acc: 0.5840, Trn Acc: 0.8333\n",
      "iter: 361, Tst Acc: 0.5850, Trn Acc: 0.8333\n",
      "iter: 362, Tst Acc: 0.5850, Trn Acc: 0.8333\n",
      "iter: 363, Tst Acc: 0.5850, Trn Acc: 0.8333\n",
      "iter: 364, Tst Acc: 0.5840, Trn Acc: 0.8333\n",
      "iter: 365, Tst Acc: 0.5850, Trn Acc: 0.8333\n",
      "iter: 366, Tst Acc: 0.5860, Trn Acc: 0.8333\n",
      "iter: 367, Tst Acc: 0.5890, Trn Acc: 0.8333\n",
      "iter: 368, Tst Acc: 0.5900, Trn Acc: 0.8333\n",
      "iter: 369, Tst Acc: 0.5910, Trn Acc: 0.8333\n",
      "iter: 370, Tst Acc: 0.5910, Trn Acc: 0.8333\n",
      "iter: 371, Tst Acc: 0.5930, Trn Acc: 0.8333\n",
      "iter: 372, Tst Acc: 0.5940, Trn Acc: 0.8333\n",
      "iter: 373, Tst Acc: 0.5950, Trn Acc: 0.8333\n",
      "iter: 374, Tst Acc: 0.5960, Trn Acc: 0.8333\n",
      "iter: 375, Tst Acc: 0.5960, Trn Acc: 0.8333\n",
      "iter: 376, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 377, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 378, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 379, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 380, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 381, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 382, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 383, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 384, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 385, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 386, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 387, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 388, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 389, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 390, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 391, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 392, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 393, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 394, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 395, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 396, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 397, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 398, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 399, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 400, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 401, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 402, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 403, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 404, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 405, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 406, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 407, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 408, Tst Acc: 0.6100, Trn Acc: 0.8333\n",
      "iter: 409, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 410, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 411, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 412, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 413, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 414, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 415, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 416, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 417, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 418, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 419, Tst Acc: 0.6100, Trn Acc: 0.8333\n",
      "iter: 420, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 421, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 422, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 423, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 424, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 425, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 426, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 427, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 428, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 429, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 430, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 431, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 432, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 433, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 434, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 435, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 436, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 437, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 438, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 439, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 440, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 441, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 442, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 443, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 444, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 445, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 446, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 447, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 448, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 449, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 450, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 451, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 452, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 453, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 454, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 455, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 456, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 457, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 458, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 459, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 460, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 461, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 462, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 463, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 464, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 465, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 466, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 467, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 468, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 469, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 470, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 471, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 472, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 473, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 474, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 475, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 476, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 477, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 478, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 479, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 480, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 481, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 482, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 483, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 484, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 485, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 486, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 487, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 488, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 489, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 490, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 491, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 492, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 493, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 494, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 495, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 496, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 497, Tst Acc: 0.5980, Trn Acc: 0.8333\n",
      "iter: 498, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 499, Tst Acc: 0.5970, Trn Acc: 0.8333\n",
      "iter: 500, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 501, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 502, Tst Acc: 0.6000, Trn Acc: 0.8333\n",
      "iter: 503, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 504, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 505, Tst Acc: 0.5990, Trn Acc: 0.8333\n",
      "iter: 506, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 507, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 508, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 509, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 510, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 511, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 512, Tst Acc: 0.6010, Trn Acc: 0.8333\n",
      "iter: 513, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 514, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 515, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 516, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 517, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 518, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 519, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 520, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 521, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 522, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 523, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 524, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 525, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 526, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 527, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 528, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 529, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 530, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 531, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 532, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 533, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 534, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 535, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 536, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 537, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 538, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 539, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 540, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 541, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 542, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 543, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 544, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 545, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 546, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 547, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 548, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 549, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 550, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 551, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 552, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 553, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 554, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 555, Tst Acc: 0.6020, Trn Acc: 0.8333\n",
      "iter: 556, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 557, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 558, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 559, Tst Acc: 0.6030, Trn Acc: 0.8333\n",
      "iter: 560, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 561, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 562, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 563, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 564, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 565, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 566, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 567, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 568, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 569, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 570, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 571, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 572, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 573, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 574, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 575, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 576, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 577, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 578, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 579, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 580, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 581, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 582, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 583, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 584, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 585, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 586, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 587, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 588, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 589, Tst Acc: 0.6040, Trn Acc: 0.8333\n",
      "iter: 590, Tst Acc: 0.6050, Trn Acc: 0.8333\n",
      "iter: 591, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 592, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 593, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 594, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 595, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 596, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 597, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 598, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 599, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 600, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 601, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 602, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 603, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 604, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 605, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 606, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 607, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 608, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 609, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 610, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 611, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 612, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 613, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 614, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 615, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 616, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 617, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 618, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 619, Tst Acc: 0.6060, Trn Acc: 0.8333\n",
      "iter: 620, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 621, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 622, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 623, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 624, Tst Acc: 0.6070, Trn Acc: 0.8333\n",
      "iter: 625, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 626, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 627, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 628, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 629, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 630, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 631, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 632, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 633, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 634, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 635, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 636, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 637, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 638, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 639, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 640, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 641, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 642, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 643, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 644, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 645, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 646, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 647, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 648, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 649, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 650, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 651, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 652, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 653, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 654, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 655, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 656, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 657, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 658, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 659, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 660, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 661, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 662, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 663, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 664, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 665, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 666, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 667, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 668, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 669, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 670, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 671, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 672, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 673, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 674, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 675, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 676, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 677, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 678, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 679, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 680, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 681, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 682, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 683, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 684, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 685, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 686, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 687, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 688, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 689, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 690, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 691, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 692, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 693, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 694, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 695, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 696, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 697, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 698, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 699, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 700, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 701, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 702, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 703, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 704, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 705, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 706, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 707, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 708, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 709, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 710, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 711, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 712, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 713, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 714, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 715, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 716, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 717, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 718, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 719, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 720, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 721, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 722, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 723, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 724, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 725, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 726, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 727, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 728, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 729, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 730, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 731, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 732, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 733, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 734, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 735, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 736, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 737, Tst Acc: 0.6080, Trn Acc: 0.8333\n",
      "iter: 738, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 739, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 740, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 741, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 742, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 743, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 744, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 745, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 746, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 747, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 748, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 749, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 750, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 751, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 752, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 753, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 754, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 755, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 756, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 757, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 758, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 759, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 760, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 761, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 762, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 763, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 764, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 765, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 766, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 767, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 768, Tst Acc: 0.6090, Trn Acc: 0.8333\n",
      "iter: 769, Tst Acc: 0.6090, Trn Acc: 0.8417\n",
      "iter: 770, Tst Acc: 0.6090, Trn Acc: 0.8833\n",
      "iter: 771, Tst Acc: 0.6090, Trn Acc: 0.9083\n",
      "iter: 772, Tst Acc: 0.6100, Trn Acc: 0.9333\n",
      "iter: 773, Tst Acc: 0.6100, Trn Acc: 0.9667\n",
      "iter: 774, Tst Acc: 0.6100, Trn Acc: 0.9667\n",
      "iter: 775, Tst Acc: 0.6100, Trn Acc: 0.9750\n",
      "iter: 776, Tst Acc: 0.6100, Trn Acc: 0.9917\n",
      "iter: 777, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 778, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 779, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 780, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 781, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 782, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 783, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 784, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 785, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 786, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 787, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 788, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 789, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 790, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 791, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 792, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 793, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 794, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 795, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 796, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 797, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 798, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 799, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 800, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 801, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 802, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 803, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 804, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 805, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 806, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 807, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 808, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 809, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 810, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 811, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 812, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 813, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 814, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 815, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 816, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 817, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 818, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 819, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 820, Tst Acc: 0.6160, Trn Acc: 1.0000\n",
      "iter: 821, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 822, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 823, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 824, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 825, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 826, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 827, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 828, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 829, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 830, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 831, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 832, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 833, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 834, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 835, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 836, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 837, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 838, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 839, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 840, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 841, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 842, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 843, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 844, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 845, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 846, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 847, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 848, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 849, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 850, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 851, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 852, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 853, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 854, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 855, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 856, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 857, Tst Acc: 0.6160, Trn Acc: 1.0000\n",
      "iter: 858, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 859, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 860, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 861, Tst Acc: 0.6140, Trn Acc: 1.0000\n",
      "iter: 862, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 863, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 864, Tst Acc: 0.6170, Trn Acc: 1.0000\n",
      "iter: 865, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 866, Tst Acc: 0.6160, Trn Acc: 1.0000\n",
      "iter: 867, Tst Acc: 0.6170, Trn Acc: 1.0000\n",
      "iter: 868, Tst Acc: 0.6170, Trn Acc: 1.0000\n",
      "iter: 869, Tst Acc: 0.6160, Trn Acc: 1.0000\n",
      "iter: 870, Tst Acc: 0.6160, Trn Acc: 1.0000\n",
      "iter: 871, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 872, Tst Acc: 0.6150, Trn Acc: 1.0000\n",
      "iter: 873, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 874, Tst Acc: 0.6130, Trn Acc: 1.0000\n",
      "iter: 875, Tst Acc: 0.6120, Trn Acc: 1.0000\n",
      "iter: 876, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 877, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 878, Tst Acc: 0.6100, Trn Acc: 1.0000\n",
      "iter: 879, Tst Acc: 0.6110, Trn Acc: 1.0000\n",
      "iter: 880, Tst Acc: 0.6080, Trn Acc: 1.0000\n",
      "iter: 881, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 882, Tst Acc: 0.6090, Trn Acc: 1.0000\n",
      "iter: 883, Tst Acc: 0.6080, Trn Acc: 1.0000\n",
      "iter: 884, Tst Acc: 0.6080, Trn Acc: 1.0000\n",
      "iter: 885, Tst Acc: 0.6070, Trn Acc: 1.0000\n",
      "iter: 886, Tst Acc: 0.6070, Trn Acc: 1.0000\n",
      "iter: 887, Tst Acc: 0.6060, Trn Acc: 1.0000\n",
      "iter: 888, Tst Acc: 0.6070, Trn Acc: 1.0000\n",
      "iter: 889, Tst Acc: 0.6060, Trn Acc: 1.0000\n",
      "iter: 890, Tst Acc: 0.6040, Trn Acc: 1.0000\n",
      "iter: 891, Tst Acc: 0.6040, Trn Acc: 1.0000\n",
      "iter: 892, Tst Acc: 0.6030, Trn Acc: 1.0000\n",
      "iter: 893, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 894, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 895, Tst Acc: 0.6010, Trn Acc: 1.0000\n",
      "iter: 896, Tst Acc: 0.6010, Trn Acc: 1.0000\n",
      "iter: 897, Tst Acc: 0.6010, Trn Acc: 1.0000\n",
      "iter: 898, Tst Acc: 0.6010, Trn Acc: 1.0000\n",
      "iter: 899, Tst Acc: 0.6020, Trn Acc: 1.0000\n",
      "iter: 900, Tst Acc: 0.6010, Trn Acc: 1.0000\n",
      "iter: 901, Tst Acc: 0.6010, Trn Acc: 1.0000\n",
      "iter: 902, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 903, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 904, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 905, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 906, Tst Acc: 0.6000, Trn Acc: 1.0000\n",
      "iter: 907, Tst Acc: 0.5980, Trn Acc: 1.0000\n",
      "iter: 908, Tst Acc: 0.5980, Trn Acc: 1.0000\n",
      "iter: 909, Tst Acc: 0.5970, Trn Acc: 1.0000\n",
      "iter: 910, Tst Acc: 0.5970, Trn Acc: 1.0000\n",
      "iter: 911, Tst Acc: 0.5970, Trn Acc: 1.0000\n",
      "iter: 912, Tst Acc: 0.5960, Trn Acc: 1.0000\n",
      "iter: 913, Tst Acc: 0.5950, Trn Acc: 1.0000\n",
      "iter: 914, Tst Acc: 0.5950, Trn Acc: 1.0000\n",
      "iter: 915, Tst Acc: 0.5940, Trn Acc: 1.0000\n",
      "iter: 916, Tst Acc: 0.5940, Trn Acc: 1.0000\n",
      "iter: 917, Tst Acc: 0.5940, Trn Acc: 1.0000\n",
      "iter: 918, Tst Acc: 0.5940, Trn Acc: 1.0000\n",
      "iter: 919, Tst Acc: 0.5930, Trn Acc: 1.0000\n",
      "iter: 920, Tst Acc: 0.5930, Trn Acc: 1.0000\n",
      "iter: 921, Tst Acc: 0.5930, Trn Acc: 1.0000\n",
      "iter: 922, Tst Acc: 0.5920, Trn Acc: 1.0000\n",
      "iter: 923, Tst Acc: 0.5900, Trn Acc: 1.0000\n",
      "iter: 924, Tst Acc: 0.5900, Trn Acc: 1.0000\n",
      "iter: 925, Tst Acc: 0.5880, Trn Acc: 1.0000\n",
      "iter: 926, Tst Acc: 0.5860, Trn Acc: 1.0000\n",
      "iter: 927, Tst Acc: 0.5860, Trn Acc: 1.0000\n",
      "iter: 928, Tst Acc: 0.5840, Trn Acc: 1.0000\n",
      "iter: 929, Tst Acc: 0.5830, Trn Acc: 1.0000\n",
      "iter: 930, Tst Acc: 0.5830, Trn Acc: 1.0000\n",
      "iter: 931, Tst Acc: 0.5830, Trn Acc: 1.0000\n",
      "iter: 932, Tst Acc: 0.5820, Trn Acc: 1.0000\n",
      "iter: 933, Tst Acc: 0.5820, Trn Acc: 1.0000\n",
      "iter: 934, Tst Acc: 0.5810, Trn Acc: 1.0000\n",
      "iter: 935, Tst Acc: 0.5790, Trn Acc: 1.0000\n",
      "iter: 936, Tst Acc: 0.5790, Trn Acc: 1.0000\n",
      "iter: 937, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 938, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 939, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 940, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 941, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 942, Tst Acc: 0.5790, Trn Acc: 1.0000\n",
      "iter: 943, Tst Acc: 0.5790, Trn Acc: 1.0000\n",
      "iter: 944, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 945, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 946, Tst Acc: 0.5770, Trn Acc: 1.0000\n",
      "iter: 947, Tst Acc: 0.5770, Trn Acc: 1.0000\n",
      "iter: 948, Tst Acc: 0.5770, Trn Acc: 1.0000\n",
      "iter: 949, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 950, Tst Acc: 0.5780, Trn Acc: 1.0000\n",
      "iter: 951, Tst Acc: 0.5760, Trn Acc: 1.0000\n",
      "iter: 952, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 953, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 954, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 955, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 956, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 957, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 958, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 959, Tst Acc: 0.5750, Trn Acc: 1.0000\n",
      "iter: 960, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 961, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 962, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 963, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 964, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 965, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 966, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 967, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 968, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 969, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 970, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 971, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 972, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 973, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 974, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 975, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 976, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 977, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 978, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 979, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 980, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 981, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 982, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 983, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 984, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 985, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 986, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 987, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 988, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 989, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 990, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 991, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 992, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 993, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 994, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 995, Tst Acc: 0.5740, Trn Acc: 1.0000\n",
      "iter: 996, Tst Acc: 0.5730, Trn Acc: 1.0000\n",
      "iter: 997, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 998, Tst Acc: 0.5720, Trn Acc: 1.0000\n",
      "iter: 999, Tst Acc: 0.5710, Trn Acc: 1.0000\n",
      "iter: 1000, Tst Acc: 0.5700, Trn Acc: 1.0000\n",
      "iter: 1001, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1002, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1003, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1004, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1005, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1006, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1007, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1008, Tst Acc: 0.5690, Trn Acc: 1.0000\n",
      "iter: 1009, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1010, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1011, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1012, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1013, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1014, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1015, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1016, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1017, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1018, Tst Acc: 0.5670, Trn Acc: 1.0000\n",
      "iter: 1019, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1020, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1021, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1022, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1023, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1024, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1025, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1026, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1027, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1028, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1029, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1030, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1031, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1032, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1033, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1034, Tst Acc: 0.5660, Trn Acc: 1.0000\n",
      "iter: 1035, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1036, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1037, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1038, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1039, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1040, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1041, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1042, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1043, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1044, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1045, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1046, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1047, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1048, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1049, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1050, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1051, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1052, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1053, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1054, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1055, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1056, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1057, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1058, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1059, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1060, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1061, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1062, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1063, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1064, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1065, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1066, Tst Acc: 0.5650, Trn Acc: 1.0000\n",
      "iter: 1067, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1068, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1069, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1070, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1071, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1072, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1073, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1074, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1075, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1076, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1077, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1078, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1079, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1080, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1081, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1082, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1083, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1084, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1085, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1086, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1087, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1088, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1089, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1090, Tst Acc: 0.5640, Trn Acc: 1.0000\n",
      "iter: 1091, Tst Acc: 0.5630, Trn Acc: 1.0000\n",
      "iter: 1092, Tst Acc: 0.5630, Trn Acc: 1.0000\n",
      "iter: 1093, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1094, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1095, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1096, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1097, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1098, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1099, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1100, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1101, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1102, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1103, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1104, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1105, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1106, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1107, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1108, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1109, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1110, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1111, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1112, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1113, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1114, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1115, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1116, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1117, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1118, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1119, Tst Acc: 0.5620, Trn Acc: 1.0000\n",
      "iter: 1120, Tst Acc: 0.5620, Trn Acc: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a26781f2eeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#         print('number of iteration: %d', iter_cnt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0miter_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_sym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_sym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindexs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy_sym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindexs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;31m#         print('Train Accuracy: %.4f', train_accuracy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mtxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4083\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4085\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1295\u001b[0m                 run_metadata):\n\u001b[1;32m   1296\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1358\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_reweight = True\n",
    "init_iter_label = 2\n",
    "init_iter_graph = 7\n",
    "iter_inst = 2\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()\n",
    "\n",
    "# init_train\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(graph = cgraph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('init_iter_label')\n",
    "    for i in range(init_iter_label):\n",
    "        gx, gy = next(label_generator)\n",
    "#         print(gx)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, l, res_l_emd_f = session.run([gl_optimizer, gl_loss, softmax_weights], feed_dict=feed_dict)\n",
    "        print 'iter label', i, l\n",
    "#         print(res_l_emd_f)\n",
    "#         sys.exit(0)\n",
    "\n",
    "#     sys.exit(0)\n",
    "    print('init_iter_graph')\n",
    "    for i in range(init_iter_graph):\n",
    "        gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#         print(list_path[0])\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                   path_sym: list_path, path_id_sym: list_path_id,\n",
    "                   w_path2pair_sym: w_path2pair}\n",
    "        if (use_reweight):\n",
    "            _, l, res_reweight_id, res_reweight, res_rnn_inputs, res_outputs, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, cg_outputs, cg_last_output, s_weight_avg, s_biase_avg], feed_dict=feed_dict)\n",
    "        else:\n",
    "            _, l, res_reweight_id, res_reweight, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, s_weight_avg, s_biase_avg], feed_dict=feed_dict)\n",
    "\n",
    "#         print(res_rnn_inputs[0])\n",
    "#         print(res_reweight)\n",
    "#         print(res_lstm_last_output[0])\n",
    "        print 'iter graph', i, l\n",
    "#         print('reweight[0]:', res_reweight[0])\n",
    "#         sys.exit(0)\n",
    "    \n",
    "    \n",
    "#     sys.exit(0)\n",
    "    \n",
    "    print('init_iter_label')\n",
    "    for i in range(init_iter_label):\n",
    "        gx, gy = next(label_generator)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "                   path_sym: list_path, path_id_sym: list_path_id,\n",
    "                   w_path2pair_sym: w_path2pair}\n",
    "#         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "        print 'iter label', i, l\n",
    "\n",
    "#### Start\n",
    "    iter_cnt = 0\n",
    "    while True:\n",
    "        for _ in range(max_iter):\n",
    "            for _ in range(comp_iter(iter_graph)):\n",
    "                gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "                print(list_path[0])\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "                _, l, res_reweight_id, res_reweight = session.run([g_optimizer, g_loss, reweight_id, reweight], feed_dict=feed_dict)\n",
    "                print 'iter graph', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_inst)):\n",
    "                xs, ys, indexs = next(inst_generator)\n",
    "#                 gx, gy = next(label_generator)\n",
    "                xs = xs.toarray()\n",
    "                feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)}\n",
    "                _, l = session.run([optimizer, l_loss], feed_dict=feed_dict)\n",
    "    #           print 'iter inst', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_label)):\n",
    "                gx, gy = next(label_generator)\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "                _, l = session.run([gl_optimizer, gl_loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        predict_y = tf.argmax(l_y, 1)\n",
    "        correct_prediction = tf.equal(predict_y, tf.argmax(y_sym, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    #     print(accuracy)\n",
    "#         print('number of iteration: %d', iter_cnt)\n",
    "        iter_cnt += 1\n",
    "        train_accuracy = accuracy.eval({x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)})\n",
    "#         print('Train Accuracy: %.4f', train_accuracy)\n",
    "        txs = tx.toarray()\n",
    "        t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)\n",
    "        test_accuracy = accuracy.eval({x_sym: txs, y_sym: ty, g_sym: t_index, gy_sym: t_index.reshape(t_index.shape[0], 1)})\n",
    "        print('iter: %d, Tst Acc: %.4f, Trn Acc: %.4f' %(iter_cnt, test_accuracy, train_accuracy))\n",
    "        \n",
    "#         if (iter_cnt == 5):\n",
    "#             sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00496026,  0.00500135,  0.00504243,  0.0049207 ,  0.00508371,\n",
       "         0.00487118,  0.00503232,  0.00513423,  0.00512543,  0.00511278,\n",
       "         0.00495276,  0.00496932,  0.00509642,  0.00476472,  0.00508202,\n",
       "         0.00513261,  0.00514402,  0.00488825,  0.00513993,  0.00504225,\n",
       "         0.0049523 ,  0.00508264,  0.00510294,  0.0050783 ,  0.00510226,\n",
       "         0.00496483,  0.00512717,  0.00503125,  0.00491823,  0.00499666,\n",
       "         0.00503218,  0.00506654,  0.00481618,  0.00498946,  0.00489985,\n",
       "         0.00475723,  0.00493511,  0.00506777,  0.00507806,  0.0049991 ,\n",
       "         0.00499596,  0.00515256,  0.00484974,  0.0048238 ,  0.00505941,\n",
       "         0.00483398,  0.00483752,  0.00492471,  0.00502468,  0.00476777,\n",
       "         0.0050379 ,  0.00510353,  0.00502782,  0.00506198,  0.00492944,\n",
       "         0.00501779,  0.00505026,  0.00508298,  0.00483345,  0.00514694,\n",
       "         0.00506833,  0.00501812,  0.00487534,  0.00484263,  0.00506357,\n",
       "         0.00490818,  0.00507065,  0.00500399,  0.00495066,  0.00499   ,\n",
       "         0.00487355,  0.00497376,  0.00498765,  0.00495542,  0.0049043 ,\n",
       "         0.0050043 ,  0.00518636,  0.00499181,  0.0051011 ,  0.00515039,\n",
       "         0.00508644,  0.00495242,  0.00508357,  0.00487556,  0.00507548,\n",
       "         0.00498322,  0.00503005,  0.00568577,  0.00489993,  0.00480872,\n",
       "         0.00503724,  0.0050563 ,  0.0048571 ,  0.00502431,  0.0048797 ,\n",
       "         0.00501142,  0.00500673,  0.00496143,  0.00503046,  0.00510918,\n",
       "         0.00500047,  0.00493136,  0.0050557 ,  0.00499628,  0.0052007 ,\n",
       "         0.00492173,  0.00488759,  0.00474988,  0.00511011,  0.00496439,\n",
       "         0.00489418,  0.0050486 ,  0.00481584,  0.00498723,  0.00491892,\n",
       "         0.00495984,  0.00485087,  0.00514758,  0.00497422,  0.00501689,\n",
       "         0.0048694 ,  0.00520414,  0.00491453,  0.00506592,  0.00500151,\n",
       "         0.00512879,  0.0050356 ,  0.0051442 ,  0.00527726,  0.00503876,\n",
       "         0.00498829,  0.00472622,  0.00505534,  0.00485775,  0.00502062,\n",
       "         0.00509845,  0.00496754,  0.00513213,  0.00498726,  0.00487203,\n",
       "         0.00500988,  0.00522336,  0.00492245,  0.00509936,  0.00493318,\n",
       "         0.00506344,  0.00474202,  0.00499739,  0.00493809,  0.00499221,\n",
       "         0.00509367,  0.00490118,  0.0048842 ,  0.00516469,  0.00521361,\n",
       "         0.00498449,  0.00506454,  0.00482399,  0.0048924 ,  0.00496765,\n",
       "         0.00494862,  0.00504681,  0.0051941 ,  0.00496056,  0.00501945,\n",
       "         0.00498026,  0.00491342,  0.005009  ,  0.00502826,  0.00501721,\n",
       "         0.00481298,  0.00485981,  0.00495513,  0.00504775,  0.00514643,\n",
       "         0.00527066,  0.00487078,  0.00480212,  0.00497824,  0.00501305,\n",
       "         0.00504017,  0.00499521,  0.00516052,  0.00504666,  0.00497709,\n",
       "         0.00505974,  0.00473788,  0.00505384,  0.00501693,  0.00513083,\n",
       "         0.00515988,  0.00520042,  0.00503958,  0.00487334,  0.0049438 ,\n",
       "         0.00499534,  0.00499547,  0.00490065,  0.00468878,  0.00494727]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax((np.matmul(res_lstm_last_output, res_s_weight) + res_s_biase).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.98579922e-03,  -1.91092666e-03,   1.13703718e-03,\n",
       "        -1.94945280e-03,  -0.00000000e+00,  -2.10431567e-03,\n",
       "        -4.10453242e-04,   0.00000000e+00,   1.46083732e-03,\n",
       "         0.00000000e+00,   1.93165115e-03,  -0.00000000e+00,\n",
       "         3.66599881e-03,  -4.00959179e-05,  -6.41093543e-03,\n",
       "        -2.04713945e-03,  -3.29010631e-03,   0.00000000e+00,\n",
       "        -0.00000000e+00,   8.90581636e-04,   6.43362349e-04,\n",
       "        -2.79952539e-03,   9.53049865e-04,   1.73583743e-04,\n",
       "        -1.58014998e-03,  -4.64183697e-03,   2.87159951e-03,\n",
       "        -0.00000000e+00,  -8.14037677e-03,  -1.79596210e-03,\n",
       "         0.00000000e+00,   0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_lstm_last_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_l_emd_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-keyword arg after keyword arg (<ipython-input-77-780a68635923>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-780a68635923>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    my_variable = tf.Variable(name=\"my_variable\", [[1,2,3],[4,5,6]], dtype = tf.float32)\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-keyword arg after keyword arg\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "my_variable = tf.Variable(name=\"my_variable\", [[1,2,3],[4,5,6]], dtype = tf.float32)\n",
    "sess.run(my_variable)\n",
    "print(sess.run(my_variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'feature_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-632e4db69670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     'department': ['sports', 'sports', 'gardening', 'gardening']}\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n\u001b[0m\u001b[1;32m      7\u001b[0m         'department', ['sports', 'gardening'])\n\u001b[1;32m      8\u001b[0m \u001b[0mdepartment_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicator_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepartment_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'feature_column'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "features = {\n",
    "    'sales' : [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'gardening', 'gardening']}\n",
    "\n",
    "department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'department', ['sports', 'gardening'])\n",
    "department_column = tf.feature_column.indicator_column(department_column)\n",
    "\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    department_column\n",
    "]\n",
    "\n",
    "inputs = tf.feature_column.input_layer(features, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
