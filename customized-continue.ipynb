{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'blogcatalog'\n",
    "\n",
    "embedding_size = 128\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 0.1\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.x\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.y\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.tx\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.ty\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.graph\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    f = \"/hdd2/graph_embedding/dataset/blogcatalog/trans.{}.{}\".format(DATASET, NAMES[i])\n",
    "    print(f)\n",
    "    OBJECTS.append(cPickle.load(open(f)))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031, 10312)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_iter(iter):\n",
    "    \"\"\"an auxiliary function used for computing the number of iterations given the argument iter.\n",
    "    iter can either be an int or a float.\n",
    "    \"\"\"\n",
    "    if iter >= 1:\n",
    "        return iter\n",
    "    return 1 if random.random() < iter else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters([3,3], [3,4], [4,2])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(str(parameters['l_emd_f_W'].eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, x.shape[1]], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, y.shape[1]], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [None, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [None], name = 'ind')\n",
    "    \n",
    "    path_sym = tf.placeholder(tf.int32, shape = [None, path_size+1], name = 'path')\n",
    "    path_id_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'path_id')\n",
    "    w_path2pair_sym = tf.placeholder(tf.float32, shape = [None, None], name = 'w_path2pair_sym')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# softmax_weights = tf.Variab`le(\n",
    "#     tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "#                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "# softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "# l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_sampled = 64\n",
    "num_ver = max(graph.keys()) + 1\n",
    "vocabulary_size = num_ver\n",
    "n_hidden = 32\n",
    "n_steps = path_size+1 #path length\n",
    "n_input = 128 # embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weight2, _bias2):\n",
    "# #     _X = tf.transpose(_X, [1, 0, 2]) \n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     print('after: tf.reshape(_X, [-1, n_input]) ')\n",
    "#     print(_X.shape)\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    keep_prob = tf.constant(.8)\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple=True)\n",
    "    seqlen = np.ones(batch_size) * (path_size+1)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "#     print(rnn_outputs[:,-1,:].shape)\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "#     print(outputs)\n",
    "    \n",
    "#     lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "#     lstm_cells = lstm_cell_1\n",
    "    # Get LSTM cell output\n",
    "#     outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    \n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = rnn_outputs[:,-1,:]\n",
    "#     scale_output = tf.layers.dense(inputs=lstm_last_output, units=1, activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(lstm_last_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, rnn_outputs[:,-1,:], lstm_last_output\n",
    "#     return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dnn_paths(_X, _weight2, _bias2):\n",
    "# #     _X = tf.transpose(_X, [1, 0, 2]) \n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     print('after: tf.reshape(_X, [-1, n_input]) ')\n",
    "#     print(_X.shape)\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    path_dnn_output = tf.layers.dense(inputs = _X, units = 1, \n",
    "                    activation = tf.nn.softmax, \n",
    "                    kernel_initializer=glorot_uniform_initializer())\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_dnn_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, rnn_outputs[:,-1,:], lstm_last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average_Paths(_X, _weight2, _bias2):\n",
    "    path_avg_output = tf.reduce_mean(_X, axis=1)\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_avg_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, path_avg_output[-1], path_avg_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "use_reweight = False\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "\n",
    "cgraph = tf.Graph()\n",
    "\n",
    "with cgraph.as_default(), tf.device('/gpu:0'):\n",
    "\n",
    "    x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym = create_placeholders()\n",
    "\n",
    "    # word embedding\n",
    "    tf.random_normal_initializer(seed = 1)\n",
    "    tf.set_random_seed(1)\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "    ##\n",
    "    \n",
    "    path_weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size], mean=1.0))\n",
    "    }\n",
    "    path_biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([vocabulary_size]))\n",
    "    }\n",
    "    \n",
    "    s_weight_avg = tf.Variable(tf.random_normal([1, embedding_size]))\n",
    "    s_biase_avg = tf.Variable(tf.random_normal([1]))\n",
    "    \n",
    "#     print('path_sym.shape:')\n",
    "#     print(path_sym.shape)\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, path_sym)\n",
    "#     print('rnn_inputs.shape:')\n",
    "#     print(rnn_inputs.shape)\n",
    "    \n",
    "    if (use_reweight):\n",
    "        reweight, cg_outputs, cg_last_output = Average_Paths(\n",
    "            rnn_inputs, s_weight_avg, s_biase_avg)    \n",
    "        reweight = tf.reshape(reweight, [-1, 1])\n",
    "        reweight_id = tf.matmul(w_path2pair_sym, reweight)\n",
    "    else:\n",
    "        reweight = tf.ones(shape=[100, 1])\n",
    "        reweight_id = tf.ones(shape=[100, 1])\n",
    "    \n",
    "\n",
    "    l_x_hid = tf.layers.dense(inputs = x_sym, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    if use_feature:\n",
    "        l_emd_z = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "        l_f = tf.concat([l_x_hid, l_emd_z], axis = 1)\n",
    "        l_y = tf.layers.dense(inputs = l_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    else:\n",
    "        l_y = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "\n",
    "    l_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_y, labels = y_sym))\n",
    "\n",
    "    if layer_loss and use_feature:\n",
    "        l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_x_hid, labels = y_sym))\n",
    "        l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_emd_z, labels = y_sym))\n",
    "        \n",
    "    # l_rnn = lstm(emb_f, units = xx)\n",
    "    # l_w = tf.layer(l_rnn, units = , activation = softmax)\n",
    "    # g_loss = weighted...\n",
    "\n",
    "    # if neg_samp == 0:\n",
    "    #     pass\n",
    "    # else:\n",
    "#     gw2v_loss = word2vec(l_emd_f, gy_sym, softmax_wecg_last_outputights, softmax_biases)\n",
    "    \n",
    "    g_loss = tf.reduce_mean(reweight_id *\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "                                   inputs = l_emd_f, labels = gy_sym, \n",
    "                                   num_sampled = vocabulary_size, \n",
    "                                   num_classes = vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(l_loss)\n",
    "    \n",
    "    g_optimizer = tf.train.GradientDescentOptimizer(g_learning_rate).minimize(g_loss)\n",
    "    \n",
    "    gl_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "                                   inputs = l_emd_f, labels = gy_sym,\n",
    "                                   num_sampled = vocabulary_size, \n",
    "                                   num_classes = vocabulary_size))\n",
    "    gl_optimizer = tf.train.AdamOptimizer(gl_learning_rate).minimize(gl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(128)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_emd_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_inst():\n",
    "    \"\"\"generator for batches for classification loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    while True:\n",
    "        ind = np.array(np.random.permutation(x.shape[0]), dtype = np.int32)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            j = min(ind.shape[0], i + batch_size)\n",
    "            yield x[ind[i: j]], y[ind[i: j]], ind[i: j]\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label_graph():\n",
    "    \"\"\"generator for batches for label context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    labels, label2inst, not_label = [], dd(list), dd(list)\n",
    "    for i in range(x.shape[0]):\n",
    "        flag = False\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i, j] == 1 and not flag:\n",
    "                labels.append(j)\n",
    "                label2inst[j].append(i)\n",
    "                flag = True\n",
    "            elif y[i, j] == 0:\n",
    "                not_label[j].append(i)\n",
    "\n",
    "    while True:\n",
    "        g, gy = [], []\n",
    "        for _ in range(g_sample_size):\n",
    "            x1 = random.randint(0, x.shape[0] - 1)\n",
    "            label = labels[x1]\n",
    "            if len(label2inst) == 1: continue\n",
    "            x2 = random.choice(label2inst[label])\n",
    "            g.append([x1, x2])\n",
    "            gy.append(1.0)\n",
    "#             for _ in range(neg_samp):\n",
    "#                 g.append([x1, random.choice(not_label[label])])\n",
    "#                 gy.append( - 1.0)\n",
    "        yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_graph():\n",
    "    \"\"\"generator for batches for graph context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_ver = max(graph.keys()) + 1\n",
    "\n",
    "    while True:\n",
    "        ind = np.random.permutation(num_ver)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            g, gy = [], []\n",
    "            list_path = []\n",
    "            list_path_id = []\n",
    "            count_path = 0\n",
    "            row_w_path2pair = 0\n",
    "            j = min(ind.shape[0], i + g_batch_size)\n",
    "            w_path2pair = np.zeros((13000, g_batch_size))\n",
    "            for k in ind[i: j]:\n",
    "                if len(graph[k]) == 0: continue\n",
    "                path = [k]\n",
    "                for _ in range(path_size):\n",
    "                    path.append(random.choice(graph[path[-1]]))\n",
    "                list_path.append(path)\n",
    "                for l in range(len(path)):\n",
    "                    for m in range(l - window_size, l + window_size + 1):\n",
    "                        if m < 0 or m >= len(path): continue\n",
    "                        g.append([path[l], path[m]])\n",
    "                        gy.append(1.0)\n",
    "                        list_path_id.append(count_path)\n",
    "                        w_path2pair[row_w_path2pair, count_path] = 1\n",
    "                        row_w_path2pair += 1\n",
    "                count_path += 1 \n",
    "#                         for _ in range(neg_samp):\n",
    "#                             # if the random number euqals to path[m], the it creates noise!\n",
    "#                             g.append([path[l], random.randint(0, num_ver - 1)])\n",
    "#                             gy.append(- 1.0)\n",
    "            yield (np.array(g, dtype = np.int32), \n",
    "                   np.array(gy, dtype = np.float32), \n",
    "                   np.array(list_path, np.float32), \n",
    "                   np.array(list_path_id, np.float32),\n",
    "                   w_path2pair)\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 5\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gx, gy = next(label_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_iter_label\n",
      "iter label 0 9.88904\n",
      "init_iter_graph\n",
      "iter graph 0 10.2535\n",
      "iter graph 1 10.1882\n",
      "iter graph 2 10.1899\n",
      "iter graph 3 10.2365\n",
      "iter graph 4 10.1667\n",
      "iter graph 5 10.175\n",
      "iter graph 6 10.2001\n",
      "iter: 1, Tst Acc: 0.0473, Trn Acc: 0.0600, max test acc: 0.0473\n",
      "iter: 2, Tst Acc: 0.0766, Trn Acc: 0.1150, max test acc: 0.0766\n",
      "iter: 3, Tst Acc: 0.0765, Trn Acc: 0.0800, max test acc: 0.0766\n",
      "iter: 4, Tst Acc: 0.0871, Trn Acc: 0.1200, max test acc: 0.0871\n",
      "iter: 5, Tst Acc: 0.0979, Trn Acc: 0.1200, max test acc: 0.0979\n",
      "iter: 6, Tst Acc: 0.1112, Trn Acc: 0.2903, max test acc: 0.1112\n",
      "iter: 7, Tst Acc: 0.1169, Trn Acc: 0.1250, max test acc: 0.1169\n",
      "iter: 8, Tst Acc: 0.1256, Trn Acc: 0.2050, max test acc: 0.1256\n",
      "iter: 9, Tst Acc: 0.1318, Trn Acc: 0.2600, max test acc: 0.1318\n",
      "iter: 10, Tst Acc: 0.1379, Trn Acc: 0.3150, max test acc: 0.1379\n",
      "iter: 11, Tst Acc: 0.1754, Trn Acc: 0.5150, max test acc: 0.1754\n",
      "iter: 12, Tst Acc: 0.1905, Trn Acc: 0.3548, max test acc: 0.1905\n",
      "iter: 13, Tst Acc: 0.1927, Trn Acc: 0.5000, max test acc: 0.1927\n",
      "iter: 14, Tst Acc: 0.1929, Trn Acc: 0.4200, max test acc: 0.1929\n",
      "iter: 15, Tst Acc: 0.1931, Trn Acc: 0.5000, max test acc: 0.1931\n",
      "iter: 16, Tst Acc: 0.2034, Trn Acc: 0.4950, max test acc: 0.2034\n",
      "iter: 17, Tst Acc: 0.2076, Trn Acc: 0.5900, max test acc: 0.2076\n",
      "iter: 18, Tst Acc: 0.2150, Trn Acc: 0.5484, max test acc: 0.2150\n",
      "iter: 19, Tst Acc: 0.2183, Trn Acc: 0.5850, max test acc: 0.2183\n",
      "iter: 20, Tst Acc: 0.2196, Trn Acc: 0.5850, max test acc: 0.2196\n",
      "iter: 21, Tst Acc: 0.2215, Trn Acc: 0.5900, max test acc: 0.2215\n",
      "iter: 22, Tst Acc: 0.2369, Trn Acc: 0.5400, max test acc: 0.2369\n",
      "iter: 23, Tst Acc: 0.2438, Trn Acc: 0.6000, max test acc: 0.2438\n",
      "iter: 24, Tst Acc: 0.2481, Trn Acc: 0.6129, max test acc: 0.2481\n",
      "iter: 25, Tst Acc: 0.2500, Trn Acc: 0.5400, max test acc: 0.2500\n",
      "iter: 26, Tst Acc: 0.2494, Trn Acc: 0.6400, max test acc: 0.2500\n",
      "iter: 27, Tst Acc: 0.2494, Trn Acc: 0.6200, max test acc: 0.2500\n",
      "iter: 28, Tst Acc: 0.2494, Trn Acc: 0.7000, max test acc: 0.2500\n",
      "iter: 29, Tst Acc: 0.2501, Trn Acc: 0.6400, max test acc: 0.2501\n",
      "iter: 30, Tst Acc: 0.2502, Trn Acc: 0.6774, max test acc: 0.2502\n",
      "iter: 31, Tst Acc: 0.2490, Trn Acc: 0.7000, max test acc: 0.2502\n",
      "iter: 32, Tst Acc: 0.2465, Trn Acc: 0.6600, max test acc: 0.2502\n",
      "iter: 33, Tst Acc: 0.2465, Trn Acc: 0.6600, max test acc: 0.2502\n",
      "iter: 34, Tst Acc: 0.2468, Trn Acc: 0.6500, max test acc: 0.2502\n",
      "iter: 35, Tst Acc: 0.2494, Trn Acc: 0.6750, max test acc: 0.2502\n",
      "iter: 36, Tst Acc: 0.2481, Trn Acc: 0.7742, max test acc: 0.2502\n",
      "iter: 37, Tst Acc: 0.2487, Trn Acc: 0.7250, max test acc: 0.2502\n",
      "iter: 38, Tst Acc: 0.2471, Trn Acc: 0.7050, max test acc: 0.2502\n",
      "iter: 39, Tst Acc: 0.2474, Trn Acc: 0.7450, max test acc: 0.2502\n",
      "iter: 40, Tst Acc: 0.2468, Trn Acc: 0.7950, max test acc: 0.2502\n",
      "iter: 41, Tst Acc: 0.2446, Trn Acc: 0.7800, max test acc: 0.2502\n",
      "iter: 42, Tst Acc: 0.2454, Trn Acc: 0.7742, max test acc: 0.2502\n",
      "iter: 43, Tst Acc: 0.2450, Trn Acc: 0.7800, max test acc: 0.2502\n",
      "iter: 44, Tst Acc: 0.2440, Trn Acc: 0.8050, max test acc: 0.2502\n",
      "iter: 45, Tst Acc: 0.2452, Trn Acc: 0.7900, max test acc: 0.2502\n",
      "iter: 46, Tst Acc: 0.2442, Trn Acc: 0.8550, max test acc: 0.2502\n",
      "iter: 47, Tst Acc: 0.2443, Trn Acc: 0.7850, max test acc: 0.2502\n",
      "iter: 48, Tst Acc: 0.2443, Trn Acc: 0.9032, max test acc: 0.2502\n",
      "iter: 49, Tst Acc: 0.2443, Trn Acc: 0.8050, max test acc: 0.2502\n",
      "iter: 50, Tst Acc: 0.2451, Trn Acc: 0.8400, max test acc: 0.2502\n",
      "iter: 51, Tst Acc: 0.2439, Trn Acc: 0.8350, max test acc: 0.2502\n",
      "iter: 52, Tst Acc: 0.2436, Trn Acc: 0.8350, max test acc: 0.2502\n",
      "iter: 53, Tst Acc: 0.2442, Trn Acc: 0.8500, max test acc: 0.2502\n",
      "iter: 54, Tst Acc: 0.2439, Trn Acc: 0.8065, max test acc: 0.2502\n",
      "iter: 55, Tst Acc: 0.2433, Trn Acc: 0.8600, max test acc: 0.2502\n",
      "iter: 56, Tst Acc: 0.2435, Trn Acc: 0.8500, max test acc: 0.2502\n",
      "iter: 57, Tst Acc: 0.2428, Trn Acc: 0.8100, max test acc: 0.2502\n",
      "iter: 58, Tst Acc: 0.2418, Trn Acc: 0.8000, max test acc: 0.2502\n",
      "iter: 59, Tst Acc: 0.2411, Trn Acc: 0.8750, max test acc: 0.2502\n",
      "iter: 60, Tst Acc: 0.2406, Trn Acc: 0.9355, max test acc: 0.2502\n",
      "iter: 61, Tst Acc: 0.2398, Trn Acc: 0.8800, max test acc: 0.2502\n",
      "iter: 62, Tst Acc: 0.2394, Trn Acc: 0.8250, max test acc: 0.2502\n",
      "iter: 63, Tst Acc: 0.2391, Trn Acc: 0.8200, max test acc: 0.2502\n",
      "iter: 64, Tst Acc: 0.2386, Trn Acc: 0.8100, max test acc: 0.2502\n",
      "iter: 65, Tst Acc: 0.2374, Trn Acc: 0.8800, max test acc: 0.2502\n",
      "iter: 66, Tst Acc: 0.2375, Trn Acc: 0.8065, max test acc: 0.2502\n",
      "iter: 67, Tst Acc: 0.2372, Trn Acc: 0.8200, max test acc: 0.2502\n"
     ]
    }
   ],
   "source": [
    "use_reweight = False\n",
    "init_iter_label = 20\n",
    "init_iter_graph = 7\n",
    "iter_inst = 1\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()\n",
    "\n",
    "# init_train\n",
    "max_acc = -1\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(graph = cgraph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('init_iter_label')\n",
    "    for i in range(init_iter_label):\n",
    "        gx, gy = next(label_generator)\n",
    "#         print(gx)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, l, res_l_emd_f = session.run([gl_optimizer, gl_loss, softmax_weights], feed_dict=feed_dict)\n",
    "        if (i % 100 == 0):\n",
    "            print 'iter label', i, l\n",
    "#         print(res_l_emd_f)\n",
    "#         sys.exit(0)\n",
    "\n",
    "#     sys.exit(0)\n",
    "    print('init_iter_graph')\n",
    "    for i in range(init_iter_graph):\n",
    "        gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#         print(list_path[0])\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                   path_sym: list_path, path_id_sym: list_path_id,\n",
    "                   w_path2pair_sym: w_path2pair}\n",
    "        if (use_reweight):\n",
    "            _, l, res_reweight_id, res_reweight, res_rnn_inputs, res_outputs, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, cg_outputs, cg_last_output, s_weight_avg, s_biase_avg], feed_dict=feed_dict)\n",
    "        else:\n",
    "            _, l, res_reweight_id, res_reweight, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, s_weight_avg, s_biase_avg], feed_dict=feed_dict)\n",
    "\n",
    "#         print(res_rnn_inputs[0])\n",
    "#         print(res_reweight)\n",
    "#         print(res_lstm_last_output[0])\n",
    "        print 'iter graph', i, l\n",
    "#         print('reweight[0]:', res_reweight[0])\n",
    "#         sys.exit(0)\n",
    "    \n",
    "    \n",
    "#     sys.exit(0)\n",
    "    \n",
    "#     print('init_iter_label')\n",
    "#     for i in range(init_iter_label):\n",
    "#         gx, gy = next(label_generator)\n",
    "#         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "#                    path_sym: list_path, path_id_sym: list_path_id,\n",
    "#                    w_path2pair_sym: w_path2pair}\n",
    "# #         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#         _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#         print 'iter label', i, l\n",
    "\n",
    "#### Start\n",
    "    iter_cnt = 0\n",
    "    while True:\n",
    "        for _ in range(max_iter):\n",
    "            for _ in range(comp_iter(iter_graph)):\n",
    "                gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#                 print(list_path[0])\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "                _, l, res_reweight_id, res_reweight = session.run([g_optimizer, g_loss, reweight_id, reweight], feed_dict=feed_dict)\n",
    "                print 'iter graph', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_inst)):\n",
    "                xs, ys, indexs = next(inst_generator)\n",
    "#                 gx, gy = next(label_generator)\n",
    "                xs = xs.toarray()\n",
    "                feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)}\n",
    "                _, l = session.run([optimizer, l_loss], feed_dict=feed_dict)\n",
    "    #           print 'iter inst', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_label)):\n",
    "                gx, gy = next(label_generator)\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "                _, l = session.run([gl_optimizer, gl_loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        predict_y = tf.argmax(l_y, 1)\n",
    "        correct_prediction = tf.equal(predict_y, tf.argmax(y_sym, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    #     print(accuracy)\n",
    "#         print('number of iteration: %d', iter_cnt)\n",
    "        iter_cnt += 1\n",
    "        train_accuracy = accuracy.eval({x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)})\n",
    "#         print('Train Accuracy: %.4f', train_accuracy)\n",
    "        txs = tx.toarray()\n",
    "        t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)\n",
    "        test_accuracy = accuracy.eval({x_sym: txs, y_sym: ty, g_sym: t_index, gy_sym: t_index.reshape(t_index.shape[0], 1)})\n",
    "        if (test_accuracy > max_acc):\n",
    "            max_acc = test_accuracy\n",
    "        print('iter: %d, Tst Acc: %.4f, Trn Acc: %.4f, max test acc: %.4f' %(iter_cnt, test_accuracy, train_accuracy, max_acc))\n",
    "        \n",
    "#         if (iter_cnt == 5):\n",
    "#             sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00497301], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00496026,  0.00500135,  0.00504243,  0.0049207 ,  0.00508371,\n",
       "         0.00487118,  0.00503232,  0.00513423,  0.00512543,  0.00511278,\n",
       "         0.00495276,  0.00496932,  0.00509642,  0.00476472,  0.00508202,\n",
       "         0.00513261,  0.00514402,  0.00488825,  0.00513993,  0.00504225,\n",
       "         0.0049523 ,  0.00508264,  0.00510294,  0.0050783 ,  0.00510226,\n",
       "         0.00496483,  0.00512717,  0.00503125,  0.00491823,  0.00499666,\n",
       "         0.00503218,  0.00506654,  0.00481618,  0.00498946,  0.00489985,\n",
       "         0.00475723,  0.00493511,  0.00506777,  0.00507806,  0.0049991 ,\n",
       "         0.00499596,  0.00515256,  0.00484974,  0.0048238 ,  0.00505941,\n",
       "         0.00483398,  0.00483752,  0.00492471,  0.00502468,  0.00476777,\n",
       "         0.0050379 ,  0.00510353,  0.00502782,  0.00506198,  0.00492944,\n",
       "         0.00501779,  0.00505026,  0.00508298,  0.00483345,  0.00514694,\n",
       "         0.00506833,  0.00501812,  0.00487534,  0.00484263,  0.00506357,\n",
       "         0.00490818,  0.00507065,  0.00500399,  0.00495066,  0.00499   ,\n",
       "         0.00487355,  0.00497376,  0.00498765,  0.00495542,  0.0049043 ,\n",
       "         0.0050043 ,  0.00518636,  0.00499181,  0.0051011 ,  0.00515039,\n",
       "         0.00508644,  0.00495242,  0.00508357,  0.00487556,  0.00507548,\n",
       "         0.00498322,  0.00503005,  0.00568577,  0.00489993,  0.00480872,\n",
       "         0.00503724,  0.0050563 ,  0.0048571 ,  0.00502431,  0.0048797 ,\n",
       "         0.00501142,  0.00500673,  0.00496143,  0.00503046,  0.00510918,\n",
       "         0.00500047,  0.00493136,  0.0050557 ,  0.00499628,  0.0052007 ,\n",
       "         0.00492173,  0.00488759,  0.00474988,  0.00511011,  0.00496439,\n",
       "         0.00489418,  0.0050486 ,  0.00481584,  0.00498723,  0.00491892,\n",
       "         0.00495984,  0.00485087,  0.00514758,  0.00497422,  0.00501689,\n",
       "         0.0048694 ,  0.00520414,  0.00491453,  0.00506592,  0.00500151,\n",
       "         0.00512879,  0.0050356 ,  0.0051442 ,  0.00527726,  0.00503876,\n",
       "         0.00498829,  0.00472622,  0.00505534,  0.00485775,  0.00502062,\n",
       "         0.00509845,  0.00496754,  0.00513213,  0.00498726,  0.00487203,\n",
       "         0.00500988,  0.00522336,  0.00492245,  0.00509936,  0.00493318,\n",
       "         0.00506344,  0.00474202,  0.00499739,  0.00493809,  0.00499221,\n",
       "         0.00509367,  0.00490118,  0.0048842 ,  0.00516469,  0.00521361,\n",
       "         0.00498449,  0.00506454,  0.00482399,  0.0048924 ,  0.00496765,\n",
       "         0.00494862,  0.00504681,  0.0051941 ,  0.00496056,  0.00501945,\n",
       "         0.00498026,  0.00491342,  0.005009  ,  0.00502826,  0.00501721,\n",
       "         0.00481298,  0.00485981,  0.00495513,  0.00504775,  0.00514643,\n",
       "         0.00527066,  0.00487078,  0.00480212,  0.00497824,  0.00501305,\n",
       "         0.00504017,  0.00499521,  0.00516052,  0.00504666,  0.00497709,\n",
       "         0.00505974,  0.00473788,  0.00505384,  0.00501693,  0.00513083,\n",
       "         0.00515988,  0.00520042,  0.00503958,  0.00487334,  0.0049438 ,\n",
       "         0.00499534,  0.00499547,  0.00490065,  0.00468878,  0.00494727]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax((np.matmul(res_lstm_last_output, res_s_weight) + res_s_biase).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.98579922e-03,  -1.91092666e-03,   1.13703718e-03,\n",
       "        -1.94945280e-03,  -0.00000000e+00,  -2.10431567e-03,\n",
       "        -4.10453242e-04,   0.00000000e+00,   1.46083732e-03,\n",
       "         0.00000000e+00,   1.93165115e-03,  -0.00000000e+00,\n",
       "         3.66599881e-03,  -4.00959179e-05,  -6.41093543e-03,\n",
       "        -2.04713945e-03,  -3.29010631e-03,   0.00000000e+00,\n",
       "        -0.00000000e+00,   8.90581636e-04,   6.43362349e-04,\n",
       "        -2.79952539e-03,   9.53049865e-04,   1.73583743e-04,\n",
       "        -1.58014998e-03,  -4.64183697e-03,   2.87159951e-03,\n",
       "        -0.00000000e+00,  -8.14037677e-03,  -1.79596210e-03,\n",
       "         0.00000000e+00,   0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_lstm_last_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_l_emd_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-keyword arg after keyword arg (<ipython-input-77-780a68635923>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-780a68635923>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    my_variable = tf.Variable(name=\"my_variable\", [[1,2,3],[4,5,6]], dtype = tf.float32)\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-keyword arg after keyword arg\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "my_variable = tf.Variable(name=\"my_variable\", [[1,2,3],[4,5,6]], dtype = tf.float32)\n",
    "sess.run(my_variable)\n",
    "print(sess.run(my_variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'feature_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-632e4db69670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     'department': ['sports', 'sports', 'gardening', 'gardening']}\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n\u001b[0m\u001b[1;32m      7\u001b[0m         'department', ['sports', 'gardening'])\n\u001b[1;32m      8\u001b[0m \u001b[0mdepartment_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicator_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepartment_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'feature_column'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "features = {\n",
    "    'sales' : [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'gardening', 'gardening']}\n",
    "\n",
    "department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'department', ['sports', 'gardening'])\n",
    "department_column = tf.feature_column.indicator_column(department_column)\n",
    "\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    department_column\n",
    "]\n",
    "\n",
    "inputs = tf.feature_column.input_layer(features, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
