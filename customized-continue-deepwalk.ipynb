{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'blogcatalog'\n",
    "\n",
    "embedding_size = 128\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 0.1\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.x\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.y\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.tx\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.ty\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.graph\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    f = \"/hdd2/graph_embedding/dataset/blogcatalog/trans.{}.{}\".format(DATASET, NAMES[i])\n",
    "    print(f)\n",
    "    OBJECTS.append(cPickle.load(open(f)))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031, 10312)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_iter(iter):\n",
    "    \"\"\"an auxiliary function used for computing the number of iterations given the argument iter.\n",
    "    iter can either be an int or a float.\n",
    "    \"\"\"\n",
    "    if iter >= 1:\n",
    "        return iter\n",
    "    return 1 if random.random() < iter else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38732192  0.12907903  0.46030581]\n",
      " [ 0.05552411  0.5345757   1.12406611]\n",
      " [ 0.40012899  0.12259372 -0.88986868]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters([3,3], [3,4], [4,2])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(str(parameters['l_emd_f_W'].eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, x.shape[1]], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, y.shape[1]], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [None, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [None], name = 'ind')\n",
    "    \n",
    "    path_sym = tf.placeholder(tf.int32, shape = [batch_size, path_size+1], name = 'path')\n",
    "    path_id_sym = tf.placeholder(tf.int32, shape = [batch_size, ], name = 'path_id')\n",
    "    w_path2pair_sym = tf.placeholder(tf.float32, shape = [batch_size, None], name = 'w_path2pair_sym')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# softmax_weights = tf.Variab`le(\n",
    "#     tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "#                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "# softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "# l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_sampled = 64\n",
    "# num_ver = max(graph.keys()) + 1\n",
    "# vocabulary_size = num_ver\n",
    "vocabulary_size = v + 1\n",
    "n_hidden = 32\n",
    "n_steps = path_size+1 #path length\n",
    "n_input = 128 # embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average_Paths(_X, _weight2, _bias2):\n",
    "    path_avg_output = tf.reduce_mean(_X, axis=1)\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_avg_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, path_avg_output[-1], path_avg_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # def build():\n",
    "# \"\"\"\n",
    "# Builds the model.\n",
    "# \"\"\"\n",
    "# np.random.seed(1)\n",
    "# random.seed(1)\n",
    "# tf.random_normal_initializer(seed = 1)\n",
    "\n",
    "# cgraph = tf.Graph()\n",
    "\n",
    "# with cgraph.as_default(), tf.device('/gpu:0'):\n",
    "\n",
    "#     x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym = create_placeholders()\n",
    "\n",
    "#     # word embedding\n",
    "#     tf.random_normal_initializer(seed = 1)\n",
    "#     tf.set_random_seed(1)\n",
    "#     embeddings = tf.Variable(\n",
    "#         tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "#     softmax_weights = tf.Variable(\n",
    "#         tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "#     softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "#     l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "#     ##\n",
    "    \n",
    "#     path_weights = {\n",
    "#         'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])),\n",
    "#         'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size], mean=1.0))\n",
    "#     }\n",
    "#     path_biases = {\n",
    "#         'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "#         'out': tf.Variable(tf.random_normal([vocabulary_size]))\n",
    "#     }\n",
    "    \n",
    "#     s_weight_avg = tf.Variable(tf.random_normal([1, embedding_size]))\n",
    "#     s_biase_avg = tf.Variable(tf.random_normal([1]))\n",
    "    \n",
    "# #     print('path_sym.shape:')\n",
    "# #     print(path_sym.shape)\n",
    "#     rnn_inputs = tf.nn.embedding_lookup(embeddings, path_sym)\n",
    "# #     print('rnn_inputs.shape:')\n",
    "# #     print(rnn_inputs.shape)\n",
    "    \n",
    "#     if (use_reweight):\n",
    "#         reweight, cg_outputs, cg_last_output = Average_Paths(\n",
    "#             rnn_inputs, s_weight_avg, s_biase_avg)    \n",
    "#         reweight = tf.reshape(reweight, [-1, 1])\n",
    "#         reweight_id = tf.matmul(w_path2pair_sym, reweight)\n",
    "#     else:\n",
    "#         reweight = tf.ones(shape=[w_path2pair_sym.shape[0], 1])\n",
    "#         reweight_id = tf.ones(shape=[path_sym.shape[0], 1])\n",
    "    \n",
    "\n",
    "#     l_x_hid = tf.layers.dense(inputs = x_sym, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#     if use_feature:\n",
    "#         l_emd_z = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#         l_f = tf.concat([l_x_hid, l_emd_z], axis = 1)\n",
    "#         l_y = tf.layers.dense(inputs = l_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#     else:\n",
    "#         l_y = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "\n",
    "#     l_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_y, labels = y_sym))\n",
    "\n",
    "#     if layer_loss and use_feature:\n",
    "#         l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_x_hid, labels = y_sym))\n",
    "#         l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_emd_z, labels = y_sym))\n",
    "        \n",
    "#     # l_rnn = lstm(emb_f, units = xx)\n",
    "#     # l_w = tf.layer(l_rnn, units = , activation = softmax)\n",
    "#     # g_loss = weighted...\n",
    "\n",
    "#     # if neg_samp == 0:\n",
    "#     #     pass\n",
    "#     # else:\n",
    "# #     gw2v_loss = word2vec(l_emd_f, gy_sym, softmax_wecg_last_outputights, softmax_biases)\n",
    "    \n",
    "#     g_loss = tf.reduce_mean(reweight_id *\n",
    "#         tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "#                                    inputs = l_emd_f, labels = gy_sym, \n",
    "#                                    num_sampled = vocabulary_size, \n",
    "#                                    num_classes = vocabulary_size))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate).minimize(l_loss)\n",
    "    \n",
    "#     g_optimizer = tf.train.GradientDescentOptimizer(g_learning_rate).minimize(g_loss)\n",
    "    \n",
    "#     gl_loss = tf.reduce_mean(\n",
    "#         tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "#                                    inputs = l_emd_f, labels = gy_sym,\n",
    "#                                    num_sampled = vocabulary_size, \n",
    "#                                    num_classes = vocabulary_size))\n",
    "#     gl_optimizer = tf.train.AdamOptimizer(gl_learning_rate).minimize(gl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "\n",
    "cgraph = tf.Graph()\n",
    "\n",
    "with cgraph.as_default(), tf.device('/gpu:0'):\n",
    "\n",
    "    x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym = create_placeholders()\n",
    "\n",
    "    # word embedding\n",
    "    tf.random_normal_initializer(seed = 1)\n",
    "    tf.set_random_seed(1)\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "   \n",
    "    l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases,\n",
    "                                   inputs = l_emd_f, labels = gy_sym, \n",
    "                                   num_sampled = 5, \n",
    "                                   num_classes = vocabulary_size))    \n",
    "    g_optimizer = tf.train.GradientDescentOptimizer(g_learning_rate).minimize(g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list = ['/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0']\n",
    "deepwalk_generator = gen_graph_from_path_collection(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_textfiles(file_list):\n",
    "    count = -1\n",
    "    num_path = 0\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                num_path += 1\n",
    "                for l in line.split():\n",
    "                    count = max(count, int(l))\n",
    "    return count, num_path\n",
    "\n",
    "def gen_graph_from_path_collection(file_list):\n",
    "    num_line = 0\n",
    "    list_path = []\n",
    "    g, gy = [], []\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                path = [int(t) for t in line.split()]\n",
    "                list_path.append(path)\n",
    "                for l in range(len(path)):\n",
    "                    for m in range(l - window_size, l + window_size + 1):\n",
    "                        if m < 0 or m >= len(path): continue\n",
    "                        g.append([path[l], path[m]])\n",
    "                        gy.append(1.0)\n",
    "                if (num_line % 1000 == 0):\n",
    "                    tmp_g, tmp_gy, tmp_path = g, gy, list_path\n",
    "                    list_path = []\n",
    "                    g, gy = [], []\n",
    "                    yield (np.array(tmp_g, dtype = np.int32),\n",
    "                           np.array(tmp_path, dtype = np.float32))\n",
    "                else:\n",
    "                    pass\n",
    "                num_line += 1\n",
    "            yield (np.array(tmp_g, dtype = np.int32),\n",
    "                           np.array(tmp_path, dtype = np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v, num_path = count_textfiles(file_list)\n",
    "vocabulary_size = v + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "representation_size = 128\n",
    "deepwalk_generator = gen_graph_from_path_collection(file_list)\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0/825\n"
     ]
    }
   ],
   "source": [
    "# unsupervised word2vec\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "tot = 0\n",
    "with tf.Session(graph = cgraph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    while (True):\n",
    "        deepwalk_generator = gen_graph_from_path_collection(file_list)\n",
    "        init_iter_deepwalk = num_path / 1000 + 1\n",
    "        for i in range(init_iter_deepwalk):\n",
    "            gx, list_path = next(deepwalk_generator)\n",
    "            feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "            _, l  = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "\n",
    "            if (i % 100 == 0):\n",
    "                print('iter: %d/%d' %(i, init_iter_deepwalk))\n",
    "                feed_dict={g_sym: np.array(range(0,v+1)), gy_sym: np.array(range(0,v+1)).reshape(v+1,1)}\n",
    "                res_l_emd_f = session.run(l_emd_f, feed_dict=feed_dict)\n",
    "                embedding_filename_tmp = '/hdd2/graph_embedding/customized/blog_embeddings.%d.txt' %(tot)\n",
    "                np.savetxt(embedding_filename_tmp, res_l_emd_f)\n",
    "            tot += 1\n",
    "        \n",
    "    \n",
    "    feed_dict={g_sym: np.array(range(0,v+1)), gy_sym: np.array(range(0,v+1)).reshape(v+1,1)}\n",
    "    res_l_emd_f = session.run(l_emd_f, feed_dict=feed_dict)\n",
    "    np.savetxt(embedding_filename, res_l_emd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4921 4921]\n",
      "(730000, 2)\n"
     ]
    }
   ],
   "source": [
    "gx, list_path = next(deepwalk_generator)\n",
    "print(gx[0])\n",
    "print(gx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824960"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hdd2/graph_embedding/customized/blog_embeddings.800.txt'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "    embeddings = np.loadtxt(embeddings_file)\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "    features_matrix = embeddings\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Results, using embeddings of dimensionality', 128)\n",
      "-------------------\n",
      "('Train percent:', 0.1)\n",
      "('Shuffle #1:   ', {'micro': 0.16959423180179489, 'macro': 0.027160346415189187})\n",
      "('Average score:', {'micro': 0.16959423180179489, 'macro': 0.027160346415189187})\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.800.txt'\n",
    "scoring(embedding_filename, matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sio.loadmat(matfile)\n",
    "A = mat['network']\n",
    "labels_matrix = mat['group']\n",
    "labels_count = labels_matrix.shape[1]\n",
    "mlb = MultiLabelBinarizer(range(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(embedding_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10311, 128)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10312, 39)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 5\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_iter_label\n",
      "iter label 0 9.8893\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-72384ff03ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print(gx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mg_sym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy_sym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_l_emd_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgl_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'iter label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_REGISTERED_EXPANSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfeed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m       raise TypeError('Feed argument %r has invalid type %r'\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m   [variables](../../api_docs/python/state_ops.md#Variable), [queues](../../api_docs/python/io_ops.md#QueueBase),\n\u001b[1;32m   1123\u001b[0m   and [readers](../../api_docs/python/io_ops.md#ReaderBase). It is important to release\n\u001b[0;32m-> 1124\u001b[0;31m   \u001b[0mthese\u001b[0m \u001b[0mresources\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mno\u001b[0m \u001b[0mlonger\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTo\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meither\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m   invoke the [`close()`](#Session.close) method on the session, or use\n\u001b[1;32m   1126\u001b[0m   \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0mare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explicit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_nesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;34m\"\"\"Closes an `InteractiveSession`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explicit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mengine\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconnect\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0musing\u001b[0m \u001b[0man\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m       \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mThe\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mlaunched\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdescribed\u001b[0m \u001b[0mabove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mproto\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconfigure\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_reweight = True\n",
    "init_iter_label = 20\n",
    "init_iter_graph = 7\n",
    "iter_inst = 1\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()\n",
    "\n",
    "# init_train\n",
    "max_acc = -1\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(graph = cgraph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('init_iter_label')\n",
    "    for i in range(init_iter_label):\n",
    "        gx, gy = next(label_generator)\n",
    "#         print(gx)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, l, res_l_emd_f = session.run([gl_optimizer, gl_loss, softmax_weights], feed_dict=feed_dict)\n",
    "        if (i % 100 == 0):\n",
    "            print 'iter label', i, l\n",
    "#         print(res_l_emd_f)\n",
    "#         sys.exit(0)\n",
    "\n",
    "#     sys.exit(0)\n",
    "    print('init_iter_graph')\n",
    "    for i in range(init_iter_graph):\n",
    "        gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#         print(list_path[0])\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                   path_sym: list_path, path_id_sym: list_path_id,\n",
    "                   w_path2pair_sym: w_path2pair}\n",
    "        if (use_reweight):\n",
    "            _, l, res_reweight_id, res_reweight, res_rnn_inputs, res_outputs, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, cg_outputs, cg_last_output], feed_dict=feed_dict)\n",
    "        else:\n",
    "            _, l, res_reweight_id, res_reweight, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs], feed_dict=feed_dict)\n",
    "\n",
    "#         print(res_rnn_inputs[0])\n",
    "#         print(res_reweight)\n",
    "#         print(res_lstm_last_output[0])\n",
    "        print 'iter graph', i, l\n",
    "#         print('reweight[0]:', res_reweight[0])\n",
    "#         sys.exit(0)\n",
    "    \n",
    "    \n",
    "#     sys.exit(0)\n",
    "    \n",
    "#     print('init_iter_label')\n",
    "#     for i in range(init_iter_label):\n",
    "#         gx, gy = next(label_generator)\n",
    "#         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "#                    path_sym: list_path, path_id_sym: list_path_id,\n",
    "#                    w_path2pair_sym: w_path2pair}\n",
    "# #         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#         _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#         print 'iter label', i, l\n",
    "\n",
    "#### Start\n",
    "    iter_cnt = 0\n",
    "    while True:\n",
    "        for _ in range(max_iter):\n",
    "            for _ in range(comp_iter(iter_graph)):\n",
    "                gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#                 print(list_path[0])\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "                _, l, res_reweight_id, res_reweight = session.run([g_optimizer, g_loss, reweight_id, reweight], feed_dict=feed_dict)\n",
    "                print 'iter graph', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_inst)):\n",
    "                xs, ys, indexs = next(inst_generator)\n",
    "#                 gx, gy = next(label_generator)\n",
    "                xs = xs.toarray()\n",
    "                feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)}\n",
    "                _, l = session.run([optimizer, l_loss], feed_dict=feed_dict)\n",
    "    #           print 'iter inst', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_label)):\n",
    "                gx, gy = next(label_generator)\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "                _, l = session.run([gl_optimizer, gl_loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        predict_y = tf.argmax(l_y, 1)\n",
    "        correct_prediction = tf.equal(predict_y, tf.argmax(y_sym, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    #     print(accuracy)\n",
    "#         print('number of iteration: %d', iter_cnt)\n",
    "        iter_cnt += 1\n",
    "        train_accuracy = accuracy.eval({x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)})\n",
    "#         print('Train Accuracy: %.4f', train_accuracy)\n",
    "        txs = tx.toarray()\n",
    "        t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)\n",
    "        test_accuracy = accuracy.eval({x_sym: txs, y_sym: ty, g_sym: t_index, gy_sym: t_index.reshape(t_index.shape[0], 1)})\n",
    "        if (test_accuracy > max_acc):\n",
    "            max_acc = test_accuracy\n",
    "        print('iter: %d, Tst Acc: %.4f, Trn Acc: %.4f, max test acc: %.4f' %(iter_cnt, test_accuracy, train_accuracy, max_acc))\n",
    "        \n",
    "#         if (iter_cnt == 5):\n",
    "#             sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00497301], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00496026,  0.00500135,  0.00504243,  0.0049207 ,  0.00508371,\n",
       "         0.00487118,  0.00503232,  0.00513423,  0.00512543,  0.00511278,\n",
       "         0.00495276,  0.00496932,  0.00509642,  0.00476472,  0.00508202,\n",
       "         0.00513261,  0.00514402,  0.00488825,  0.00513993,  0.00504225,\n",
       "         0.0049523 ,  0.00508264,  0.00510294,  0.0050783 ,  0.00510226,\n",
       "         0.00496483,  0.00512717,  0.00503125,  0.00491823,  0.00499666,\n",
       "         0.00503218,  0.00506654,  0.00481618,  0.00498946,  0.00489985,\n",
       "         0.00475723,  0.00493511,  0.00506777,  0.00507806,  0.0049991 ,\n",
       "         0.00499596,  0.00515256,  0.00484974,  0.0048238 ,  0.00505941,\n",
       "         0.00483398,  0.00483752,  0.00492471,  0.00502468,  0.00476777,\n",
       "         0.0050379 ,  0.00510353,  0.00502782,  0.00506198,  0.00492944,\n",
       "         0.00501779,  0.00505026,  0.00508298,  0.00483345,  0.00514694,\n",
       "         0.00506833,  0.00501812,  0.00487534,  0.00484263,  0.00506357,\n",
       "         0.00490818,  0.00507065,  0.00500399,  0.00495066,  0.00499   ,\n",
       "         0.00487355,  0.00497376,  0.00498765,  0.00495542,  0.0049043 ,\n",
       "         0.0050043 ,  0.00518636,  0.00499181,  0.0051011 ,  0.00515039,\n",
       "         0.00508644,  0.00495242,  0.00508357,  0.00487556,  0.00507548,\n",
       "         0.00498322,  0.00503005,  0.00568577,  0.00489993,  0.00480872,\n",
       "         0.00503724,  0.0050563 ,  0.0048571 ,  0.00502431,  0.0048797 ,\n",
       "         0.00501142,  0.00500673,  0.00496143,  0.00503046,  0.00510918,\n",
       "         0.00500047,  0.00493136,  0.0050557 ,  0.00499628,  0.0052007 ,\n",
       "         0.00492173,  0.00488759,  0.00474988,  0.00511011,  0.00496439,\n",
       "         0.00489418,  0.0050486 ,  0.00481584,  0.00498723,  0.00491892,\n",
       "         0.00495984,  0.00485087,  0.00514758,  0.00497422,  0.00501689,\n",
       "         0.0048694 ,  0.00520414,  0.00491453,  0.00506592,  0.00500151,\n",
       "         0.00512879,  0.0050356 ,  0.0051442 ,  0.00527726,  0.00503876,\n",
       "         0.00498829,  0.00472622,  0.00505534,  0.00485775,  0.00502062,\n",
       "         0.00509845,  0.00496754,  0.00513213,  0.00498726,  0.00487203,\n",
       "         0.00500988,  0.00522336,  0.00492245,  0.00509936,  0.00493318,\n",
       "         0.00506344,  0.00474202,  0.00499739,  0.00493809,  0.00499221,\n",
       "         0.00509367,  0.00490118,  0.0048842 ,  0.00516469,  0.00521361,\n",
       "         0.00498449,  0.00506454,  0.00482399,  0.0048924 ,  0.00496765,\n",
       "         0.00494862,  0.00504681,  0.0051941 ,  0.00496056,  0.00501945,\n",
       "         0.00498026,  0.00491342,  0.005009  ,  0.00502826,  0.00501721,\n",
       "         0.00481298,  0.00485981,  0.00495513,  0.00504775,  0.00514643,\n",
       "         0.00527066,  0.00487078,  0.00480212,  0.00497824,  0.00501305,\n",
       "         0.00504017,  0.00499521,  0.00516052,  0.00504666,  0.00497709,\n",
       "         0.00505974,  0.00473788,  0.00505384,  0.00501693,  0.00513083,\n",
       "         0.00515988,  0.00520042,  0.00503958,  0.00487334,  0.0049438 ,\n",
       "         0.00499534,  0.00499547,  0.00490065,  0.00468878,  0.00494727]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax((np.matmul(res_lstm_last_output, res_s_weight) + res_s_biase).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.98579922e-03,  -1.91092666e-03,   1.13703718e-03,\n",
       "        -1.94945280e-03,  -0.00000000e+00,  -2.10431567e-03,\n",
       "        -4.10453242e-04,   0.00000000e+00,   1.46083732e-03,\n",
       "         0.00000000e+00,   1.93165115e-03,  -0.00000000e+00,\n",
       "         3.66599881e-03,  -4.00959179e-05,  -6.41093543e-03,\n",
       "        -2.04713945e-03,  -3.29010631e-03,   0.00000000e+00,\n",
       "        -0.00000000e+00,   8.90581636e-04,   6.43362349e-04,\n",
       "        -2.79952539e-03,   9.53049865e-04,   1.73583743e-04,\n",
       "        -1.58014998e-03,  -4.64183697e-03,   2.87159951e-03,\n",
       "        -0.00000000e+00,  -8.14037677e-03,  -1.79596210e-03,\n",
       "         0.00000000e+00,   0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_lstm_last_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iter_test():\n",
    "    for i in xrange(2):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_generator = iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "res_i = next(test_generator)\n",
    "print(res_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: [u'stream0']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'logits']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
