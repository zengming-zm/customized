{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from multiprocessing import cpu_count\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "import word2vec\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = ['/hdd2/graph_embedding/customized/tmp/citeseer.embeddings.walks']\n",
    "file_list = ['/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0']\n",
    "dataset = genfromtxt(file_list[0], delimiter=' ')\n",
    "\n",
    "def get_num_vacabulary(dataset):\n",
    "    word_count = 0\n",
    "    for d in dataset:\n",
    "        word_count = max(word_count, max(d))\n",
    "    return int(word_count)\n",
    "\n",
    "vocabulary_size = get_num_vacabulary(dataset) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = dataset.flatten()\n",
    "words = [str(int(w)) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common words (+UNK)', [('4838', 196735), ('175', 195187), ('4373', 169490), ('8156', 147968), ('1225', 137221)])\n",
      "('Sample data', [1193, 2874, 225, 457, 19, 34, 5662, 37, 3046, 29])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = []\n",
    "#     count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "#         else:\n",
    "#             index = 0  # dictionary['UNK']\n",
    "        data.append(index)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_labeled_instant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b291967202f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrn_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munserialized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trn_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munserialized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trn_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munserialized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tst_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munserialized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tst_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labeled_instant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_labeled_instant' is not defined"
     ]
    }
   ],
   "source": [
    "## Read the data, the trn_x is the position in the embedding matrix\n",
    "data_splited_filename = '/hdd2/graph_embedding/customized/blogcatalog_splited_p10.pickle'\n",
    "with open(data_splited_filename, 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n",
    "trn_x, trn_y, tst_x, tst_y = (unserialized_data['trn_x'], unserialized_data['trn_y'], unserialized_data['tst_x'], unserialized_data['tst_y'])\n",
    "\n",
    "trn_idx, trn_y, tst_idx, tst_y, = get_labeled_instant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 20 and skip_window = 10:\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "path_index = 0\n",
    "batch_path_size = 2\n",
    "batch_size = batch_path_size * 400\n",
    "def generate_batch(batch_path_size, num_skips, skip_window):\n",
    "    global path_index\n",
    "    batch_size = batch_path_size * 400\n",
    "    batch = []\n",
    "    labels = []\n",
    "    path_list = []\n",
    "    path_index_list = []\n",
    "    w_p2p = np.zeros([batch_size, batch_path_size])\n",
    "#     span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=batch_path_size)\n",
    "    for i in range(batch_path_size):\n",
    "        len_path = len(dataset[path_index])\n",
    "        path_list.append(dataset[path_index])\n",
    "        for l in range(skip_window, len_path - skip_window): # [ skip_window target skip_window ]\n",
    "            for m in range(l - skip_window, l + skip_window + 1):\n",
    "                if m < 0 or m >= len_path or m == l: \n",
    "                    continue\n",
    "                batch.append(dictionary[str(int(dataset[path_index][l]))])\n",
    "                labels.append(dictionary[str(int(dataset[path_index][m]))])\n",
    "                path_index_list.append(i)\n",
    "                \n",
    "        w_p2p[i * 400 : (i+1) * 400, i] = 1\n",
    "                \n",
    "        path_index = (path_index + 1) % len(dataset)\n",
    "    return (np.asarray(batch, dtype = np.int32), \n",
    "            np.asarray(labels, dtype = np.int32).reshape([len(labels), 1]),\n",
    "            np.asarray(path_list, dtype=np.float32),\n",
    "            np.asarray(path_index_list, dtype=np.float32),\n",
    "            w_p2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce Gensim weights initialization\n",
    "def seeded_vector(seed_string, vector_size):\n",
    "    \"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\n",
    "    # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "    once = np.random.RandomState(hash(seed_string) & 0xffffffff)\n",
    "    return (once.rand(vector_size) - 0.5) / vector_size\n",
    "\n",
    "features_list = []\n",
    "for idx in range(vocabulary_size):\n",
    "    str_node = reverse_dictionary[idx]\n",
    "    features_list.append(seeded_vector(str_node + str(1), 128))\n",
    "features_matrix = np.asarray(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average_Paths(X, _weight, _bias):\n",
    "    path_avg_output = tf.reduce_mean(X, axis=1)\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight, tf.transpose(path_avg_output)) + _bias)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, path_avg_output[-1], path_avg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_feature = False\n",
    "use_reweight = True\n",
    "labeled_size = trn_idx.shape[0]\n",
    "batch_path_size = 2\n",
    "batch_size = batch_path_size * 400\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 10 # How many words to consider left and right.\n",
    "num_skips = 20 # How many times to reuse an input to generate a label.\n",
    "num_class = 39\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    w_path2pair = tf.placeholder(tf.float32, shape = [batch_size, batch_path_size])\n",
    "    path_dataset= tf.placeholder(tf.int32, shape = [batch_path_size, len(dataset[0])])\n",
    "    path_id = tf.placeholder(tf.int32, shape = [None, ])\n",
    "        \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(features_matrix, dtype=tf.float32)\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "    weight_avg = tf.Variable(\n",
    "        tf.truncated_normal([1, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biase_avg = tf.Variable(tf.zeros([1]))\n",
    "    if (use_reweight):\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, path_dataset)\n",
    "        reweight_each_path, cg_outputs, cg_last_output = Average_Paths(\n",
    "            rnn_inputs, weight_avg, biase_avg) \n",
    "#         reweight_each_path = tf.reshape(reweight, [-1, 1])\n",
    "        reweight_each_pair = tf.matmul(w_path2pair, tf.transpose(reweight_each_path))\n",
    "    else:\n",
    "#         reweight_each_path = tf.ones(shape=[batch_path_size, 1])\n",
    "        reweight_each_pair = tf.ones(shape=[batch_size, 1])\n",
    "    \n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean( reweight_each_pair *\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.25).minimize(loss, global_step=global_step)\n",
    "\n",
    "    \n",
    "#     clf_lr = 0.25\n",
    "    clf_idx = tf.placeholder(tf.int32, shape=[None])\n",
    "    clf_y = tf.placeholder(tf.float32, shape=[None, trn_y.shape[1]])\n",
    "    \n",
    "    embed_x = tf.nn.embedding_lookup(embeddings, clf_idx)\n",
    "\n",
    "# #   for datasets in the deepwalk, multi-class\n",
    "    logit_y = tf.layers.dense(inputs = embed_x, units = clf_y.shape[1], \n",
    "                              activation=tf.nn.sigmoid, kernel_initializer=glorot_uniform_initializer())\n",
    "    clf_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits = logit_y, labels = clf_y))\n",
    "\n",
    "# # for 1 class\n",
    "#     feature_dataset = tf.placeholder(tf.float32, shape=[None, trn_f.shape[1]])\n",
    "#     l_x_hid = tf.layers.dense(inputs = feature_dataset, units = clf_y.shape[1],\n",
    "#                               activation = tf.nn.softmax, kernel_initializer = glorot_uniform_initializer())\n",
    "#     if (use_feature):\n",
    "#         logit_emd = tf.layers.dense(inputs = embed_x, units = clf_y.shape[1],\n",
    "#                                     activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#         l_f = tf.concat([l_x_hid, logit_emd], axis = 1)\n",
    "#         logit_y = tf.layers.dense(inputs = l_f, units = clf_y.shape[1],\n",
    "#                                   activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#     else:\n",
    "#     #   for datasets in the icml paper, single-class\n",
    "#         logit_y = tf.layers.dense(inputs = embed_x, units = clf_y.shape[1], \n",
    "#                                   activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "        \n",
    "#     clf_loss = tf.reduce_mean(\n",
    "#         tf.nn.softmax_cross_entropy_with_logits(logits = logit_y, labels = clf_y))\n",
    "    \n",
    "    clf_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25).minimize(clf_loss)\n",
    "    \n",
    "    \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "#     norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "#     normalized_embeddings = embeddings / norm\n",
    "#     valid_embeddings = tf.nn.embedding_lookup(\n",
    "#         normalized_embeddings, valid_dataset)\n",
    "#     similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5560\n"
     ]
    }
   ],
   "source": [
    "def running_test():\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        batch_data, batch_labels, batch_path, batch_path_id, w_p2p = generate_batch(\n",
    "            batch_path_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, \n",
    "                     train_labels : batch_labels, \n",
    "                     path_dataset : batch_path,\n",
    "                     path_id : batch_path_id,\n",
    "                     w_path2pair : w_p2p}\n",
    "        _, l, res_embed = session.run([optimizer, loss, embed], feed_dict=feed_dict)\n",
    "        print('loss: %.4f' %(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average embedding loss at step 0: 2.520512, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.474854, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.308920, lr 0.250000\n"
     ]
    }
   ],
   "source": [
    "use_feature = False\n",
    "emb_steps = 10000 #50000001\n",
    "clf_steps = 1000\n",
    "def running():\n",
    "    total_step = 0\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        while (True):\n",
    "            average_emb_loss = 0\n",
    "            average_clf_loss = 0\n",
    "            for step in range(emb_steps):\n",
    "                batch_data, batch_labels, batch_path, batch_path_id, w_p2p = generate_batch(\n",
    "                    batch_path_size, num_skips, skip_window)\n",
    "                feed_dict = {train_dataset : batch_data,\n",
    "                             train_labels : batch_labels, \n",
    "                             path_dataset : batch_path,\n",
    "                             path_id : batch_path_id,\n",
    "                             w_path2pair : w_p2p}\n",
    "                _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                average_emb_loss += l\n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_emb_loss = average_emb_loss / 2000.0\n",
    "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                    print('Average embedding loss at step %d: %f, lr %f' % (step, average_emb_loss, 0.25))\n",
    "                    average_emb_loss = 0\n",
    "\n",
    "            for step in range(clf_steps):\n",
    "                if (use_feature):\n",
    "                    # for datasets in the icml paper\n",
    "                    feed_dict = {clf_idx : trn_idx, clf_y : trn_y, feature_dataset : trn_f}\n",
    "                else:\n",
    "                    feed_dict = {clf_idx : trn_idx, clf_y : trn_y}\n",
    "\n",
    "                _, l = session.run([clf_optimizer, clf_loss], feed_dict=feed_dict)\n",
    "                average_clf_loss += l\n",
    "                if step % 1000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_clf_loss = average_clf_loss / 1000.0\n",
    "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                    print('Average classification loss at step %d: %f, lr %f' % (step, average_clf_loss, 0.25))\n",
    "                    average_clf_loss = 0\n",
    "\n",
    "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    #         y_p = tf.argmax(logit_y, 1)\n",
    "    #         feed_dict = {clf_x : tst_x, clf_y : tst_y}\n",
    "    #         _, l, res_logit_y = session.run([clf_optimizer, clf_loss, logit_y], feed_dict=feed_dict)\n",
    "    #         y_true = np.argmax(tst_y,1)\n",
    "    #         print(\"micro: \", f1_score(y_true, res_pred_y, average='micro'))\n",
    "    #         print(\"macro: \", f1_score(y_true, res_pred_y, average='macro'))\n",
    "\n",
    "            res_y_pred = tf.argmax(logit_y, 1)\n",
    "            res_y_true = tf.argmax(clf_y, 1)\n",
    "            if (use_feature):\n",
    "                trn_y_pred = res_y_pred.eval({clf_idx : trn_idx, clf_y: trn_y, feature_dataset : trn_f})\n",
    "                trn_y_ture = res_y_true.eval({clf_idx : trn_idx, clf_y: trn_y, feature_dataset : trn_f})\n",
    "                tst_y_pred = res_y_pred.eval({clf_idx : tst_idx, clf_y: tst_y, feature_dataset : tst_f})\n",
    "                tst_y_ture = res_y_true.eval({clf_idx : tst_idx, clf_y: tst_y, feature_dataset : tst_f})\n",
    "                print(\"Epoch %d, trn acc %.6f acc %.6f:\" % (total_step,\n",
    "                                                            accuracy_score(trn_y_ture, trn_y_pred.flatten()),\n",
    "                                                            accuracy_score(tst_y_ture, tst_y_pred.flatten())))\n",
    "            else:\n",
    "                pass\n",
    "    #             trn_y_pred = res_y_pred.eval({clf_idx : trn_idx, clf_y: trn_y})\n",
    "    #             trn_y_ture = res_y_true.eval({clf_idx : trn_idx, clf_y: trn_y})\n",
    "    #             tst_y_pred = res_y_pred.eval({clf_idx : tst_idx, clf_y: tst_y})\n",
    "    #             tst_y_ture = res_y_true.eval({clf_idx : tst_idx, clf_y: tst_y})\n",
    "    #             print(trn_y_pred.tolist())\n",
    "    #             print(trn_y_ture.tolist())\n",
    "\n",
    "\n",
    "            if total_step % 1 == 0:\n",
    "                embedding_filename = '/hdd2/graph_embedding/customized/results/exp_blogcatalog_semi_avg1/blog_embeddings_iter%d.txt' %total_step\n",
    "                not_normal_embeddings = embeddings.eval()\n",
    "                ordered_embeddings = [not_normal_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "                np.savetxt(embedding_filename, ordered_embeddings)\n",
    "\n",
    "            total_step += 1\n",
    "\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        not_normal_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
