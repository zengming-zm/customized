{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'citeseer'\n",
    "\n",
    "embedding_size = 50\n",
    "learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 1\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 0.01\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    OBJECTS.append(cPickle.load(open(\"/hdd2/graph_embedding/planetoid/data/trans.{}.{}\".format(DATASET, NAMES[i]))))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38732192  0.12907903  0.46030581]\n",
      " [ 0.05552411  0.5345757   1.12406611]\n",
      " [ 0.40012899  0.12259372 -0.88986874]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters([3,3], [3,4], [4,2])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(str(parameters['l_emd_f_W'].eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_supervised_cost(a_C, a_G):\n",
    "    \"\"\"\n",
    "    Computes the supervised cost\n",
    "    \n",
    "    Arguments:  todo\n",
    "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_supervised -- scalar that you compute using equation 1 above.\n",
    "    \"\"\"\n",
    "    \n",
    "    return J_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_embedding_weight():\n",
    "    \"\"\"\n",
    "    Computes the embedding weights\n",
    "    \n",
    "    Return:\n",
    "    reweight -- vector for each path\n",
    "    \"\"\"\n",
    "    \n",
    "    return reweight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_embedding_cost():\n",
    "    \"\"\"\n",
    "    Computes the embedding cost\n",
    "    \n",
    "    Returns:\n",
    "    J_embedding -- scalar that you compute using equation x above.\n",
    "    \"\"\"\n",
    "    \n",
    "    return J_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, num_ver], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, 2], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [200], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [200, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [num_ver], name = 'ind')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6)\n",
      "(200, 50)\n",
      "Tensor(\"ind_17:0\", shape=(3327,), dtype=int32)\n",
      "<tf.Variable 'Variable_66:0' shape=(3327, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_67:0' shape=(3327, 50) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(l_x_hid.shape)\n",
    "print(l_emd_f.shape)\n",
    "print(ind_sym)\n",
    "print(embeddings)\n",
    "print(softmax_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3327), Dimension(50)])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "num_sampled = 64\n",
    "num_ver = max(graph.keys()) + 1\n",
    "vocabulary_size = num_ver\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# with graph.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "\n",
    "\n",
    "\n",
    "# word embedding\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocabulary_size, embedding_size, input_length = path_size))\n",
    "\n",
    "##\n",
    "\n",
    "l_x_hid = tf.layers.dense(inputs = x_sym, units = y.shape[1], activation = tf.nn.softmax)\n",
    "if use_feature:\n",
    "#     l_emd_f = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax)\n",
    "    l_y = tf.concat([l_x_hid, l_emd_f], axis = 1)\n",
    "    l_y = tf.layers.dense(inputs = l_y, units = y.shape[1], activation = tf.nn.softmax)\n",
    "else:\n",
    "    l_y = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_y, labels = y_sym))\n",
    "\n",
    "if layer_loss and use_feature:\n",
    "    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_x_hid, labels = y_sym))\n",
    "    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_emd_f, labels = y_sym))\n",
    "\n",
    "if neg_samp == 0:\n",
    "    pass\n",
    "else:\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, \n",
    "                                   inputs = l_emd_f, labels = gy_sym, \n",
    "                                   num_sampled = num_sampled, \n",
    "                                   num_classes = vocabulary_size))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2.0).minimize(loss)\n",
    "\n",
    "g_optimizer = tf.train.AdamOptimizer(2.0).minimize(g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_inst():\n",
    "    \"\"\"generator for batches for classification loss.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        ind = np.array(np.random.permutation(x.shape[0]), dtype = np.int32)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            j = min(ind.shape[0], i + batch_size)\n",
    "            yield x[ind[i: j]], y[ind[i: j]], ind[i: j]\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label_graph():\n",
    "    \"\"\"generator for batches for label context loss.\n",
    "    \"\"\"\n",
    "    labels, label2inst, not_label = [], dd(list), dd(list)\n",
    "    for i in range(x.shape[0]):\n",
    "        flag = False\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i, j] == 1 and not flag:\n",
    "                labels.append(j)\n",
    "                label2inst[j].append(i)\n",
    "                flag = True\n",
    "            elif y[i, j] == 0:\n",
    "                not_label[j].append(i)\n",
    "\n",
    "    while True:\n",
    "        g, gy = [], []\n",
    "        for _ in range(g_sample_size):\n",
    "            x1 = random.randint(0, x.shape[0] - 1)\n",
    "            label = labels[x1]\n",
    "            if len(label2inst) == 1: continue\n",
    "            x2 = random.choice(label2inst[label])\n",
    "            g.append([x1, x2])\n",
    "            gy.append(1.0)\n",
    "            for _ in range(neg_samp):\n",
    "                g.append([x1, random.choice(not_label[label])])\n",
    "                gy.append( - 1.0)\n",
    "        yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_graph():\n",
    "    \"\"\"generator for batches for graph context loss.\n",
    "    \"\"\"\n",
    "\n",
    "    num_ver = max(graph.keys()) + 1\n",
    "\n",
    "    while True:\n",
    "        ind = np.random.permutation(num_ver)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            g, gy = [], []\n",
    "            j = min(ind.shape[0], i + g_batch_size)\n",
    "            for k in ind[i: j]:\n",
    "                if len(graph[k]) == 0: continue\n",
    "                path = [k]\n",
    "                for _ in range(path_size):\n",
    "                    path.append(random.choice(graph[path[-1]]))\n",
    "                for l in range(len(path)):\n",
    "                    for m in range(l - window_size, l + window_size + 1):\n",
    "                        if m < 0 or m >= len(path): continue\n",
    "                        g.append([path[l], path[m]])\n",
    "                        gy.append(1.0)\n",
    "                        for _ in range(neg_samp):\n",
    "                            # if the random number euqals to path[m], the it creates noise!\n",
    "                            g.append([path[l], random.randint(0, num_ver - 1)])\n",
    "                            gy.append(- 1.0)\n",
    "            yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_train(self, init_iter_label, init_iter_graph):\n",
    "    \"\"\"pre-training of graph embeddings.\n",
    "    init_iter_label (int): # iterations for optimizing label context loss.\n",
    "    init_iter_graph (int): # iterations for optimizing graph context loss.\n",
    "    \"\"\"\n",
    "    for i in range(init_iter_label):\n",
    "        gx, gy = next(label_generator)\n",
    "        loss = g_fn(gx, gy)\n",
    "        print 'iter label', i, loss\n",
    "\n",
    "    for i in range(init_iter_graph):\n",
    "        gx, gy = next(graph_generator)\n",
    "        loss = g_fn(gx, gy)\n",
    "        print 'iter graph', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 2000\n",
    "init_iter_graph = 70\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gx, gy = next(label_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict={X: gx[:, 0], Y: gx[:, 1]}\n",
    "# feed_dict={X: minibatch_X, Y: minibatch_Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(200,1)}\n",
    "    _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5630565"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gx[:, 0].reshape(200, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [1,2,3,4,5]\n",
    "random.choice(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
