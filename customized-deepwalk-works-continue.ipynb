{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzeng/anaconda2/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from multiprocessing import cpu_count\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "import word2vec\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "    embeddings = np.loadtxt(embeddings_file)\n",
    "    \n",
    "    ## for original deepwalk\n",
    "    #model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    \n",
    "    ## for external word2vec lib\n",
    "#     model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "    features_matrix = embeddings\n",
    "\n",
    "    # original code\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "\n",
    "    # use other word2vec lib\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "#     rand_list = range(len(model.vocab)-1)\n",
    "# #     random.shuffle(rand_list)\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in rand_list])\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix, random_state = 1))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'citeseer'\n",
    "\n",
    "embedding_size = 128\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 10\n",
    "path_size = 10\n",
    "\n",
    "\n",
    "# use_feature = True\n",
    "# update_emb = True\n",
    "# layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = ['/hdd2/graph_embedding/customized/tmp/citeseer.embeddings.walks']\n",
    "file_list = ['/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0']\n",
    "dataset = genfromtxt(file_list[0], delimiter=' ')\n",
    "\n",
    "def get_num_vacabulary(dataset):\n",
    "    word_count = 0\n",
    "    for d in dataset:\n",
    "        word_count = max(word_count, max(d))\n",
    "    return int(word_count)\n",
    "\n",
    "vocabulary_size = get_num_vacabulary(dataset) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dataset.flatten()\n",
    "words = [str(int(w)) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10312"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1614.,  5573.,  4732.,  1159.,   448.,  5704.,  5382.,  1008.,\n",
       "        5395.,  6958.,  1320.,  4139.,  6814.,  5404.,  7988.,  8384.,\n",
       "       10139.,  6248.,  4226.,  2829.,  9090.,  8456.,  6892.,   738.,\n",
       "        3898.,  6958.,  3048.,  6796.,  7636.,  4104.,  7450.,  1225.,\n",
       "        6061.,   457.,  8968.,    35.,  1141.,  4996.,  1453.,  2979.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [('4838', 196735), ('175', 195187), ('4373', 169490), ('8156', 147968), ('1225', 137221)]\n",
      "Sample data [1193, 2874, 225, 457, 19, 34, 5661, 37, 3046, 29]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = []\n",
    "#     count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "#         else:\n",
    "#             index = 0  # dictionary['UNK']\n",
    "        data.append(index)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = [row.nonzero()[1][0] for row in labels_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "mat = sio.loadmat(matfile)\n",
    "A = mat['network']\n",
    "graph = sparse2graph(A)\n",
    "labels_matrix = mat['group']\n",
    "\n",
    "def split_dataset(matfile, vocabulary_size, dictionary):\n",
    "    # 1. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group'].toarray()\n",
    "    \n",
    "    # node id list\n",
    "    node_id_list = np.asarray([dictionary[str(node)] for node in range(vocabulary_size)], dtype=np.int32)\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(node_id_list, labels_matrix, random_state = 1))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size]\n",
    "            y_train = y[:training_size, :]\n",
    "\n",
    "            X_test = X[training_size:]\n",
    "            y_test = y[training_size:, :]\n",
    "            \n",
    "            trn_idx = np.array([int(dictionary[str(idx)]) for idx in range(training_size)])\n",
    "            tst_idx = np.array([int(dictionary[str(idx)]) for idx in range(training_size, X.shape[0])])\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test, trn_idx, tst_idx\n",
    "\n",
    "trn_x, trn_y, tst_x, tst_y, trn_idx, tst_idx = split_dataset(matfile, vocabulary_size, dictionary)\n",
    "data_splited_filename = '/hdd2/graph_embedding/customized/blogcatalog_splited_p10.pickle'\n",
    "data_trn_tst = {'trn_x' : trn_x,\n",
    "                'trn_y' : trn_y,\n",
    "                'tst_x' : tst_x,\n",
    "                'tst_y' : tst_y,\n",
    "                'trn_idx' : trn_idx,\n",
    "                'tst_idx' : tst_idx}\n",
    "with open(data_splited_filename, 'wb') as handle:\n",
    "    pickle.dump(data_trn_tst, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data, the trn_x is the position in the embedding matrix\n",
    "with open(data_splited_filename, 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n",
    "trn_x, trn_y, tst_x, tst_y, trn_idx, tst_idx = (unserialized_data['trn_x'], \n",
    "                                                unserialized_data['trn_y'], \n",
    "                                                unserialized_data['tst_x'], \n",
    "                                                unserialized_data['tst_y'],\n",
    "                                                unserialized_data['trn_idx'],\n",
    "                                                unserialized_data['tst_idx'])\n",
    "\n",
    "trn_idx, trn_y, tst_idx, tst_y, = get_labeled_instant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 20 and skip_window = 10:\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "path_index = 0\n",
    "batch_path_size = 2\n",
    "batch_size = batch_path_size * 400\n",
    "def generate_batch(batch_path_size, num_skips, skip_window):\n",
    "    global path_index\n",
    "#     assert batch_size % num_skips == 0\n",
    "#     print(num_skips, skip_window)\n",
    "#     assert num_skips <= 2 * skip_window\n",
    "#     batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "#     labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    batch_size = batch_path_size * 400\n",
    "    batch = []\n",
    "    labels = []\n",
    "    path_list = []\n",
    "    path_index_list = []\n",
    "    w_p2p = np.zeros([batch_size, batch_path_size])\n",
    "#     span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=batch_path_size)\n",
    "    for i in range(batch_path_size):\n",
    "        len_path = len(dataset[path_index])\n",
    "        path_list.append(dataset[path_index])\n",
    "        for l in range(skip_window, len_path - skip_window): # [ skip_window target skip_window ]\n",
    "            for m in range(l - skip_window, l + skip_window + 1):\n",
    "                if m < 0 or m >= len_path or m == l: \n",
    "                    continue\n",
    "                batch.append(dictionary[str(int(dataset[path_index][l]))])\n",
    "                labels.append(dictionary[str(int(dataset[path_index][m]))])\n",
    "                path_index_list.append(i)\n",
    "                \n",
    "        w_p2p[i * 400 : (i+1) * 400, i] = 1\n",
    "                \n",
    "        path_index = (path_index + 1) % len(dataset)\n",
    "    return (np.asarray(batch, dtype = np.int32), \n",
    "            np.asarray(labels, dtype = np.int32).reshape([len(labels), 1]),\n",
    "            np.asarray(path_list, dtype=np.float32),\n",
    "            np.asarray(path_index_list, dtype=np.float32),\n",
    "            w_p2p)\n",
    "\n",
    "# print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for batch_path_size, num_skips, skip_window in [(2, 20, 10)]:\n",
    "    data_index = 0\n",
    "    batch, labels, batch_path, batch_path_id, w_p2p = generate_batch(\n",
    "        batch_path_size=batch_path_size, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print(len(batch))\n",
    "#     print('    batch node id:', [reverse_dictionary[bi] for bi in batch])\n",
    "#     print('    node position:')\n",
    "#     print(batch.tolist())\n",
    "#     print(labels)\n",
    "#     print('    labels:', [reverse_dictionary[li] for li in labels.reshape(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # inject trained embedding\n",
    "# model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "# features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "# # make the order consistant to the current one\n",
    "# reordered_list = np.asarray([int(reverse_dictionary[idx]) for idx in range(len(model.vocab)-1)])\n",
    "# reordered_features_matrix = features_matrix[reordered_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce Gensim weights initialization\n",
    "def seeded_vector(seed_string, vector_size):\n",
    "    \"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\n",
    "    # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "    once = np.random.RandomState(hash(seed_string) & 0xffffffff)\n",
    "    return (once.rand(vector_size) - 0.5) / vector_size\n",
    "\n",
    "features_list = []\n",
    "for idx in range(vocabulary_size):\n",
    "    str_node = reverse_dictionary[idx]\n",
    "    features_list.append(seeded_vector(str_node + str(1), 128))\n",
    "features_matrix = np.asarray(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.x\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.y\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.tx\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.ty\n",
      "/hdd2/graph_embedding/planetoid/data/trans.citeseer.graph\n"
     ]
    }
   ],
   "source": [
    "# # for datasets in the icml paper\n",
    "# file_path = '/hdd2/graph_embedding/planetoid/data/'\n",
    "# def get_labeled_instant():\n",
    "#     DATASET = 'citeseer'\n",
    "#     NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "#     OBJECTS = []\n",
    "#     for i in range(len(NAMES)):\n",
    "#         f = file_path + 'trans.{}.{}'.format(DATASET, NAMES[i])\n",
    "#         print(f)\n",
    "#         OBJECTS.append(cPickle.load(open(f)))\n",
    "#     x, y, tx, ty, graph = tuple(OBJECTS)\n",
    "    \n",
    "#     trn_idx = np.array([int(dictionary[str(idx)]) for idx in range(x.shape[0])])\n",
    "# #     trn_idx = trn_idx.reshape(trn_idx.shape[0], 1)\n",
    "    \n",
    "#     tst_idx = np.array([int(dictionary[str(idx)]) for idx in range(x.shape[0], x.shape[0]+tx.shape[0])])\n",
    "# #     tst_idx = tst_idx.reshape(tst_idx.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     return trn_idx, y, tst_idx, ty, x.toarray(), tx.toarray()\n",
    "\n",
    "# trn_idx, trn_y, tst_idx, tst_y, trn_f, tst_f = get_labeled_instant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average_Paths(X, _weight, _bias):\n",
    "    path_avg_output = tf.reduce_mean(X, axis=1)\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight, tf.transpose(path_avg_output)) + _bias)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, path_avg_output[-1], path_avg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_feature = False\n",
    "use_reweight = True\n",
    "labeled_size = trn_idx.shape[0]\n",
    "batch_path_size = 2\n",
    "batch_size = batch_path_size * 400\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 10 # How many words to consider left and right.\n",
    "num_skips = 20 # How many times to reuse an input to generate a label.\n",
    "num_class = 39\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    w_path2pair = tf.placeholder(tf.float32, shape = [batch_size, batch_path_size])\n",
    "    path_dataset= tf.placeholder(tf.int32, shape = [batch_path_size, len(dataset[0])])\n",
    "    path_id = tf.placeholder(tf.int32, shape = [None, ])\n",
    "        \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(features_matrix, dtype=tf.float32)\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "    weight_avg = tf.Variable(\n",
    "        tf.truncated_normal([1, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biase_avg = tf.Variable(tf.zeros([1]))\n",
    "    if (use_reweight):\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, path_dataset)\n",
    "        reweight_each_path, cg_outputs, cg_last_output = Average_Paths(\n",
    "            rnn_inputs, weight_avg, biase_avg) \n",
    "#         reweight_each_path = tf.reshape(reweight, [-1, 1])\n",
    "        reweight_each_pair = tf.matmul(w_path2pair, tf.transpose(reweight_each_path))\n",
    "    else:\n",
    "#         reweight_each_path = tf.ones(shape=[batch_path_size, 1])\n",
    "        reweight_each_pair = tf.ones(shape=[batch_size, 1])\n",
    "    \n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean( reweight_each_pair *\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.25).minimize(loss, global_step=global_step)\n",
    "\n",
    "    \n",
    "#     clf_lr = 0.25\n",
    "    clf_idx = tf.placeholder(tf.int32, shape=[None])\n",
    "    clf_y = tf.placeholder(tf.float32, shape=[None, trn_y.shape[1]])\n",
    "    \n",
    "    embed_x = tf.nn.embedding_lookup(embeddings, clf_idx)\n",
    "\n",
    "# #   for datasets in the deepwalk, multi-class\n",
    "    logit_y = tf.layers.dense(inputs = embed_x, units = clf_y.shape[1], \n",
    "                              activation=tf.nn.sigmoid, kernel_initializer=glorot_uniform_initializer())\n",
    "    clf_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits = logit_y, labels = clf_y))\n",
    "\n",
    "# # for 1 class\n",
    "#     feature_dataset = tf.placeholder(tf.float32, shape=[None, trn_f.shape[1]])\n",
    "#     l_x_hid = tf.layers.dense(inputs = feature_dataset, units = clf_y.shape[1],\n",
    "#                               activation = tf.nn.softmax, kernel_initializer = glorot_uniform_initializer())\n",
    "#     if (use_feature):\n",
    "#         logit_emd = tf.layers.dense(inputs = embed_x, units = clf_y.shape[1],\n",
    "#                                     activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#         l_f = tf.concat([l_x_hid, logit_emd], axis = 1)\n",
    "#         logit_y = tf.layers.dense(inputs = l_f, units = clf_y.shape[1],\n",
    "#                                   activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "#     else:\n",
    "#     #   for datasets in the icml paper, single-class\n",
    "#         logit_y = tf.layers.dense(inputs = embed_x, units = clf_y.shape[1], \n",
    "#                                   activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "        \n",
    "#     clf_loss = tf.reduce_mean(\n",
    "#         tf.nn.softmax_cross_entropy_with_logits(logits = logit_y, labels = clf_y))\n",
    "    \n",
    "    clf_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25).minimize(clf_loss)\n",
    "    \n",
    "    \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "#     norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "#     normalized_embeddings = embeddings / norm\n",
    "#     valid_embeddings = tf.nn.embedding_lookup(\n",
    "#         normalized_embeddings, valid_dataset)\n",
    "#     similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5560\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    batch_data, batch_labels, batch_path, batch_path_id, w_p2p = generate_batch(\n",
    "        batch_path_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, \n",
    "                 train_labels : batch_labels, \n",
    "                 path_dataset : batch_path,\n",
    "                 path_id : batch_path_id,\n",
    "                 w_path2pair : w_p2p}\n",
    "    _, l, res_embed = session.run([optimizer, loss, embed], feed_dict=feed_dict)\n",
    "    print('loss: %.4f' %(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6156'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['6156']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 23)\t1.0\n",
      "  (0, 24)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(labels_matrix[6156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average embedding loss at step 0: 2.520512, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.474854, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.308920, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.234404, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.190951, lr 0.250000\n",
      "Average classification loss at step 0: 0.956387, lr 0.250000\n",
      "Average embedding loss at step 0: 2.128681, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.144123, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.130839, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.117866, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.108785, lr 0.250000\n",
      "Average classification loss at step 0: 0.843484, lr 0.250000\n",
      "Average embedding loss at step 0: 2.095018, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.095617, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.090446, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.085864, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.082721, lr 0.250000\n",
      "Average classification loss at step 0: 0.785485, lr 0.250000\n",
      "Average embedding loss at step 0: 2.084022, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.075150, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.073834, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.072802, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.069242, lr 0.250000\n",
      "Average classification loss at step 0: 0.756580, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036169, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.067005, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.064538, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.063710, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.061595, lr 0.250000\n",
      "Average classification loss at step 0: 0.740563, lr 0.250000\n",
      "Average embedding loss at step 0: 2.079051, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.058516, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.058706, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.058026, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.057198, lr 0.250000\n",
      "Average classification loss at step 0: 0.730560, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060606, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.054741, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.052999, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.053808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.053722, lr 0.250000\n",
      "Average classification loss at step 0: 0.723982, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062632, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.051572, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.051552, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.051172, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.050984, lr 0.250000\n",
      "Average classification loss at step 0: 0.719222, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056039, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.048788, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.049339, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.049131, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.048901, lr 0.250000\n",
      "Average classification loss at step 0: 0.715757, lr 0.250000\n",
      "Average embedding loss at step 0: 1.979979, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.047385, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.047145, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.046727, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.047469, lr 0.250000\n",
      "Average classification loss at step 0: 0.713377, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009363, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.046818, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.046964, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.046307, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.046618, lr 0.250000\n",
      "Average classification loss at step 0: 0.710689, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049501, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.045733, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.044387, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.045201, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.044113, lr 0.250000\n",
      "Average classification loss at step 0: 0.709499, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057938, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.044405, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.043770, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.044119, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.043818, lr 0.250000\n",
      "Average classification loss at step 0: 0.707500, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065976, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.043077, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.043255, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.043449, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.043737, lr 0.250000\n",
      "Average classification loss at step 0: 0.706505, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038244, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.042510, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.042043, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.042866, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.042950, lr 0.250000\n",
      "Average classification loss at step 0: 0.705799, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060110, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.042821, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.042331, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.041793, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.041328, lr 0.250000\n",
      "Average classification loss at step 0: 0.704684, lr 0.250000\n",
      "Average embedding loss at step 0: 2.050473, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.041699, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.040972, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.042777, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.041091, lr 0.250000\n",
      "Average classification loss at step 0: 0.703545, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013572, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.041661, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.040788, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.041078, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.040776, lr 0.250000\n",
      "Average classification loss at step 0: 0.702972, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041262, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.040049, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.040298, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.038997, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.040474, lr 0.250000\n",
      "Average classification loss at step 0: 0.702564, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049664, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.040248, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.039136, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.039777, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.040475, lr 0.250000\n",
      "Average classification loss at step 0: 0.702341, lr 0.250000\n",
      "Average embedding loss at step 0: 2.050827, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.039550, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.037246, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.040489, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.038424, lr 0.250000\n",
      "Average classification loss at step 0: 0.701987, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025055, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.039170, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.037033, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.039191, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.040146, lr 0.250000\n",
      "Average classification loss at step 0: 0.701626, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021317, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.038378, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.038351, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.039339, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.038647, lr 0.250000\n",
      "Average classification loss at step 0: 0.700804, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023462, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.037395, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.037137, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.037259, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.038395, lr 0.250000\n",
      "Average classification loss at step 0: 0.700145, lr 0.250000\n",
      "Average embedding loss at step 0: 2.087036, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.038278, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.037880, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.037593, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.038002, lr 0.250000\n",
      "Average classification loss at step 0: 0.699783, lr 0.250000\n",
      "Average embedding loss at step 0: 2.067074, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.038439, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.039341, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.037576, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.038229, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042836, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.037075, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.036681, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.036704, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.035575, lr 0.250000\n",
      "Average classification loss at step 0: 0.698701, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022087, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.035703, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.036910, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035142, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.036345, lr 0.250000\n",
      "Average classification loss at step 0: 0.698629, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044132, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.037022, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.035131, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.036148, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034825, lr 0.250000\n",
      "Average classification loss at step 0: 0.698494, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030578, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.036359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.035538, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035499, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.035822, lr 0.250000\n",
      "Average classification loss at step 0: 0.698364, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031356, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.034747, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.035248, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035257, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034966, lr 0.250000\n",
      "Average classification loss at step 0: 0.698233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039618, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.036806, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034712, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035419, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034559, lr 0.250000\n",
      "Average classification loss at step 0: 0.698127, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029223, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.037095, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.035888, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035535, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.035860, lr 0.250000\n",
      "Average classification loss at step 0: 0.698046, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061348, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.035277, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.035428, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035445, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034882, lr 0.250000\n",
      "Average classification loss at step 0: 0.697975, lr 0.250000\n",
      "Average embedding loss at step 0: 2.081711, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.035592, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034945, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035664, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034478, lr 0.250000\n",
      "Average classification loss at step 0: 0.697874, lr 0.250000\n",
      "Average embedding loss at step 0: 2.053184, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.034390, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.035613, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.033751, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.035051, lr 0.250000\n",
      "Average classification loss at step 0: 0.697676, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987234, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.034946, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034399, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.034033, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.033810, lr 0.250000\n",
      "Average classification loss at step 0: 0.697301, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028603, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.035734, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034069, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.034171, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.033739, lr 0.250000\n",
      "Average classification loss at step 0: 0.696994, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025773, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.033122, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032539, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.034022, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.035341, lr 0.250000\n",
      "Average classification loss at step 0: 0.696722, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010560, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.033873, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034725, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.033540, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.033426, lr 0.250000\n",
      "Average classification loss at step 0: 0.696509, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985949, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032418, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.033423, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.035405, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.033246, lr 0.250000\n",
      "Average classification loss at step 0: 0.696369, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039589, lr 0.250000\n",
      "Average classification loss at step 0: 0.696267, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052305, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.033671, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034173, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032988, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034414, lr 0.250000\n",
      "Average classification loss at step 0: 0.696201, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019107, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032418, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.033200, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032921, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032706, lr 0.250000\n",
      "Average classification loss at step 0: 0.696119, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055956, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032810, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032696, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032475, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.034793, lr 0.250000\n",
      "Average classification loss at step 0: 0.696024, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042064, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.034087, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032639, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032946, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032873, lr 0.250000\n",
      "Average classification loss at step 0: 0.695941, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003161, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032728, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.034368, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.033131, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.033248, lr 0.250000\n",
      "Average classification loss at step 0: 0.695889, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999570, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032267, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032762, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032261, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032616, lr 0.250000\n",
      "Average classification loss at step 0: 0.695831, lr 0.250000\n",
      "Average embedding loss at step 0: 2.080419, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031948, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.033633, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031905, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032224, lr 0.250000\n",
      "Average classification loss at step 0: 0.695752, lr 0.250000\n",
      "Average embedding loss at step 0: 1.956207, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031806, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032775, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032417, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032867, lr 0.250000\n",
      "Average classification loss at step 0: 0.695686, lr 0.250000\n",
      "Average embedding loss at step 0: 2.105283, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031284, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032219, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032313, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.031872, lr 0.250000\n",
      "Average classification loss at step 0: 0.695625, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041266, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.033178, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.033120, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.033319, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.033266, lr 0.250000\n",
      "Average classification loss at step 0: 0.695565, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027731, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032800, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032497, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.033208, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032619, lr 0.250000\n",
      "Average classification loss at step 0: 0.695510, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992548, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032369, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032452, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032517, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032632, lr 0.250000\n",
      "Average classification loss at step 0: 0.695466, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035823, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031927, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030834, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032908, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032448, lr 0.250000\n",
      "Average classification loss at step 0: 0.695428, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991462, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032597, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031711, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030331, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032688, lr 0.250000\n",
      "Average classification loss at step 0: 0.695400, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013978, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032068, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.032062, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031814, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032073, lr 0.250000\n",
      "Average classification loss at step 0: 0.695376, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044576, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.032030, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031486, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030264, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030399, lr 0.250000\n",
      "Average classification loss at step 0: 0.695349, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060390, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030781, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031204, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031247, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.032699, lr 0.250000\n",
      "Average classification loss at step 0: 0.695324, lr 0.250000\n",
      "Average embedding loss at step 0: 2.107172, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030883, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031224, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030101, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.031724, lr 0.250000\n",
      "Average classification loss at step 0: 0.695303, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046853, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031513, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031815, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032137, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030789, lr 0.250000\n",
      "Average classification loss at step 0: 0.695283, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013033, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031265, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031104, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031875, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.031596, lr 0.250000\n",
      "Average classification loss at step 0: 0.695266, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049141, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030538, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031373, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.032075, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030415, lr 0.250000\n",
      "Average classification loss at step 0: 0.695252, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000111, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031819, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031236, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031618, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.029184, lr 0.250000\n",
      "Average classification loss at step 0: 0.695241, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024568, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031289, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030270, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031163, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030287, lr 0.250000\n",
      "Average classification loss at step 0: 0.695231, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058633, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030897, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030603, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029806, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030680, lr 0.250000\n",
      "Average classification loss at step 0: 0.695222, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036643, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030799, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031323, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030312, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030706, lr 0.250000\n",
      "Average classification loss at step 0: 0.695214, lr 0.250000\n",
      "Average embedding loss at step 0: 1.950953, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030718, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031026, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030817, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030176, lr 0.250000\n",
      "Average classification loss at step 0: 0.695206, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066579, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030825, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029488, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031977, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.029923, lr 0.250000\n",
      "Average classification loss at step 0: 0.695200, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981907, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030959, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029980, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029143, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030181, lr 0.250000\n",
      "Average classification loss at step 0: 0.695194, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052536, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030400, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029607, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030482, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030954, lr 0.250000\n",
      "Average classification loss at step 0: 0.695188, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043180, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029788, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029508, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.031047, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030438, lr 0.250000\n",
      "Average classification loss at step 0: 0.695182, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003555, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030433, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031173, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029747, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030460, lr 0.250000\n",
      "Average classification loss at step 0: 0.695178, lr 0.250000\n",
      "Average embedding loss at step 0: 2.074422, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029635, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.031594, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030355, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030966, lr 0.250000\n",
      "Average classification loss at step 0: 0.695174, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978909, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029566, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030505, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030114, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030565, lr 0.250000\n",
      "Average classification loss at step 0: 0.695170, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967532, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030074, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030194, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030294, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.029954, lr 0.250000\n",
      "Average classification loss at step 0: 0.695167, lr 0.250000\n",
      "Average embedding loss at step 0: 2.086413, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029803, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030392, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030316, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.029614, lr 0.250000\n",
      "Average classification loss at step 0: 0.695164, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035600, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030531, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029727, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029737, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.030269, lr 0.250000\n",
      "Average classification loss at step 0: 0.695161, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043064, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029014, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029879, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030530, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028124, lr 0.250000\n",
      "Average classification loss at step 0: 0.695157, lr 0.250000\n",
      "Average embedding loss at step 0: 1.964275, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.030168, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027718, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028849, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028238, lr 0.250000\n",
      "Average classification loss at step 0: 0.695153, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013353, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028328, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030430, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028574, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.029483, lr 0.250000\n",
      "Average classification loss at step 0: 0.695149, lr 0.250000\n",
      "Average embedding loss at step 0: 2.076988, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029570, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029079, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029032, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028665, lr 0.250000\n",
      "Average classification loss at step 0: 0.695145, lr 0.250000\n",
      "Average embedding loss at step 0: 2.112024, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029236, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028175, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029628, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028820, lr 0.250000\n",
      "Average classification loss at step 0: 0.695141, lr 0.250000\n",
      "Average embedding loss at step 0: 1.951806, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029488, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030102, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028848, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028257, lr 0.250000\n",
      "Average classification loss at step 0: 0.695136, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992841, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029020, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029498, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029068, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028462, lr 0.250000\n",
      "Average classification loss at step 0: 0.695132, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057969, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027676, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.030141, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028131, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028981, lr 0.250000\n",
      "Average classification loss at step 0: 0.695127, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027186, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028060, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029050, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027900, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028955, lr 0.250000\n",
      "Average classification loss at step 0: 0.695122, lr 0.250000\n",
      "Average embedding loss at step 0: 2.083317, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028007, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027490, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.030167, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027893, lr 0.250000\n",
      "Average classification loss at step 0: 0.695116, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005627, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.031152, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028511, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028910, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027530, lr 0.250000\n",
      "Average classification loss at step 0: 0.695110, lr 0.250000\n",
      "Average embedding loss at step 0: 2.063112, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027907, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028838, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029161, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027528, lr 0.250000\n",
      "Average classification loss at step 0: 0.695104, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051094, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027829, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028578, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028514, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027087, lr 0.250000\n",
      "Average classification loss at step 0: 0.695096, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004485, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028443, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027484, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027274, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028988, lr 0.250000\n",
      "Average classification loss at step 0: 0.695088, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042652, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027159, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028640, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029563, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028003, lr 0.250000\n",
      "Average classification loss at step 0: 0.695080, lr 0.250000\n",
      "Average embedding loss at step 0: 1.961368, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027693, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.029023, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029211, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028451, lr 0.250000\n",
      "Average classification loss at step 0: 0.695072, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009084, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.029195, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028047, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028339, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.029219, lr 0.250000\n",
      "Average classification loss at step 0: 0.695063, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973749, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027479, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028347, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028630, lr 0.250000\n",
      "Average classification loss at step 0: 0.695055, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984197, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027522, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028159, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027628, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027105, lr 0.250000\n",
      "Average classification loss at step 0: 0.695048, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985036, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028312, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027173, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027919, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027755, lr 0.250000\n",
      "Average classification loss at step 0: 0.695040, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968801, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027463, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027699, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.029321, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026287, lr 0.250000\n",
      "Average classification loss at step 0: 0.695032, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997295, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028801, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027824, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027783, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026342, lr 0.250000\n",
      "Average classification loss at step 0: 0.695024, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041456, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026908, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027903, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027455, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027270, lr 0.250000\n",
      "Average classification loss at step 0: 0.695018, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036233, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028115, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027213, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.028125, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027997, lr 0.250000\n",
      "Average classification loss at step 0: 0.695011, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061006, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027736, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027421, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027493, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027622, lr 0.250000\n",
      "Average classification loss at step 0: 0.695006, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001189, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027545, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026498, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026225, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027261, lr 0.250000\n",
      "Average classification loss at step 0: 0.695001, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975362, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027001, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027071, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026930, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027179, lr 0.250000\n",
      "Average classification loss at step 0: 0.694996, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055278, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026255, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027590, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026061, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025589, lr 0.250000\n",
      "Average classification loss at step 0: 0.694991, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013180, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026277, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028088, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026326, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026660, lr 0.250000\n",
      "Average classification loss at step 0: 0.694986, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013534, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027395, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027067, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026680, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027447, lr 0.250000\n",
      "Average classification loss at step 0: 0.694981, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057796, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024929, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026390, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026769, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026918, lr 0.250000\n",
      "Average classification loss at step 0: 0.694976, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992250, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025855, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028119, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026565, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.028183, lr 0.250000\n",
      "Average classification loss at step 0: 0.694971, lr 0.250000\n",
      "Average embedding loss at step 0: 2.067126, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026452, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.028059, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026856, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027727, lr 0.250000\n",
      "Average classification loss at step 0: 0.694966, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995133, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028379, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025938, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027654, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026460, lr 0.250000\n",
      "Average classification loss at step 0: 0.694960, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000477, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026603, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026610, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026174, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026170, lr 0.250000\n",
      "Average classification loss at step 0: 0.694955, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068611, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026575, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025725, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026270, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026065, lr 0.250000\n",
      "Average classification loss at step 0: 0.694949, lr 0.250000\n",
      "Average embedding loss at step 0: 1.980104, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026916, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026320, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.027060, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027852, lr 0.250000\n",
      "Average classification loss at step 0: 0.694943, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017084, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026570, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026224, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026435, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025642, lr 0.250000\n",
      "Average classification loss at step 0: 0.694937, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065943, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027347, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026767, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026504, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027419, lr 0.250000\n",
      "Average classification loss at step 0: 0.694931, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990536, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026494, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026438, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026700, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027478, lr 0.250000\n",
      "Average classification loss at step 0: 0.694924, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974664, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025531, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027408, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026975, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026110, lr 0.250000\n",
      "Average classification loss at step 0: 0.694917, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052358, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024548, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025621, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025379, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.027494, lr 0.250000\n",
      "Average classification loss at step 0: 0.694909, lr 0.250000\n",
      "Average embedding loss at step 0: 1.971489, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026985, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026625, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025287, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026615, lr 0.250000\n",
      "Average classification loss at step 0: 0.694901, lr 0.250000\n",
      "Average embedding loss at step 0: 2.102433, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.028527, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026777, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025793, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026563, lr 0.250000\n",
      "Average classification loss at step 0: 0.694893, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002479, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026343, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025554, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025816, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024531, lr 0.250000\n",
      "Average classification loss at step 0: 0.694883, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005005, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027252, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024991, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024948, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025150, lr 0.250000\n",
      "Average classification loss at step 0: 0.694873, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987508, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026731, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025884, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026136, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025385, lr 0.250000\n",
      "Average classification loss at step 0: 0.694862, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022075, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025292, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026887, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026069, lr 0.250000\n",
      "Average classification loss at step 0: 0.694851, lr 0.250000\n",
      "Average embedding loss at step 0: 1.979347, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025922, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025828, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026055, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024479, lr 0.250000\n",
      "Average classification loss at step 0: 0.694840, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017343, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.027196, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025709, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024743, lr 0.250000\n",
      "Average classification loss at step 0: 0.694827, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062745, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024757, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024972, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026323, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026501, lr 0.250000\n",
      "Average classification loss at step 0: 0.694813, lr 0.250000\n",
      "Average embedding loss at step 0: 2.093840, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025124, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.027699, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026099, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025548, lr 0.250000\n",
      "Average classification loss at step 0: 0.694798, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017014, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024364, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024739, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025297, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025743, lr 0.250000\n",
      "Average classification loss at step 0: 0.694781, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054173, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024997, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025485, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025199, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023882, lr 0.250000\n",
      "Average classification loss at step 0: 0.694764, lr 0.250000\n",
      "Average embedding loss at step 0: 2.087023, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025416, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026399, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025928, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023536, lr 0.250000\n",
      "Average classification loss at step 0: 0.694747, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011502, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023940, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025813, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026001, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025511, lr 0.250000\n",
      "Average classification loss at step 0: 0.694730, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968004, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.026441, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024765, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024637, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026290, lr 0.250000\n",
      "Average classification loss at step 0: 0.694714, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030010, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025407, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026894, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025296, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024887, lr 0.250000\n",
      "Average classification loss at step 0: 0.694697, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049318, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025532, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.026501, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024348, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025453, lr 0.250000\n",
      "Average classification loss at step 0: 0.694682, lr 0.250000\n",
      "Average embedding loss at step 0: 1.957977, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024596, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023708, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025780, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025910, lr 0.250000\n",
      "Average classification loss at step 0: 0.694666, lr 0.250000\n",
      "Average embedding loss at step 0: 1.969588, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024924, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025673, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025446, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025025, lr 0.250000\n",
      "Average classification loss at step 0: 0.694650, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018122, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024177, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.026082, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025062, lr 0.250000\n",
      "Average classification loss at step 0: 0.694635, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033045, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024070, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024465, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025670, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.026108, lr 0.250000\n",
      "Average classification loss at step 0: 0.694620, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977749, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025081, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024807, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023924, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025461, lr 0.250000\n",
      "Average classification loss at step 0: 0.694606, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022689, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024093, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024248, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024597, lr 0.250000\n",
      "Average classification loss at step 0: 0.694593, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024889, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025157, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025094, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024114, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024082, lr 0.250000\n",
      "Average classification loss at step 0: 0.694582, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027705, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025227, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024305, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024784, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024165, lr 0.250000\n",
      "Average classification loss at step 0: 0.694571, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009075, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024664, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023611, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024504, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024940, lr 0.250000\n",
      "Average classification loss at step 0: 0.694560, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065352, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024199, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022426, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024724, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024200, lr 0.250000\n",
      "Average classification loss at step 0: 0.694551, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965556, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023960, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023980, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025028, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023543, lr 0.250000\n",
      "Average classification loss at step 0: 0.694542, lr 0.250000\n",
      "Average embedding loss at step 0: 1.980076, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024012, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025867, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022845, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023752, lr 0.250000\n",
      "Average classification loss at step 0: 0.694534, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018878, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022960, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023834, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023314, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.025840, lr 0.250000\n",
      "Average classification loss at step 0: 0.694526, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041568, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024295, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023156, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024362, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024615, lr 0.250000\n",
      "Average classification loss at step 0: 0.694519, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029083, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023994, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025264, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025827, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023707, lr 0.250000\n",
      "Average classification loss at step 0: 0.694512, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984562, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023672, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023930, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024080, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023505, lr 0.250000\n",
      "Average classification loss at step 0: 0.694505, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987263, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.025643, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022755, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024352, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022476, lr 0.250000\n",
      "Average classification loss at step 0: 0.694500, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064705, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024466, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023399, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024826, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024889, lr 0.250000\n",
      "Average classification loss at step 0: 0.694494, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026616, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023400, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024478, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023943, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024024, lr 0.250000\n",
      "Average classification loss at step 0: 0.694489, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020343, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024482, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024783, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023528, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024057, lr 0.250000\n",
      "Average classification loss at step 0: 0.694484, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035583, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024146, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.025234, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023698, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023600, lr 0.250000\n",
      "Average classification loss at step 0: 0.694479, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004273, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024594, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024710, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023436, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023166, lr 0.250000\n",
      "Average classification loss at step 0: 0.694475, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996339, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024147, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022977, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023814, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023932, lr 0.250000\n",
      "Average classification loss at step 0: 0.694470, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068158, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024159, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022540, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023899, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023529, lr 0.250000\n",
      "Average classification loss at step 0: 0.694466, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035480, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023273, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024130, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.025461, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023468, lr 0.250000\n",
      "Average classification loss at step 0: 0.694462, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024102, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022577, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024212, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024822, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022208, lr 0.250000\n",
      "Average classification loss at step 0: 0.694459, lr 0.250000\n",
      "Average embedding loss at step 0: 1.953220, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022779, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023132, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024403, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023829, lr 0.250000\n",
      "Average classification loss at step 0: 0.694455, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981619, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022756, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023785, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022966, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023638, lr 0.250000\n",
      "Average classification loss at step 0: 0.694452, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041390, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023828, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023930, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024379, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022800, lr 0.250000\n",
      "Average classification loss at step 0: 0.694448, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030685, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.024366, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021485, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024220, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024256, lr 0.250000\n",
      "Average classification loss at step 0: 0.694445, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066153, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022295, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022731, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024013, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024115, lr 0.250000\n",
      "Average classification loss at step 0: 0.694443, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033902, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022867, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024185, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023985, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023042, lr 0.250000\n",
      "Average classification loss at step 0: 0.694440, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021609, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022876, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022650, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022089, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022788, lr 0.250000\n",
      "Average classification loss at step 0: 0.694438, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056122, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023543, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024597, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024172, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024330, lr 0.250000\n",
      "Average classification loss at step 0: 0.694435, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003508, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022780, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021373, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023020, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022816, lr 0.250000\n",
      "Average classification loss at step 0: 0.694433, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022749, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023898, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021682, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023319, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023420, lr 0.250000\n",
      "Average classification loss at step 0: 0.694431, lr 0.250000\n",
      "Average embedding loss at step 0: 2.075376, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022852, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023865, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022894, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023865, lr 0.250000\n",
      "Average classification loss at step 0: 0.694429, lr 0.250000\n",
      "Average embedding loss at step 0: 2.100621, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022930, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022467, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022362, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022267, lr 0.250000\n",
      "Average classification loss at step 0: 0.694427, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066757, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023993, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023581, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023499, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022652, lr 0.250000\n",
      "Average classification loss at step 0: 0.694425, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999534, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022946, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022920, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023518, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023527, lr 0.250000\n",
      "Average classification loss at step 0: 0.694423, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052386, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023726, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024703, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.024212, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022574, lr 0.250000\n",
      "Average classification loss at step 0: 0.694421, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007132, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023255, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023396, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021521, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023789, lr 0.250000\n",
      "Average classification loss at step 0: 0.694419, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057052, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022381, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022157, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022709, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022560, lr 0.250000\n",
      "Average classification loss at step 0: 0.694417, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035381, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022926, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022233, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023274, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022795, lr 0.250000\n",
      "Average classification loss at step 0: 0.694416, lr 0.250000\n",
      "Average embedding loss at step 0: 2.102502, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022199, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021907, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021973, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.024442, lr 0.250000\n",
      "Average classification loss at step 0: 0.694414, lr 0.250000\n",
      "Average embedding loss at step 0: 2.119248, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022629, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023268, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022666, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023110, lr 0.250000\n",
      "Average classification loss at step 0: 0.694412, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061488, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022711, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022266, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021741, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022026, lr 0.250000\n",
      "Average classification loss at step 0: 0.694411, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024396, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023181, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021883, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022228, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021933, lr 0.250000\n",
      "Average classification loss at step 0: 0.694410, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042246, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023536, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.024800, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022021, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022933, lr 0.250000\n",
      "Average classification loss at step 0: 0.694408, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043035, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023217, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023336, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021339, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022350, lr 0.250000\n",
      "Average classification loss at step 0: 0.694407, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960961, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023061, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022207, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021201, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023168, lr 0.250000\n",
      "Average classification loss at step 0: 0.694405, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045507, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022664, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022094, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022151, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022287, lr 0.250000\n",
      "Average classification loss at step 0: 0.694404, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985814, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022158, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021327, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023445, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022267, lr 0.250000\n",
      "Average classification loss at step 0: 0.694403, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963677, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023030, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021482, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021505, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021202, lr 0.250000\n",
      "Average classification loss at step 0: 0.694401, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004259, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022186, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022097, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022700, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022219, lr 0.250000\n",
      "Average classification loss at step 0: 0.694400, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073758, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023765, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023783, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022696, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022497, lr 0.250000\n",
      "Average classification loss at step 0: 0.694398, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013329, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022134, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021245, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021789, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021617, lr 0.250000\n",
      "Average classification loss at step 0: 0.694397, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993896, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022340, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022409, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021971, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022327, lr 0.250000\n",
      "Average classification loss at step 0: 0.694395, lr 0.250000\n",
      "Average embedding loss at step 0: 1.932492, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022329, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022186, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022179, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022016, lr 0.250000\n",
      "Average classification loss at step 0: 0.694394, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027372, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022430, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022360, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022259, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022510, lr 0.250000\n",
      "Average classification loss at step 0: 0.694393, lr 0.250000\n",
      "Average embedding loss at step 0: 1.956112, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023215, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022899, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021392, lr 0.250000\n",
      "Average classification loss at step 0: 0.694391, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005036, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023230, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021397, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021524, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022700, lr 0.250000\n",
      "Average classification loss at step 0: 0.694390, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009006, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022779, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022992, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022483, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021816, lr 0.250000\n",
      "Average classification loss at step 0: 0.694388, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013875, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022911, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023710, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019957, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022458, lr 0.250000\n",
      "Average classification loss at step 0: 0.694387, lr 0.250000\n",
      "Average embedding loss at step 0: 2.050581, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022342, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021511, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021389, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021927, lr 0.250000\n",
      "Average classification loss at step 0: 0.694386, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001291, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023679, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022858, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022416, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022799, lr 0.250000\n",
      "Average classification loss at step 0: 0.694385, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025536, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021536, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022175, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021216, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023670, lr 0.250000\n",
      "Average classification loss at step 0: 0.694383, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015056, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022879, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021498, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.023020, lr 0.250000\n",
      "Average classification loss at step 0: 0.694382, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065256, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020709, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020333, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022359, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020320, lr 0.250000\n",
      "Average classification loss at step 0: 0.694380, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019265, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022121, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021768, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021709, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022682, lr 0.250000\n",
      "Average classification loss at step 0: 0.694379, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057341, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.023147, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022901, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020657, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021887, lr 0.250000\n",
      "Average classification loss at step 0: 0.694377, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996830, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021895, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020827, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021766, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022354, lr 0.250000\n",
      "Average classification loss at step 0: 0.694376, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024198, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021325, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019778, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021643, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021526, lr 0.250000\n",
      "Average classification loss at step 0: 0.694374, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009801, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022181, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021051, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019673, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021364, lr 0.250000\n",
      "Average classification loss at step 0: 0.694373, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046153, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019913, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.023323, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021092, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021293, lr 0.250000\n",
      "Average classification loss at step 0: 0.694371, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011961, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021207, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021153, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020800, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020708, lr 0.250000\n",
      "Average classification loss at step 0: 0.694370, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036096, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021479, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022403, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020207, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020904, lr 0.250000\n",
      "Average classification loss at step 0: 0.694368, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032051, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020466, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020463, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022184, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021568, lr 0.250000\n",
      "Average classification loss at step 0: 0.694366, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064081, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020940, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021121, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020661, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021014, lr 0.250000\n",
      "Average classification loss at step 0: 0.694365, lr 0.250000\n",
      "Average embedding loss at step 0: 2.092815, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022327, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022150, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021486, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022085, lr 0.250000\n",
      "Average classification loss at step 0: 0.694363, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023051, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022302, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021945, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022442, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022225, lr 0.250000\n",
      "Average classification loss at step 0: 0.694361, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993298, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022616, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021042, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022050, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021793, lr 0.250000\n",
      "Average classification loss at step 0: 0.694359, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038228, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022092, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021095, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022361, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020091, lr 0.250000\n",
      "Average classification loss at step 0: 0.694358, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025050, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021239, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022191, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021451, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022114, lr 0.250000\n",
      "Average classification loss at step 0: 0.694356, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031137, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020970, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020918, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021190, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021804, lr 0.250000\n",
      "Average classification loss at step 0: 0.694354, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012895, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020775, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021017, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020184, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020413, lr 0.250000\n",
      "Average classification loss at step 0: 0.694353, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014057, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021281, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020882, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021139, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021360, lr 0.250000\n",
      "Average classification loss at step 0: 0.694351, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973958, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020628, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020794, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019621, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020231, lr 0.250000\n",
      "Average classification loss at step 0: 0.694349, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027411, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020857, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021803, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022529, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021002, lr 0.250000\n",
      "Average classification loss at step 0: 0.694347, lr 0.250000\n",
      "Average embedding loss at step 0: 2.106450, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021437, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021027, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.023032, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020961, lr 0.250000\n",
      "Average classification loss at step 0: 0.694345, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005749, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020742, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021398, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021917, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020268, lr 0.250000\n",
      "Average classification loss at step 0: 0.694344, lr 0.250000\n",
      "Average embedding loss at step 0: 2.085864, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021200, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021114, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020809, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020348, lr 0.250000\n",
      "Average classification loss at step 0: 0.694342, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064512, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021628, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020019, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021484, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020328, lr 0.250000\n",
      "Average classification loss at step 0: 0.694340, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023446, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021251, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020671, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020685, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021421, lr 0.250000\n",
      "Average classification loss at step 0: 0.694338, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002064, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020190, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021632, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019511, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020375, lr 0.250000\n",
      "Average classification loss at step 0: 0.694337, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985348, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020666, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022092, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021641, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020865, lr 0.250000\n",
      "Average classification loss at step 0: 0.694335, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061661, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020825, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021927, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019809, lr 0.250000\n",
      "Average classification loss at step 0: 0.694333, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028384, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021493, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020254, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020148, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020139, lr 0.250000\n",
      "Average classification loss at step 0: 0.694331, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021173, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021797, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020340, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020885, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021805, lr 0.250000\n",
      "Average classification loss at step 0: 0.694329, lr 0.250000\n",
      "Average embedding loss at step 0: 2.080859, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020113, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020539, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020660, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021267, lr 0.250000\n",
      "Average classification loss at step 0: 0.694327, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039621, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022473, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021009, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020332, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019734, lr 0.250000\n",
      "Average classification loss at step 0: 0.694326, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001944, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019229, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021532, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020120, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021593, lr 0.250000\n",
      "Average classification loss at step 0: 0.694324, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036541, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020472, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019800, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020549, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020778, lr 0.250000\n",
      "Average classification loss at step 0: 0.694322, lr 0.250000\n",
      "Average embedding loss at step 0: 2.008884, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021184, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020580, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.022214, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020076, lr 0.250000\n",
      "Average classification loss at step 0: 0.694320, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051534, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020985, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021575, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020835, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019399, lr 0.250000\n",
      "Average classification loss at step 0: 0.694319, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037719, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020336, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020155, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019826, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021958, lr 0.250000\n",
      "Average classification loss at step 0: 0.694317, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017395, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020193, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021886, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021743, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020113, lr 0.250000\n",
      "Average classification loss at step 0: 0.694315, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007802, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021402, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019380, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021240, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020296, lr 0.250000\n",
      "Average classification loss at step 0: 0.694314, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963443, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019782, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.022084, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020658, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020656, lr 0.250000\n",
      "Average classification loss at step 0: 0.694312, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003462, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021034, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019565, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019839, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021277, lr 0.250000\n",
      "Average classification loss at step 0: 0.694311, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038227, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020275, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020601, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020895, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020450, lr 0.250000\n",
      "Average classification loss at step 0: 0.694310, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073904, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020541, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020472, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020887, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020465, lr 0.250000\n",
      "Average classification loss at step 0: 0.694308, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020036, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020109, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020633, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019185, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020485, lr 0.250000\n",
      "Average classification loss at step 0: 0.694307, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056172, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020272, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020289, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019342, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020484, lr 0.250000\n",
      "Average classification loss at step 0: 0.694306, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023652, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020639, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020436, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020252, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020339, lr 0.250000\n",
      "Average classification loss at step 0: 0.694305, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000119, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020778, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019586, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020615, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018979, lr 0.250000\n",
      "Average classification loss at step 0: 0.694303, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055512, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.022738, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019806, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020802, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019738, lr 0.250000\n",
      "Average classification loss at step 0: 0.694302, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045717, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019732, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020058, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020255, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020804, lr 0.250000\n",
      "Average classification loss at step 0: 0.694301, lr 0.250000\n",
      "Average embedding loss at step 0: 1.951335, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019207, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019107, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020642, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020247, lr 0.250000\n",
      "Average classification loss at step 0: 0.694300, lr 0.250000\n",
      "Average embedding loss at step 0: 2.081988, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021173, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020026, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020072, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020541, lr 0.250000\n",
      "Average classification loss at step 0: 0.694299, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997110, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020103, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019999, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020685, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019675, lr 0.250000\n",
      "Average classification loss at step 0: 0.694298, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031209, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020112, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020196, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021480, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019716, lr 0.250000\n",
      "Average classification loss at step 0: 0.694297, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045838, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021089, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019584, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020383, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020556, lr 0.250000\n",
      "Average classification loss at step 0: 0.694296, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996000, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020887, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019506, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019999, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.022075, lr 0.250000\n",
      "Average classification loss at step 0: 0.694295, lr 0.250000\n",
      "Average embedding loss at step 0: 2.110611, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019370, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019469, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020849, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019554, lr 0.250000\n",
      "Average classification loss at step 0: 0.694294, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039410, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019239, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020092, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020066, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019905, lr 0.250000\n",
      "Average classification loss at step 0: 0.694293, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031345, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019031, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019284, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021029, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019462, lr 0.250000\n",
      "Average classification loss at step 0: 0.694292, lr 0.250000\n",
      "Average embedding loss at step 0: 1.941230, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020178, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020468, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020282, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019829, lr 0.250000\n",
      "Average classification loss at step 0: 0.694291, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046183, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019689, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019492, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019338, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017762, lr 0.250000\n",
      "Average classification loss at step 0: 0.694290, lr 0.250000\n",
      "Average embedding loss at step 0: 2.077210, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020650, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020227, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019560, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021096, lr 0.250000\n",
      "Average classification loss at step 0: 0.694290, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039029, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.021314, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019476, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020093, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021460, lr 0.250000\n",
      "Average classification loss at step 0: 0.694289, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006723, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020811, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018861, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019532, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021002, lr 0.250000\n",
      "Average classification loss at step 0: 0.694288, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043054, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019570, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019679, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019934, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018961, lr 0.250000\n",
      "Average classification loss at step 0: 0.694287, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992095, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019459, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020302, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019505, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019166, lr 0.250000\n",
      "Average classification loss at step 0: 0.694286, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018620, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020928, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018975, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018905, lr 0.250000\n",
      "Average classification loss at step 0: 0.694286, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015916, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020738, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020711, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.021622, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018995, lr 0.250000\n",
      "Average classification loss at step 0: 0.694285, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041859, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018905, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019740, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020435, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020750, lr 0.250000\n",
      "Average classification loss at step 0: 0.694284, lr 0.250000\n",
      "Average embedding loss at step 0: 1.966223, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018264, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020705, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019204, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021557, lr 0.250000\n",
      "Average classification loss at step 0: 0.694283, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990593, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020107, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020534, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018410, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020043, lr 0.250000\n",
      "Average classification loss at step 0: 0.694283, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983477, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019778, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019903, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019077, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020244, lr 0.250000\n",
      "Average classification loss at step 0: 0.694282, lr 0.250000\n",
      "Average embedding loss at step 0: 1.966955, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019730, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019128, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018499, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019479, lr 0.250000\n",
      "Average classification loss at step 0: 0.694281, lr 0.250000\n",
      "Average embedding loss at step 0: 1.982961, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018925, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020218, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018881, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020411, lr 0.250000\n",
      "Average classification loss at step 0: 0.694281, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031157, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019812, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019432, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019070, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020126, lr 0.250000\n",
      "Average classification loss at step 0: 0.694280, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984186, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019946, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019485, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020157, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018303, lr 0.250000\n",
      "Average classification loss at step 0: 0.694280, lr 0.250000\n",
      "Average embedding loss at step 0: 2.098980, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020246, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019394, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018432, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020208, lr 0.250000\n",
      "Average classification loss at step 0: 0.694279, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981074, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019919, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019261, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020037, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020015, lr 0.250000\n",
      "Average classification loss at step 0: 0.694279, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009442, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019507, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.021572, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020318, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018074, lr 0.250000\n",
      "Average classification loss at step 0: 0.694278, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989620, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017985, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019346, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019331, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020498, lr 0.250000\n",
      "Average classification loss at step 0: 0.694278, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044304, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019843, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020805, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018525, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019335, lr 0.250000\n",
      "Average classification loss at step 0: 0.694277, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978233, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020793, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018099, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018687, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018968, lr 0.250000\n",
      "Average classification loss at step 0: 0.694277, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034778, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020074, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019491, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018708, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017565, lr 0.250000\n",
      "Average classification loss at step 0: 0.694276, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974831, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019271, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018828, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018544, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019080, lr 0.250000\n",
      "Average classification loss at step 0: 0.694276, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064148, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020482, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019372, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019666, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018768, lr 0.250000\n",
      "Average classification loss at step 0: 0.694275, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994008, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018273, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019707, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019761, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018598, lr 0.250000\n",
      "Average classification loss at step 0: 0.694275, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045883, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019734, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018201, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019290, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018222, lr 0.250000\n",
      "Average classification loss at step 0: 0.694274, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021221, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019480, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019746, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019507, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019581, lr 0.250000\n",
      "Average classification loss at step 0: 0.694274, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004319, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019072, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018253, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019218, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018985, lr 0.250000\n",
      "Average classification loss at step 0: 0.694274, lr 0.250000\n",
      "Average embedding loss at step 0: 2.080185, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019211, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019781, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019395, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018706, lr 0.250000\n",
      "Average classification loss at step 0: 0.694273, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031014, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017907, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018709, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018305, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019976, lr 0.250000\n",
      "Average classification loss at step 0: 0.694273, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013818, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019287, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019056, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019003, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019671, lr 0.250000\n",
      "Average classification loss at step 0: 0.694272, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036053, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018778, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019283, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018065, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018670, lr 0.250000\n",
      "Average classification loss at step 0: 0.694272, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024664, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017492, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018911, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019211, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019712, lr 0.250000\n",
      "Average classification loss at step 0: 0.694272, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006182, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019614, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018775, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018852, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019939, lr 0.250000\n",
      "Average classification loss at step 0: 0.694271, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021502, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019653, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019897, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019050, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018377, lr 0.250000\n",
      "Average classification loss at step 0: 0.694271, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032571, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020022, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020117, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018776, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.020494, lr 0.250000\n",
      "Average classification loss at step 0: 0.694270, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033941, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017387, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018409, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020160, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019516, lr 0.250000\n",
      "Average classification loss at step 0: 0.694270, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036763, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018639, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018606, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018314, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018360, lr 0.250000\n",
      "Average classification loss at step 0: 0.694270, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027578, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019428, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019402, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019198, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019413, lr 0.250000\n",
      "Average classification loss at step 0: 0.694269, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994652, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016694, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018976, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.020222, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017539, lr 0.250000\n",
      "Average classification loss at step 0: 0.694269, lr 0.250000\n",
      "Average embedding loss at step 0: 1.959506, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018736, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019676, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018882, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019821, lr 0.250000\n",
      "Average classification loss at step 0: 0.694268, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007522, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018838, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018958, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018632, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018838, lr 0.250000\n",
      "Average classification loss at step 0: 0.694268, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044814, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019741, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020129, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018646, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018359, lr 0.250000\n",
      "Average classification loss at step 0: 0.694268, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041690, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019081, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017654, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018161, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019311, lr 0.250000\n",
      "Average classification loss at step 0: 0.694267, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035174, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018826, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018295, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019606, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019697, lr 0.250000\n",
      "Average classification loss at step 0: 0.694267, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033488, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018836, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016328, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019367, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019272, lr 0.250000\n",
      "Average classification loss at step 0: 0.694266, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018538, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018638, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019002, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019431, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018004, lr 0.250000\n",
      "Average classification loss at step 0: 0.694266, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995018, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017873, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.020054, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017751, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018839, lr 0.250000\n",
      "Average classification loss at step 0: 0.694266, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073453, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018002, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016945, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016783, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.021003, lr 0.250000\n",
      "Average classification loss at step 0: 0.694265, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021508, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018502, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017928, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019004, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019795, lr 0.250000\n",
      "Average classification loss at step 0: 0.694265, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044105, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018802, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018885, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019760, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016299, lr 0.250000\n",
      "Average classification loss at step 0: 0.694265, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049915, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017710, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018376, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018586, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018048, lr 0.250000\n",
      "Average classification loss at step 0: 0.694264, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039777, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020434, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017511, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019785, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017134, lr 0.250000\n",
      "Average classification loss at step 0: 0.694264, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000539, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018685, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019174, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018070, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018313, lr 0.250000\n",
      "Average classification loss at step 0: 0.694264, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021656, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017574, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019090, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018386, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018351, lr 0.250000\n",
      "Average classification loss at step 0: 0.694263, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068785, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019293, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019818, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018501, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019412, lr 0.250000\n",
      "Average classification loss at step 0: 0.694263, lr 0.250000\n",
      "Average embedding loss at step 0: 1.944385, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017581, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019278, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018918, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018199, lr 0.250000\n",
      "Average classification loss at step 0: 0.694262, lr 0.250000\n",
      "Average embedding loss at step 0: 2.053754, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019219, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018494, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017805, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018581, lr 0.250000\n",
      "Average classification loss at step 0: 0.694262, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068674, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017810, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018570, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019450, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019073, lr 0.250000\n",
      "Average classification loss at step 0: 0.694262, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991757, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018458, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017364, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018689, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019198, lr 0.250000\n",
      "Average classification loss at step 0: 0.694261, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997280, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018329, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018215, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019940, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018635, lr 0.250000\n",
      "Average classification loss at step 0: 0.694261, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054816, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017333, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019766, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019003, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017274, lr 0.250000\n",
      "Average classification loss at step 0: 0.694261, lr 0.250000\n",
      "Average embedding loss at step 0: 1.980383, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016742, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018374, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018472, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017558, lr 0.250000\n",
      "Average classification loss at step 0: 0.694260, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999036, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017871, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019055, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017418, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018322, lr 0.250000\n",
      "Average classification loss at step 0: 0.694260, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040477, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018727, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019330, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018364, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018055, lr 0.250000\n",
      "Average classification loss at step 0: 0.694260, lr 0.250000\n",
      "Average embedding loss at step 0: 1.954524, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018862, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017984, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017897, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019010, lr 0.250000\n",
      "Average classification loss at step 0: 0.694259, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058982, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017166, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018296, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018544, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017386, lr 0.250000\n",
      "Average classification loss at step 0: 0.694259, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029752, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018848, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018209, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019149, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018273, lr 0.250000\n",
      "Average classification loss at step 0: 0.694259, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040138, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018416, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017198, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017532, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018921, lr 0.250000\n",
      "Average classification loss at step 0: 0.694258, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992513, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017766, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018093, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018381, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019770, lr 0.250000\n",
      "Average classification loss at step 0: 0.694258, lr 0.250000\n",
      "Average embedding loss at step 0: 2.047763, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017232, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015821, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017669, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017502, lr 0.250000\n",
      "Average classification loss at step 0: 0.694258, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065522, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017968, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017535, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017898, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018573, lr 0.250000\n",
      "Average classification loss at step 0: 0.694257, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990153, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017356, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019943, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016979, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018192, lr 0.250000\n",
      "Average classification loss at step 0: 0.694257, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024365, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018322, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016846, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018332, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018086, lr 0.250000\n",
      "Average classification loss at step 0: 0.694257, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033727, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.020348, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019081, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017699, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017622, lr 0.250000\n",
      "Average classification loss at step 0: 0.694256, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043118, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019274, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017327, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018198, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018192, lr 0.250000\n",
      "Average classification loss at step 0: 0.694256, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025471, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018376, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019170, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018270, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017598, lr 0.250000\n",
      "Average classification loss at step 0: 0.694256, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038200, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018734, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018965, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017005, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018967, lr 0.250000\n",
      "Average classification loss at step 0: 0.694255, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026582, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018515, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017503, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017986, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017984, lr 0.250000\n",
      "Average classification loss at step 0: 0.694255, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043512, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018799, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017374, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018009, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017504, lr 0.250000\n",
      "Average classification loss at step 0: 0.694255, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056309, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018216, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016563, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017535, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019381, lr 0.250000\n",
      "Average classification loss at step 0: 0.694254, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036869, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017935, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018882, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018276, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018779, lr 0.250000\n",
      "Average classification loss at step 0: 0.694254, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025974, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017422, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018798, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017332, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017087, lr 0.250000\n",
      "Average classification loss at step 0: 0.694254, lr 0.250000\n",
      "Average embedding loss at step 0: 2.089789, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018401, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017798, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017924, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018486, lr 0.250000\n",
      "Average classification loss at step 0: 0.694253, lr 0.250000\n",
      "Average embedding loss at step 0: 1.956739, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019437, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019046, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017060, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018262, lr 0.250000\n",
      "Average classification loss at step 0: 0.694253, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021182, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018523, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018580, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017391, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018116, lr 0.250000\n",
      "Average classification loss at step 0: 0.694252, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043094, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018297, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018191, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015969, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017924, lr 0.250000\n",
      "Average classification loss at step 0: 0.694252, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990948, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015873, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017819, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017482, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017282, lr 0.250000\n",
      "Average classification loss at step 0: 0.694252, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987295, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017246, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017753, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019479, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017501, lr 0.250000\n",
      "Average classification loss at step 0: 0.694251, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028306, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017247, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017585, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016788, lr 0.250000\n",
      "Average classification loss at step 0: 0.694251, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993843, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018944, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018390, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018001, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017793, lr 0.250000\n",
      "Average classification loss at step 0: 0.694251, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043820, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018131, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.019002, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017616, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018912, lr 0.250000\n",
      "Average classification loss at step 0: 0.694250, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963565, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018756, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017378, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017397, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017812, lr 0.250000\n",
      "Average classification loss at step 0: 0.694250, lr 0.250000\n",
      "Average embedding loss at step 0: 2.082475, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017623, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018459, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017163, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018625, lr 0.250000\n",
      "Average classification loss at step 0: 0.694250, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004401, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017005, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017872, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018388, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017318, lr 0.250000\n",
      "Average classification loss at step 0: 0.694249, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999700, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018856, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018147, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017223, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017261, lr 0.250000\n",
      "Average classification loss at step 0: 0.694249, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978192, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018583, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018194, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018656, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017608, lr 0.250000\n",
      "Average classification loss at step 0: 0.694249, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066580, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018225, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018599, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016886, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018145, lr 0.250000\n",
      "Average classification loss at step 0: 0.694249, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006216, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.019533, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017280, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018075, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017166, lr 0.250000\n",
      "Average classification loss at step 0: 0.694248, lr 0.250000\n",
      "Average embedding loss at step 0: 2.069139, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018678, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018610, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016573, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018145, lr 0.250000\n",
      "Average classification loss at step 0: 0.694248, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043808, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017821, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017710, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018888, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016973, lr 0.250000\n",
      "Average classification loss at step 0: 0.694248, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004057, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018829, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017530, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017932, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017611, lr 0.250000\n",
      "Average classification loss at step 0: 0.694247, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997303, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016671, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016705, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017712, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.019290, lr 0.250000\n",
      "Average classification loss at step 0: 0.694247, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052152, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017782, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016867, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018387, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018802, lr 0.250000\n",
      "Average classification loss at step 0: 0.694247, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032683, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016914, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016532, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018261, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016893, lr 0.250000\n",
      "Average classification loss at step 0: 0.694246, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030635, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016800, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018046, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018876, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018762, lr 0.250000\n",
      "Average classification loss at step 0: 0.694246, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985809, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018154, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016529, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017576, lr 0.250000\n",
      "Average classification loss at step 0: 0.694246, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968715, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017141, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016219, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018488, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018365, lr 0.250000\n",
      "Average classification loss at step 0: 0.694246, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991401, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017740, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015350, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017204, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017210, lr 0.250000\n",
      "Average classification loss at step 0: 0.694245, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987617, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017174, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018206, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017301, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017314, lr 0.250000\n",
      "Average classification loss at step 0: 0.694245, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006515, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016706, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018350, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016242, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017542, lr 0.250000\n",
      "Average classification loss at step 0: 0.694245, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028398, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017326, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017519, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016358, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017108, lr 0.250000\n",
      "Average classification loss at step 0: 0.694244, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030872, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018128, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018224, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016240, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017996, lr 0.250000\n",
      "Average classification loss at step 0: 0.694244, lr 0.250000\n",
      "Average embedding loss at step 0: 1.962438, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017777, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017196, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017461, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017661, lr 0.250000\n",
      "Average classification loss at step 0: 0.694244, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998227, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016639, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016977, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017449, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017104, lr 0.250000\n",
      "Average classification loss at step 0: 0.694243, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004178, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017842, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017810, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018122, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018092, lr 0.250000\n",
      "Average classification loss at step 0: 0.694243, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034166, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018582, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017842, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018637, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017386, lr 0.250000\n",
      "Average classification loss at step 0: 0.694243, lr 0.250000\n",
      "Average embedding loss at step 0: 1.979245, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017177, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018209, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017365, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017728, lr 0.250000\n",
      "Average classification loss at step 0: 0.694243, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993096, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017803, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017660, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017646, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016602, lr 0.250000\n",
      "Average classification loss at step 0: 0.694243, lr 0.250000\n",
      "Average embedding loss at step 0: 1.979570, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017103, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017897, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017418, lr 0.250000\n",
      "Average classification loss at step 0: 0.694242, lr 0.250000\n",
      "Average embedding loss at step 0: 1.941615, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017124, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016931, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016759, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017459, lr 0.250000\n",
      "Average classification loss at step 0: 0.694242, lr 0.250000\n",
      "Average embedding loss at step 0: 2.122921, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017236, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017759, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016760, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016466, lr 0.250000\n",
      "Average classification loss at step 0: 0.694242, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045442, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017578, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018507, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017364, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017835, lr 0.250000\n",
      "Average classification loss at step 0: 0.694242, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965682, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017115, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017809, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018029, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016519, lr 0.250000\n",
      "Average classification loss at step 0: 0.694241, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036681, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017446, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017959, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017934, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016754, lr 0.250000\n",
      "Average classification loss at step 0: 0.694241, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037501, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017029, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016993, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018873, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017109, lr 0.250000\n",
      "Average classification loss at step 0: 0.694241, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021029, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016879, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016559, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017841, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015595, lr 0.250000\n",
      "Average classification loss at step 0: 0.694241, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998077, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017327, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018137, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016972, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015891, lr 0.250000\n",
      "Average classification loss at step 0: 0.694241, lr 0.250000\n",
      "Average embedding loss at step 0: 1.948718, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017769, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016510, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016515, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016385, lr 0.250000\n",
      "Average classification loss at step 0: 0.694240, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034425, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017115, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016612, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016978, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018319, lr 0.250000\n",
      "Average classification loss at step 0: 0.694240, lr 0.250000\n",
      "Average embedding loss at step 0: 1.988899, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017136, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017548, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015602, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017148, lr 0.250000\n",
      "Average classification loss at step 0: 0.694240, lr 0.250000\n",
      "Average embedding loss at step 0: 1.907642, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018746, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017105, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017593, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017354, lr 0.250000\n",
      "Average classification loss at step 0: 0.694240, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976352, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017729, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017385, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.019152, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016469, lr 0.250000\n",
      "Average classification loss at step 0: 0.694239, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991024, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017438, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016322, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015628, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017213, lr 0.250000\n",
      "Average classification loss at step 0: 0.694239, lr 0.250000\n",
      "Average embedding loss at step 0: 1.947643, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017549, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017689, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017235, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017706, lr 0.250000\n",
      "Average classification loss at step 0: 0.694239, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013196, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014968, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015446, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017589, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017400, lr 0.250000\n",
      "Average classification loss at step 0: 0.694239, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038929, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016857, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018676, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016715, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016742, lr 0.250000\n",
      "Average classification loss at step 0: 0.694239, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072854, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016238, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018163, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016902, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017048, lr 0.250000\n",
      "Average classification loss at step 0: 0.694239, lr 0.250000\n",
      "Average embedding loss at step 0: 1.964475, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017139, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016708, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016595, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017018, lr 0.250000\n",
      "Average classification loss at step 0: 0.694238, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044152, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016653, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018493, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017918, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017391, lr 0.250000\n",
      "Average classification loss at step 0: 0.694238, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035185, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016953, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017390, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017462, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016402, lr 0.250000\n",
      "Average classification loss at step 0: 0.694238, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006113, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017527, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018863, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016437, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017053, lr 0.250000\n",
      "Average classification loss at step 0: 0.694238, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012890, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015648, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017522, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016063, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016939, lr 0.250000\n",
      "Average classification loss at step 0: 0.694238, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052083, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017514, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017128, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017343, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015816, lr 0.250000\n",
      "Average classification loss at step 0: 0.694237, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043251, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015094, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017738, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016616, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015540, lr 0.250000\n",
      "Average classification loss at step 0: 0.694237, lr 0.250000\n",
      "Average embedding loss at step 0: 2.076119, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017222, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016451, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016065, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015877, lr 0.250000\n",
      "Average classification loss at step 0: 0.694237, lr 0.250000\n",
      "Average embedding loss at step 0: 2.090367, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017182, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016511, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017422, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016836, lr 0.250000\n",
      "Average classification loss at step 0: 0.694237, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985504, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016942, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017350, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017278, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017935, lr 0.250000\n",
      "Average classification loss at step 0: 0.694237, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033830, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017254, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017821, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015843, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017297, lr 0.250000\n",
      "Average classification loss at step 0: 0.694236, lr 0.250000\n",
      "Average embedding loss at step 0: 2.069621, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016334, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017317, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015819, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015836, lr 0.250000\n",
      "Average classification loss at step 0: 0.694236, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034015, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017157, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016842, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016047, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016849, lr 0.250000\n",
      "Average classification loss at step 0: 0.694236, lr 0.250000\n",
      "Average embedding loss at step 0: 2.059484, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017018, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016624, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017338, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016650, lr 0.250000\n",
      "Average classification loss at step 0: 0.694236, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003977, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018539, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015242, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016528, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015117, lr 0.250000\n",
      "Average classification loss at step 0: 0.694236, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011557, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016981, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017451, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017266, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016373, lr 0.250000\n",
      "Average classification loss at step 0: 0.694236, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030935, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016410, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016658, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016572, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017296, lr 0.250000\n",
      "Average classification loss at step 0: 0.694235, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044095, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017598, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015665, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016794, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017304, lr 0.250000\n",
      "Average classification loss at step 0: 0.694235, lr 0.250000\n",
      "Average embedding loss at step 0: 1.948706, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016144, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016483, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017091, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016241, lr 0.250000\n",
      "Average classification loss at step 0: 0.694235, lr 0.250000\n",
      "Average embedding loss at step 0: 2.008249, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016716, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017209, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.018121, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017075, lr 0.250000\n",
      "Average classification loss at step 0: 0.694235, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046079, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015938, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017180, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016877, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016894, lr 0.250000\n",
      "Average classification loss at step 0: 0.694235, lr 0.250000\n",
      "Average embedding loss at step 0: 1.941762, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017957, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016167, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015838, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018626, lr 0.250000\n",
      "Average classification loss at step 0: 0.694235, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064018, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016193, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016605, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016414, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017614, lr 0.250000\n",
      "Average classification loss at step 0: 0.694234, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072153, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016121, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017010, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015870, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016245, lr 0.250000\n",
      "Average classification loss at step 0: 0.694234, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054091, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015466, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016235, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017235, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015868, lr 0.250000\n",
      "Average classification loss at step 0: 0.694234, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054692, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017110, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017531, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017138, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017256, lr 0.250000\n",
      "Average classification loss at step 0: 0.694234, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985057, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017581, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017083, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015760, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015937, lr 0.250000\n",
      "Average classification loss at step 0: 0.694234, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029424, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017572, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016665, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016356, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017247, lr 0.250000\n",
      "Average classification loss at step 0: 0.694233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026463, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017012, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016525, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017148, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017828, lr 0.250000\n",
      "Average classification loss at step 0: 0.694233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065918, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016718, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014852, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015766, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018043, lr 0.250000\n",
      "Average classification loss at step 0: 0.694233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015832, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016156, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017141, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017177, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015631, lr 0.250000\n",
      "Average classification loss at step 0: 0.694233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061825, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015513, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017161, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015817, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016129, lr 0.250000\n",
      "Average classification loss at step 0: 0.694233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028327, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015777, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017884, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015367, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016877, lr 0.250000\n",
      "Average classification loss at step 0: 0.694233, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036164, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016055, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015894, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016316, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016129, lr 0.250000\n",
      "Average classification loss at step 0: 0.694232, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015619, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015936, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017613, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016016, lr 0.250000\n",
      "Average classification loss at step 0: 0.694232, lr 0.250000\n",
      "Average embedding loss at step 0: 1.972691, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014868, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017799, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017514, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018606, lr 0.250000\n",
      "Average classification loss at step 0: 0.694232, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064475, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016389, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.018019, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015968, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015652, lr 0.250000\n",
      "Average classification loss at step 0: 0.694232, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062002, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016514, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016939, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016318, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017176, lr 0.250000\n",
      "Average classification loss at step 0: 0.694232, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066245, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016584, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015738, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016004, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015908, lr 0.250000\n",
      "Average classification loss at step 0: 0.694232, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033469, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016543, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017226, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017421, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016447, lr 0.250000\n",
      "Average classification loss at step 0: 0.694231, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978934, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016857, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016501, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015729, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.018511, lr 0.250000\n",
      "Average classification loss at step 0: 0.694231, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001004, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017100, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015786, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017245, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016097, lr 0.250000\n",
      "Average classification loss at step 0: 0.694231, lr 0.250000\n",
      "Average embedding loss at step 0: 2.080048, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016164, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017032, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015515, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017572, lr 0.250000\n",
      "Average classification loss at step 0: 0.694231, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024046, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015822, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016178, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016248, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016644, lr 0.250000\n",
      "Average classification loss at step 0: 0.694231, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011658, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017256, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017161, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017313, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016468, lr 0.250000\n",
      "Average classification loss at step 0: 0.694230, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981220, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015179, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016839, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015881, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016679, lr 0.250000\n",
      "Average classification loss at step 0: 0.694230, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014755, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017536, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017115, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015686, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016516, lr 0.250000\n",
      "Average classification loss at step 0: 0.694230, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011233, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.018095, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016211, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017251, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016182, lr 0.250000\n",
      "Average classification loss at step 0: 0.694230, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027067, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016476, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016333, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016287, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015702, lr 0.250000\n",
      "Average classification loss at step 0: 0.694230, lr 0.250000\n",
      "Average embedding loss at step 0: 1.986344, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016730, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016382, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015267, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016741, lr 0.250000\n",
      "Average classification loss at step 0: 0.694229, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013882, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017966, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016827, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017408, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017057, lr 0.250000\n",
      "Average classification loss at step 0: 0.694229, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967355, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015644, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016142, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017287, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016073, lr 0.250000\n",
      "Average classification loss at step 0: 0.694229, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983663, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016624, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014778, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017015, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014799, lr 0.250000\n",
      "Average classification loss at step 0: 0.694229, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020960, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016673, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015321, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016777, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016695, lr 0.250000\n",
      "Average classification loss at step 0: 0.694229, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028065, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014644, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015730, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016476, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015398, lr 0.250000\n",
      "Average classification loss at step 0: 0.694228, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994646, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015388, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017626, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016121, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016475, lr 0.250000\n",
      "Average classification loss at step 0: 0.694228, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981049, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016104, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014925, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016585, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017695, lr 0.250000\n",
      "Average classification loss at step 0: 0.694228, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062531, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016248, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015590, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016226, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016709, lr 0.250000\n",
      "Average classification loss at step 0: 0.694228, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026204, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016513, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016902, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015718, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016430, lr 0.250000\n",
      "Average classification loss at step 0: 0.694228, lr 0.250000\n",
      "Average embedding loss at step 0: 1.955842, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015331, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015410, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016490, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017348, lr 0.250000\n",
      "Average classification loss at step 0: 0.694227, lr 0.250000\n",
      "Average embedding loss at step 0: 1.952880, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016880, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016050, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015191, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016709, lr 0.250000\n",
      "Average classification loss at step 0: 0.694227, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024355, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016212, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015757, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017654, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016069, lr 0.250000\n",
      "Average classification loss at step 0: 0.694227, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026769, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016566, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017005, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015789, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015791, lr 0.250000\n",
      "Average classification loss at step 0: 0.694227, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019448, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015040, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014726, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016588, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016584, lr 0.250000\n",
      "Average classification loss at step 0: 0.694227, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013770, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015411, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015417, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016159, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016922, lr 0.250000\n",
      "Average classification loss at step 0: 0.694226, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009565, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015147, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015448, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016465, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016467, lr 0.250000\n",
      "Average classification loss at step 0: 0.694226, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035279, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014926, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016525, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017708, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015218, lr 0.250000\n",
      "Average classification loss at step 0: 0.694226, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013317, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017217, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015449, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015778, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016965, lr 0.250000\n",
      "Average classification loss at step 0: 0.694226, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017022, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016768, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016884, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016181, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016580, lr 0.250000\n",
      "Average classification loss at step 0: 0.694226, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038766, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016730, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016655, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014500, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016635, lr 0.250000\n",
      "Average classification loss at step 0: 0.694225, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057977, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017012, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016344, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015527, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016257, lr 0.250000\n",
      "Average classification loss at step 0: 0.694225, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007946, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016237, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014874, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017177, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016146, lr 0.250000\n",
      "Average classification loss at step 0: 0.694225, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068387, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015209, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013858, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016075, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015910, lr 0.250000\n",
      "Average classification loss at step 0: 0.694225, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028918, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016899, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014975, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016737, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014531, lr 0.250000\n",
      "Average classification loss at step 0: 0.694225, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055849, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015846, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016242, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014912, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015012, lr 0.250000\n",
      "Average classification loss at step 0: 0.694224, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020304, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015880, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015269, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015262, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015820, lr 0.250000\n",
      "Average classification loss at step 0: 0.694224, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072351, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015436, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014139, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017077, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017333, lr 0.250000\n",
      "Average classification loss at step 0: 0.694224, lr 0.250000\n",
      "Average embedding loss at step 0: 2.048471, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016656, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017026, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017647, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015114, lr 0.250000\n",
      "Average classification loss at step 0: 0.694224, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010878, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015278, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015865, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015695, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015882, lr 0.250000\n",
      "Average classification loss at step 0: 0.694223, lr 0.250000\n",
      "Average embedding loss at step 0: 2.082709, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016580, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014897, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016572, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014443, lr 0.250000\n",
      "Average classification loss at step 0: 0.694223, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994602, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016551, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015854, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016597, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015643, lr 0.250000\n",
      "Average classification loss at step 0: 0.694223, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051621, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016027, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016894, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016090, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016651, lr 0.250000\n",
      "Average classification loss at step 0: 0.694223, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998898, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017161, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016760, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015148, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016526, lr 0.250000\n",
      "Average classification loss at step 0: 0.694222, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992848, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016498, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015904, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016656, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015700, lr 0.250000\n",
      "Average classification loss at step 0: 0.694222, lr 0.250000\n",
      "Average embedding loss at step 0: 2.047667, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016140, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015946, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016076, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015359, lr 0.250000\n",
      "Average classification loss at step 0: 0.694222, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002419, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015933, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016298, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016495, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016975, lr 0.250000\n",
      "Average classification loss at step 0: 0.694222, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017648, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016406, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015123, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015203, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015595, lr 0.250000\n",
      "Average classification loss at step 0: 0.694221, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004320, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014674, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016346, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016632, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014349, lr 0.250000\n",
      "Average classification loss at step 0: 0.694221, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983416, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014425, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016379, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015842, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016854, lr 0.250000\n",
      "Average classification loss at step 0: 0.694221, lr 0.250000\n",
      "Average embedding loss at step 0: 1.956931, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015784, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015302, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016623, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016664, lr 0.250000\n",
      "Average classification loss at step 0: 0.694220, lr 0.250000\n",
      "Average embedding loss at step 0: 1.953473, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015488, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016378, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015707, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014961, lr 0.250000\n",
      "Average classification loss at step 0: 0.694220, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978880, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016928, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017449, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015600, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015657, lr 0.250000\n",
      "Average classification loss at step 0: 0.694220, lr 0.250000\n",
      "Average embedding loss at step 0: 2.050157, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016091, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014929, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015660, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016328, lr 0.250000\n",
      "Average classification loss at step 0: 0.694219, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995390, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015113, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017032, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016812, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015086, lr 0.250000\n",
      "Average classification loss at step 0: 0.694219, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025474, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014948, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015765, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016406, lr 0.250000\n",
      "Average classification loss at step 0: 0.694219, lr 0.250000\n",
      "Average embedding loss at step 0: 2.098169, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016040, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015102, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014175, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016578, lr 0.250000\n",
      "Average classification loss at step 0: 0.694218, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984200, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015653, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015405, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016988, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015952, lr 0.250000\n",
      "Average classification loss at step 0: 0.694218, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965348, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015603, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014695, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015070, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014501, lr 0.250000\n",
      "Average classification loss at step 0: 0.694218, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020175, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015776, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015591, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015944, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014893, lr 0.250000\n",
      "Average classification loss at step 0: 0.694217, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018969, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016104, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016523, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015543, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015421, lr 0.250000\n",
      "Average classification loss at step 0: 0.694217, lr 0.250000\n",
      "Average embedding loss at step 0: 1.916523, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016277, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014601, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014887, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015428, lr 0.250000\n",
      "Average classification loss at step 0: 0.694216, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029479, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016823, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016117, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015949, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016111, lr 0.250000\n",
      "Average classification loss at step 0: 0.694216, lr 0.250000\n",
      "Average embedding loss at step 0: 1.962218, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016414, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016650, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016378, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015849, lr 0.250000\n",
      "Average classification loss at step 0: 0.694215, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041547, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014930, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016907, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015564, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015419, lr 0.250000\n",
      "Average classification loss at step 0: 0.694215, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998030, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015298, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016990, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014289, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015518, lr 0.250000\n",
      "Average classification loss at step 0: 0.694215, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992032, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015630, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016097, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015858, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015470, lr 0.250000\n",
      "Average classification loss at step 0: 0.694214, lr 0.250000\n",
      "Average embedding loss at step 0: 1.966944, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015572, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015053, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015932, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014678, lr 0.250000\n",
      "Average classification loss at step 0: 0.694214, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963605, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014533, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014684, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015306, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017119, lr 0.250000\n",
      "Average classification loss at step 0: 0.694213, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009407, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015900, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016632, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016937, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015386, lr 0.250000\n",
      "Average classification loss at step 0: 0.694213, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975188, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014468, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016010, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014646, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015174, lr 0.250000\n",
      "Average classification loss at step 0: 0.694212, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039339, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016152, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015756, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015161, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015514, lr 0.250000\n",
      "Average classification loss at step 0: 0.694212, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056065, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016531, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017514, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014867, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016999, lr 0.250000\n",
      "Average classification loss at step 0: 0.694211, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054287, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016296, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015798, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013450, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015656, lr 0.250000\n",
      "Average classification loss at step 0: 0.694211, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042030, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015676, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014523, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013978, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014497, lr 0.250000\n",
      "Average classification loss at step 0: 0.694210, lr 0.250000\n",
      "Average embedding loss at step 0: 1.938666, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013944, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015475, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015339, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015885, lr 0.250000\n",
      "Average classification loss at step 0: 0.694210, lr 0.250000\n",
      "Average embedding loss at step 0: 1.962478, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015891, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015521, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016034, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015244, lr 0.250000\n",
      "Average classification loss at step 0: 0.694209, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000275, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015706, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015094, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014378, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013924, lr 0.250000\n",
      "Average classification loss at step 0: 0.694209, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062781, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016379, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013888, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015316, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016518, lr 0.250000\n",
      "Average classification loss at step 0: 0.694208, lr 0.250000\n",
      "Average embedding loss at step 0: 1.936857, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016573, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015982, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016592, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015624, lr 0.250000\n",
      "Average classification loss at step 0: 0.694207, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019452, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015447, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013600, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014961, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014995, lr 0.250000\n",
      "Average classification loss at step 0: 0.694207, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965547, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016356, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016070, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015187, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015590, lr 0.250000\n",
      "Average classification loss at step 0: 0.694206, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012976, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016025, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015280, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016165, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014773, lr 0.250000\n",
      "Average classification loss at step 0: 0.694206, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011845, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016538, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015636, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016529, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015652, lr 0.250000\n",
      "Average classification loss at step 0: 0.694205, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963112, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016686, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016387, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016507, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014926, lr 0.250000\n",
      "Average classification loss at step 0: 0.694204, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990839, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015469, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015874, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015656, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016163, lr 0.250000\n",
      "Average classification loss at step 0: 0.694204, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975629, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015772, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015426, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015067, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015424, lr 0.250000\n",
      "Average classification loss at step 0: 0.694203, lr 0.250000\n",
      "Average embedding loss at step 0: 1.982644, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015470, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016074, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015135, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015468, lr 0.250000\n",
      "Average classification loss at step 0: 0.694202, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025139, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014796, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015907, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016043, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014827, lr 0.250000\n",
      "Average classification loss at step 0: 0.694202, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968849, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015895, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016828, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015680, lr 0.250000\n",
      "Average classification loss at step 0: 0.694201, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992102, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014441, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014526, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015305, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.017441, lr 0.250000\n",
      "Average classification loss at step 0: 0.694200, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984578, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014888, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014538, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015270, lr 0.250000\n",
      "Average classification loss at step 0: 0.694199, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038980, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013183, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015709, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015004, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013724, lr 0.250000\n",
      "Average classification loss at step 0: 0.694199, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044216, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015828, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015294, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.017307, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015675, lr 0.250000\n",
      "Average classification loss at step 0: 0.694198, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020843, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.017092, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016539, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013465, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016053, lr 0.250000\n",
      "Average classification loss at step 0: 0.694197, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058483, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015731, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013611, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016071, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015916, lr 0.250000\n",
      "Average classification loss at step 0: 0.694196, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014898, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013966, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015988, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016069, lr 0.250000\n",
      "Average classification loss at step 0: 0.694195, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018980, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015604, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015773, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015247, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014815, lr 0.250000\n",
      "Average classification loss at step 0: 0.694195, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981394, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013911, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014906, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014975, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015002, lr 0.250000\n",
      "Average classification loss at step 0: 0.694194, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987494, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015438, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015282, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014712, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014043, lr 0.250000\n",
      "Average classification loss at step 0: 0.694193, lr 0.250000\n",
      "Average embedding loss at step 0: 1.890887, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014238, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016084, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013709, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015588, lr 0.250000\n",
      "Average classification loss at step 0: 0.694192, lr 0.250000\n",
      "Average embedding loss at step 0: 1.943242, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014584, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016403, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016955, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015130, lr 0.250000\n",
      "Average classification loss at step 0: 0.694191, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038260, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013890, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013998, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014988, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015106, lr 0.250000\n",
      "Average classification loss at step 0: 0.694190, lr 0.250000\n",
      "Average embedding loss at step 0: 2.070130, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014534, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017043, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015523, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015727, lr 0.250000\n",
      "Average classification loss at step 0: 0.694189, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965305, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016326, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015350, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016602, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016864, lr 0.250000\n",
      "Average classification loss at step 0: 0.694188, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005474, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015813, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015237, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016559, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015369, lr 0.250000\n",
      "Average classification loss at step 0: 0.694187, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037543, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015211, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013597, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015815, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014722, lr 0.250000\n",
      "Average classification loss at step 0: 0.694186, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991786, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014788, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016414, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013872, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016791, lr 0.250000\n",
      "Average classification loss at step 0: 0.694185, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978999, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015286, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016298, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014489, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015297, lr 0.250000\n",
      "Average classification loss at step 0: 0.694184, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012052, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015574, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015041, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014047, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013896, lr 0.250000\n",
      "Average classification loss at step 0: 0.694183, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994899, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015942, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014933, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016166, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016436, lr 0.250000\n",
      "Average classification loss at step 0: 0.694181, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015016, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015176, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014398, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014211, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014537, lr 0.250000\n",
      "Average classification loss at step 0: 0.694180, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007045, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013878, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016357, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015652, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015220, lr 0.250000\n",
      "Average classification loss at step 0: 0.694179, lr 0.250000\n",
      "Average embedding loss at step 0: 1.918992, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015005, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015281, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016745, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015658, lr 0.250000\n",
      "Average classification loss at step 0: 0.694178, lr 0.250000\n",
      "Average embedding loss at step 0: 1.988313, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014933, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016252, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015158, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014917, lr 0.250000\n",
      "Average classification loss at step 0: 0.694176, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037500, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015917, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014767, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014405, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013086, lr 0.250000\n",
      "Average classification loss at step 0: 0.694175, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974827, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015204, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013469, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014964, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014843, lr 0.250000\n",
      "Average classification loss at step 0: 0.694174, lr 0.250000\n",
      "Average embedding loss at step 0: 1.957443, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014700, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014817, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014493, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015697, lr 0.250000\n",
      "Average classification loss at step 0: 0.694173, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025604, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014287, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016048, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014817, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015040, lr 0.250000\n",
      "Average classification loss at step 0: 0.694171, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036512, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014833, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016154, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015623, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014595, lr 0.250000\n",
      "Average classification loss at step 0: 0.694170, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030995, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015517, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015631, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016514, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014699, lr 0.250000\n",
      "Average classification loss at step 0: 0.694169, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963850, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015353, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014801, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013703, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015138, lr 0.250000\n",
      "Average classification loss at step 0: 0.694167, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031128, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016449, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015165, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014659, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016426, lr 0.250000\n",
      "Average classification loss at step 0: 0.694166, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989016, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013546, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014230, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015348, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014577, lr 0.250000\n",
      "Average classification loss at step 0: 0.694164, lr 0.250000\n",
      "Average embedding loss at step 0: 2.071707, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015744, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015406, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014437, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015127, lr 0.250000\n",
      "Average classification loss at step 0: 0.694163, lr 0.250000\n",
      "Average embedding loss at step 0: 1.959666, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014363, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016750, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015162, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016138, lr 0.250000\n",
      "Average classification loss at step 0: 0.694161, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036325, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015380, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014972, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015359, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015035, lr 0.250000\n",
      "Average classification loss at step 0: 0.694160, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002069, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013875, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015449, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014981, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015065, lr 0.250000\n",
      "Average classification loss at step 0: 0.694158, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998922, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015335, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016332, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015878, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014582, lr 0.250000\n",
      "Average classification loss at step 0: 0.694156, lr 0.250000\n",
      "Average embedding loss at step 0: 2.100039, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015576, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016385, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014337, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014857, lr 0.250000\n",
      "Average classification loss at step 0: 0.694155, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012140, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014240, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015332, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015188, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014505, lr 0.250000\n",
      "Average classification loss at step 0: 0.694153, lr 0.250000\n",
      "Average embedding loss at step 0: 1.966556, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016751, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014542, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014773, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014273, lr 0.250000\n",
      "Average classification loss at step 0: 0.694151, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058182, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014080, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.017166, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014779, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015064, lr 0.250000\n",
      "Average classification loss at step 0: 0.694149, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013831, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014190, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015139, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014377, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015396, lr 0.250000\n",
      "Average classification loss at step 0: 0.694148, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036772, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014090, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015143, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016502, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015104, lr 0.250000\n",
      "Average classification loss at step 0: 0.694146, lr 0.250000\n",
      "Average embedding loss at step 0: 2.084250, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014191, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015389, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015067, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014666, lr 0.250000\n",
      "Average classification loss at step 0: 0.694144, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016086, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015482, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015061, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014752, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015751, lr 0.250000\n",
      "Average classification loss at step 0: 0.694142, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981530, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014793, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013989, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014829, lr 0.250000\n",
      "Average classification loss at step 0: 0.694140, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984018, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014483, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015010, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014718, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013936, lr 0.250000\n",
      "Average classification loss at step 0: 0.694138, lr 0.250000\n",
      "Average embedding loss at step 0: 2.105466, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015172, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013374, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016029, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014866, lr 0.250000\n",
      "Average classification loss at step 0: 0.694136, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006726, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016038, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015271, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014363, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014021, lr 0.250000\n",
      "Average classification loss at step 0: 0.694134, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012174, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013753, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015507, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016036, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014068, lr 0.250000\n",
      "Average classification loss at step 0: 0.694132, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045635, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015541, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013468, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014772, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015524, lr 0.250000\n",
      "Average classification loss at step 0: 0.694130, lr 0.250000\n",
      "Average embedding loss at step 0: 1.979745, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015660, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013790, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015236, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013524, lr 0.250000\n",
      "Average classification loss at step 0: 0.694128, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030412, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014283, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014370, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015283, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016245, lr 0.250000\n",
      "Average classification loss at step 0: 0.694126, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020598, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014456, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015990, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015006, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015445, lr 0.250000\n",
      "Average classification loss at step 0: 0.694123, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019639, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014685, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014421, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015049, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015369, lr 0.250000\n",
      "Average classification loss at step 0: 0.694121, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996772, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016102, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014013, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013369, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015979, lr 0.250000\n",
      "Average classification loss at step 0: 0.694119, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055379, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015167, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015150, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015530, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014820, lr 0.250000\n",
      "Average classification loss at step 0: 0.694117, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032416, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015125, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014132, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015291, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015236, lr 0.250000\n",
      "Average classification loss at step 0: 0.694115, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983819, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013138, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014112, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014966, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013270, lr 0.250000\n",
      "Average classification loss at step 0: 0.694112, lr 0.250000\n",
      "Average embedding loss at step 0: 1.959535, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015709, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015554, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014691, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015073, lr 0.250000\n",
      "Average classification loss at step 0: 0.694110, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016938, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015442, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013868, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014206, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014072, lr 0.250000\n",
      "Average classification loss at step 0: 0.694108, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009747, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013976, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015159, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015908, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015225, lr 0.250000\n",
      "Average classification loss at step 0: 0.694105, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055277, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015216, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014721, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015815, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015653, lr 0.250000\n",
      "Average classification loss at step 0: 0.694103, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021549, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015401, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013184, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014459, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016153, lr 0.250000\n",
      "Average classification loss at step 0: 0.694100, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036233, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014451, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015070, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014909, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013504, lr 0.250000\n",
      "Average classification loss at step 0: 0.694098, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991599, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013554, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016031, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013272, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014628, lr 0.250000\n",
      "Average classification loss at step 0: 0.694095, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984608, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013744, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015440, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014306, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015199, lr 0.250000\n",
      "Average classification loss at step 0: 0.694093, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035758, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015398, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013941, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014263, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013472, lr 0.250000\n",
      "Average classification loss at step 0: 0.694090, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016080, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014559, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014268, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015523, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014359, lr 0.250000\n",
      "Average classification loss at step 0: 0.694088, lr 0.250000\n",
      "Average embedding loss at step 0: 2.070542, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012709, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015885, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013926, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016840, lr 0.250000\n",
      "Average classification loss at step 0: 0.694086, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034916, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015877, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016185, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015078, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014296, lr 0.250000\n",
      "Average classification loss at step 0: 0.694084, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025279, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015346, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014460, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014606, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014482, lr 0.250000\n",
      "Average classification loss at step 0: 0.694081, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027877, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013959, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013661, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013835, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014999, lr 0.250000\n",
      "Average classification loss at step 0: 0.694079, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057251, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015053, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015364, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014871, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014592, lr 0.250000\n",
      "Average classification loss at step 0: 0.694077, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046182, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015100, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014529, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015358, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014338, lr 0.250000\n",
      "Average classification loss at step 0: 0.694075, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028227, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014779, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014646, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015078, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014060, lr 0.250000\n",
      "Average classification loss at step 0: 0.694073, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998198, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015071, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015234, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014287, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016201, lr 0.250000\n",
      "Average classification loss at step 0: 0.694071, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032771, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014058, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015007, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015323, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014676, lr 0.250000\n",
      "Average classification loss at step 0: 0.694069, lr 0.250000\n",
      "Average embedding loss at step 0: 2.086227, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015373, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016207, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014793, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014629, lr 0.250000\n",
      "Average classification loss at step 0: 0.694067, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019759, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013235, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014133, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015188, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014752, lr 0.250000\n",
      "Average classification loss at step 0: 0.694064, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042069, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016875, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016007, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013477, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015216, lr 0.250000\n",
      "Average classification loss at step 0: 0.694062, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992146, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015355, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014520, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015767, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014204, lr 0.250000\n",
      "Average classification loss at step 0: 0.694060, lr 0.250000\n",
      "Average embedding loss at step 0: 2.099419, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014581, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014975, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013196, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013699, lr 0.250000\n",
      "Average classification loss at step 0: 0.694058, lr 0.250000\n",
      "Average embedding loss at step 0: 2.048903, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013965, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012922, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013698, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015342, lr 0.250000\n",
      "Average classification loss at step 0: 0.694055, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998808, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015498, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014746, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015922, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013971, lr 0.250000\n",
      "Average classification loss at step 0: 0.694052, lr 0.250000\n",
      "Average embedding loss at step 0: 1.971708, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013737, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014183, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015482, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014077, lr 0.250000\n",
      "Average classification loss at step 0: 0.694050, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968584, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015703, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013391, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014554, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013520, lr 0.250000\n",
      "Average classification loss at step 0: 0.694047, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009678, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016509, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014477, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015014, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015205, lr 0.250000\n",
      "Average classification loss at step 0: 0.694045, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036173, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014194, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014794, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014767, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015048, lr 0.250000\n",
      "Average classification loss at step 0: 0.694042, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994570, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013857, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015652, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013854, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014506, lr 0.250000\n",
      "Average classification loss at step 0: 0.694039, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022129, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014251, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013169, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014623, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013986, lr 0.250000\n",
      "Average classification loss at step 0: 0.694037, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066025, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015097, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013427, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013894, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014856, lr 0.250000\n",
      "Average classification loss at step 0: 0.694035, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995817, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015226, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014805, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014060, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013691, lr 0.250000\n",
      "Average classification loss at step 0: 0.694032, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007338, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013261, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014082, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014270, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015455, lr 0.250000\n",
      "Average classification loss at step 0: 0.694029, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978586, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015302, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013482, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015546, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015185, lr 0.250000\n",
      "Average classification loss at step 0: 0.694026, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057549, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015463, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014534, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015857, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014551, lr 0.250000\n",
      "Average classification loss at step 0: 0.694024, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035704, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015153, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015829, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013470, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014431, lr 0.250000\n",
      "Average classification loss at step 0: 0.694021, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014071, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014805, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014757, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014421, lr 0.250000\n",
      "Average classification loss at step 0: 0.694019, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065560, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013802, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014697, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014258, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014114, lr 0.250000\n",
      "Average classification loss at step 0: 0.694016, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998379, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014230, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014116, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014315, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015101, lr 0.250000\n",
      "Average classification loss at step 0: 0.694014, lr 0.250000\n",
      "Average embedding loss at step 0: 2.050976, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013183, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014569, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015615, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013527, lr 0.250000\n",
      "Average classification loss at step 0: 0.694011, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977239, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014921, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014669, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014382, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015096, lr 0.250000\n",
      "Average classification loss at step 0: 0.694008, lr 0.250000\n",
      "Average embedding loss at step 0: 1.986948, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014230, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014757, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014782, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014274, lr 0.250000\n",
      "Average classification loss at step 0: 0.694005, lr 0.250000\n",
      "Average embedding loss at step 0: 2.088330, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014924, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013953, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013976, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014095, lr 0.250000\n",
      "Average classification loss at step 0: 0.694002, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020576, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014811, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013705, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013686, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013484, lr 0.250000\n",
      "Average classification loss at step 0: 0.693999, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036293, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013166, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013875, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014625, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014144, lr 0.250000\n",
      "Average classification loss at step 0: 0.693996, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999903, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014250, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012001, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014862, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013651, lr 0.250000\n",
      "Average classification loss at step 0: 0.693993, lr 0.250000\n",
      "Average embedding loss at step 0: 1.979604, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014516, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013875, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015474, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013676, lr 0.250000\n",
      "Average classification loss at step 0: 0.693990, lr 0.250000\n",
      "Average embedding loss at step 0: 2.008354, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013932, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015785, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014375, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014605, lr 0.250000\n",
      "Average classification loss at step 0: 0.693988, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024708, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013749, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013762, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013765, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014037, lr 0.250000\n",
      "Average classification loss at step 0: 0.693984, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006646, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015184, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013199, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015037, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014450, lr 0.250000\n",
      "Average classification loss at step 0: 0.693981, lr 0.250000\n",
      "Average embedding loss at step 0: 2.086445, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014349, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015182, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015384, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013097, lr 0.250000\n",
      "Average classification loss at step 0: 0.693978, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998703, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013897, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013762, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014065, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014125, lr 0.250000\n",
      "Average classification loss at step 0: 0.693975, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973074, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015342, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013117, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014957, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013285, lr 0.250000\n",
      "Average classification loss at step 0: 0.693972, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064944, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014293, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013233, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015764, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013909, lr 0.250000\n",
      "Average classification loss at step 0: 0.693969, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984923, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013148, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014732, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014009, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015090, lr 0.250000\n",
      "Average classification loss at step 0: 0.693966, lr 0.250000\n",
      "Average embedding loss at step 0: 1.921304, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014358, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015756, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014187, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015107, lr 0.250000\n",
      "Average classification loss at step 0: 0.693964, lr 0.250000\n",
      "Average embedding loss at step 0: 1.982463, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014085, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014768, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014183, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014755, lr 0.250000\n",
      "Average classification loss at step 0: 0.693961, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022096, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014386, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014405, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014677, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013141, lr 0.250000\n",
      "Average classification loss at step 0: 0.693958, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031169, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013494, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014967, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013910, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014986, lr 0.250000\n",
      "Average classification loss at step 0: 0.693954, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073791, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015096, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014253, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013455, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013603, lr 0.250000\n",
      "Average classification loss at step 0: 0.693951, lr 0.250000\n",
      "Average embedding loss at step 0: 1.964440, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013062, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015897, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.016456, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014383, lr 0.250000\n",
      "Average classification loss at step 0: 0.693947, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021611, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013482, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015196, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014998, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014731, lr 0.250000\n",
      "Average classification loss at step 0: 0.693944, lr 0.250000\n",
      "Average embedding loss at step 0: 2.063964, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012972, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014031, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014673, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013153, lr 0.250000\n",
      "Average classification loss at step 0: 0.693941, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073100, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014283, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014573, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014141, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013646, lr 0.250000\n",
      "Average classification loss at step 0: 0.693937, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024738, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014721, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015097, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015055, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014599, lr 0.250000\n",
      "Average classification loss at step 0: 0.693934, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983577, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014727, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013713, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013154, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015143, lr 0.250000\n",
      "Average classification loss at step 0: 0.693930, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004788, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014051, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014636, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014377, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013900, lr 0.250000\n",
      "Average classification loss at step 0: 0.693927, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006521, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013671, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014934, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014856, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014019, lr 0.250000\n",
      "Average classification loss at step 0: 0.693924, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985754, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014312, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013474, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013094, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014645, lr 0.250000\n",
      "Average classification loss at step 0: 0.693920, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021856, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014525, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014153, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015609, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014899, lr 0.250000\n",
      "Average classification loss at step 0: 0.693917, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999286, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014943, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012965, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012966, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014349, lr 0.250000\n",
      "Average classification loss at step 0: 0.693913, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023884, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014321, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012758, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014671, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013247, lr 0.250000\n",
      "Average classification loss at step 0: 0.693910, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030796, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013167, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015315, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014502, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013876, lr 0.250000\n",
      "Average classification loss at step 0: 0.693907, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960054, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014085, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013822, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013988, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014154, lr 0.250000\n",
      "Average classification loss at step 0: 0.693904, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001422, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014847, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015329, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012932, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013851, lr 0.250000\n",
      "Average classification loss at step 0: 0.693900, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016508, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014679, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013825, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014432, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013959, lr 0.250000\n",
      "Average classification loss at step 0: 0.693897, lr 0.250000\n",
      "Average embedding loss at step 0: 1.986448, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013879, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.016444, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014770, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013887, lr 0.250000\n",
      "Average classification loss at step 0: 0.693893, lr 0.250000\n",
      "Average embedding loss at step 0: 2.070674, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014630, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013329, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014049, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014211, lr 0.250000\n",
      "Average classification loss at step 0: 0.693890, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018863, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014629, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013839, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013908, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013886, lr 0.250000\n",
      "Average classification loss at step 0: 0.693886, lr 0.250000\n",
      "Average embedding loss at step 0: 2.074414, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014597, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013642, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013780, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013378, lr 0.250000\n",
      "Average classification loss at step 0: 0.693883, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037062, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013563, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013501, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013528, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014408, lr 0.250000\n",
      "Average classification loss at step 0: 0.693880, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994108, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014049, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014624, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015380, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014186, lr 0.250000\n",
      "Average classification loss at step 0: 0.693876, lr 0.250000\n",
      "Average embedding loss at step 0: 2.085293, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012935, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014706, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013654, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013184, lr 0.250000\n",
      "Average classification loss at step 0: 0.693872, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996225, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015055, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014478, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013710, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013689, lr 0.250000\n",
      "Average classification loss at step 0: 0.693869, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985262, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014610, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014781, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012820, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014424, lr 0.250000\n",
      "Average classification loss at step 0: 0.693864, lr 0.250000\n",
      "Average embedding loss at step 0: 1.907961, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015017, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013993, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014173, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013942, lr 0.250000\n",
      "Average classification loss at step 0: 0.693860, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039429, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013800, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013228, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012241, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013540, lr 0.250000\n",
      "Average classification loss at step 0: 0.693856, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020216, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012244, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013703, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013725, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013817, lr 0.250000\n",
      "Average classification loss at step 0: 0.693853, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049416, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014350, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013186, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015510, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013680, lr 0.250000\n",
      "Average classification loss at step 0: 0.693849, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017000, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014689, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012218, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012761, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011933, lr 0.250000\n",
      "Average classification loss at step 0: 0.693845, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012170, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015546, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013748, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013090, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015234, lr 0.250000\n",
      "Average classification loss at step 0: 0.693841, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018397, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015060, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014581, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014180, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014646, lr 0.250000\n",
      "Average classification loss at step 0: 0.693837, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028995, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014400, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013207, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014283, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013435, lr 0.250000\n",
      "Average classification loss at step 0: 0.693833, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997688, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014161, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015220, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013706, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014216, lr 0.250000\n",
      "Average classification loss at step 0: 0.693829, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992624, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014126, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013845, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014219, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014097, lr 0.250000\n",
      "Average classification loss at step 0: 0.693826, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027085, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014830, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012762, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013197, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013511, lr 0.250000\n",
      "Average classification loss at step 0: 0.693822, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974186, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016081, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013619, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015230, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013540, lr 0.250000\n",
      "Average classification loss at step 0: 0.693818, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967699, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014127, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014838, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014615, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014211, lr 0.250000\n",
      "Average classification loss at step 0: 0.693814, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029762, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015847, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013928, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014804, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013703, lr 0.250000\n",
      "Average classification loss at step 0: 0.693810, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021549, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014609, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014414, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014094, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013621, lr 0.250000\n",
      "Average classification loss at step 0: 0.693807, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984537, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013934, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014793, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015121, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013231, lr 0.250000\n",
      "Average classification loss at step 0: 0.693802, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992722, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014278, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014266, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015539, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014876, lr 0.250000\n",
      "Average classification loss at step 0: 0.693798, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041648, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013975, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013364, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013786, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015492, lr 0.250000\n",
      "Average classification loss at step 0: 0.693794, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006026, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.016073, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013454, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013052, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014879, lr 0.250000\n",
      "Average classification loss at step 0: 0.693790, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016874, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012451, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013324, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015256, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014722, lr 0.250000\n",
      "Average classification loss at step 0: 0.693786, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006184, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014644, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013750, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014485, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013771, lr 0.250000\n",
      "Average classification loss at step 0: 0.693782, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989114, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015142, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014251, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012600, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014414, lr 0.250000\n",
      "Average classification loss at step 0: 0.693778, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975600, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014337, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013922, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014595, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013519, lr 0.250000\n",
      "Average classification loss at step 0: 0.693774, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018251, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013423, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012528, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014534, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015724, lr 0.250000\n",
      "Average classification loss at step 0: 0.693769, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976729, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013730, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014127, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013298, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012817, lr 0.250000\n",
      "Average classification loss at step 0: 0.693765, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995950, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012888, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014694, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013359, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013584, lr 0.250000\n",
      "Average classification loss at step 0: 0.693760, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018859, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014294, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013537, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012275, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013290, lr 0.250000\n",
      "Average classification loss at step 0: 0.693756, lr 0.250000\n",
      "Average embedding loss at step 0: 2.083782, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013652, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014711, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012365, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014552, lr 0.250000\n",
      "Average classification loss at step 0: 0.693752, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973419, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013950, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014467, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014962, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013097, lr 0.250000\n",
      "Average classification loss at step 0: 0.693748, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034023, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013566, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012470, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012539, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013237, lr 0.250000\n",
      "Average classification loss at step 0: 0.693743, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975095, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013580, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015840, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015226, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012980, lr 0.250000\n",
      "Average classification loss at step 0: 0.693739, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033357, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015586, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015355, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015064, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.016040, lr 0.250000\n",
      "Average classification loss at step 0: 0.693735, lr 0.250000\n",
      "Average embedding loss at step 0: 1.944665, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014066, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014545, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014252, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014107, lr 0.250000\n",
      "Average classification loss at step 0: 0.693730, lr 0.250000\n",
      "Average embedding loss at step 0: 1.930472, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014447, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013967, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013900, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013484, lr 0.250000\n",
      "Average classification loss at step 0: 0.693726, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978133, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013960, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014511, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012921, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013594, lr 0.250000\n",
      "Average classification loss at step 0: 0.693722, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033564, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012782, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013705, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013159, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014213, lr 0.250000\n",
      "Average classification loss at step 0: 0.693717, lr 0.250000\n",
      "Average embedding loss at step 0: 1.948959, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014638, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013070, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013721, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012834, lr 0.250000\n",
      "Average classification loss at step 0: 0.693713, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040425, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014221, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013733, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014376, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014135, lr 0.250000\n",
      "Average classification loss at step 0: 0.693708, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046784, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014497, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014356, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014306, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013921, lr 0.250000\n",
      "Average classification loss at step 0: 0.693704, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011822, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013688, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014844, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014762, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012977, lr 0.250000\n",
      "Average classification loss at step 0: 0.693700, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967591, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013985, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015089, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015435, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013469, lr 0.250000\n",
      "Average classification loss at step 0: 0.693694, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007758, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012623, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014110, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015542, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012718, lr 0.250000\n",
      "Average classification loss at step 0: 0.693690, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997806, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014315, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013787, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011733, lr 0.250000\n",
      "Average classification loss at step 0: 0.693686, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994381, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013679, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013216, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013955, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013377, lr 0.250000\n",
      "Average classification loss at step 0: 0.693681, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027466, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012801, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013643, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012362, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014989, lr 0.250000\n",
      "Average classification loss at step 0: 0.693676, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051672, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013813, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012987, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013871, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013877, lr 0.250000\n",
      "Average classification loss at step 0: 0.693671, lr 0.250000\n",
      "Average embedding loss at step 0: 1.932658, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014328, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013996, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013189, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014087, lr 0.250000\n",
      "Average classification loss at step 0: 0.693667, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018034, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014418, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013597, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015532, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013160, lr 0.250000\n",
      "Average classification loss at step 0: 0.693662, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072052, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013390, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012598, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012488, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013610, lr 0.250000\n",
      "Average classification loss at step 0: 0.693657, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018195, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013766, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013760, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012797, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014095, lr 0.250000\n",
      "Average classification loss at step 0: 0.693653, lr 0.250000\n",
      "Average embedding loss at step 0: 1.936224, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012457, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013003, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014459, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014290, lr 0.250000\n",
      "Average classification loss at step 0: 0.693648, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991499, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013925, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015568, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013788, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014515, lr 0.250000\n",
      "Average classification loss at step 0: 0.693644, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042413, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012338, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015911, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014117, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014850, lr 0.250000\n",
      "Average classification loss at step 0: 0.693640, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013518, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013847, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013706, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013751, lr 0.250000\n",
      "Average classification loss at step 0: 0.693635, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996857, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013994, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014576, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013595, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013430, lr 0.250000\n",
      "Average classification loss at step 0: 0.693630, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983691, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013219, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013895, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014484, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012278, lr 0.250000\n",
      "Average classification loss at step 0: 0.693625, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037280, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015280, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013719, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013751, lr 0.250000\n",
      "Average classification loss at step 0: 0.693621, lr 0.250000\n",
      "Average embedding loss at step 0: 1.986593, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012685, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015204, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013050, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014139, lr 0.250000\n",
      "Average classification loss at step 0: 0.693616, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996074, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014010, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013229, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013867, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013802, lr 0.250000\n",
      "Average classification loss at step 0: 0.693612, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006727, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014349, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015194, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013035, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014427, lr 0.250000\n",
      "Average classification loss at step 0: 0.693607, lr 0.250000\n",
      "Average embedding loss at step 0: 1.958818, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013056, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013538, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013421, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013200, lr 0.250000\n",
      "Average classification loss at step 0: 0.693601, lr 0.250000\n",
      "Average embedding loss at step 0: 2.077909, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013733, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013341, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013667, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013874, lr 0.250000\n",
      "Average classification loss at step 0: 0.693596, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993906, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013739, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013198, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014268, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013497, lr 0.250000\n",
      "Average classification loss at step 0: 0.693591, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058707, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014485, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014193, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013317, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014531, lr 0.250000\n",
      "Average classification loss at step 0: 0.693587, lr 0.250000\n",
      "Average embedding loss at step 0: 2.070533, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014422, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012401, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012777, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013447, lr 0.250000\n",
      "Average classification loss at step 0: 0.693582, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004055, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013164, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013509, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013228, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012844, lr 0.250000\n",
      "Average classification loss at step 0: 0.693577, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999914, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013254, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012475, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013867, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013743, lr 0.250000\n",
      "Average classification loss at step 0: 0.693573, lr 0.250000\n",
      "Average embedding loss at step 0: 1.969239, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015445, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013025, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013848, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013269, lr 0.250000\n",
      "Average classification loss at step 0: 0.693568, lr 0.250000\n",
      "Average embedding loss at step 0: 1.922585, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012395, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014086, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014144, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012631, lr 0.250000\n",
      "Average classification loss at step 0: 0.693564, lr 0.250000\n",
      "Average embedding loss at step 0: 1.953462, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012579, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013270, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013789, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014746, lr 0.250000\n",
      "Average classification loss at step 0: 0.693559, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002056, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014386, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012651, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012664, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012971, lr 0.250000\n",
      "Average classification loss at step 0: 0.693554, lr 0.250000\n",
      "Average embedding loss at step 0: 1.944602, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013003, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013467, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014177, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014300, lr 0.250000\n",
      "Average classification loss at step 0: 0.693550, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011362, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012564, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014987, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012856, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014290, lr 0.250000\n",
      "Average classification loss at step 0: 0.693545, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032339, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013201, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014181, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014886, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014158, lr 0.250000\n",
      "Average classification loss at step 0: 0.693539, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001227, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014904, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012286, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013025, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015379, lr 0.250000\n",
      "Average classification loss at step 0: 0.693535, lr 0.250000\n",
      "Average embedding loss at step 0: 2.049816, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014364, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014751, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014381, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013661, lr 0.250000\n",
      "Average classification loss at step 0: 0.693531, lr 0.250000\n",
      "Average embedding loss at step 0: 2.070304, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013201, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012042, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014258, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012942, lr 0.250000\n",
      "Average classification loss at step 0: 0.693526, lr 0.250000\n",
      "Average embedding loss at step 0: 1.988302, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012947, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012624, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013377, lr 0.250000\n",
      "Average classification loss at step 0: 0.693521, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009085, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014302, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013847, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014072, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014554, lr 0.250000\n",
      "Average classification loss at step 0: 0.693516, lr 0.250000\n",
      "Average embedding loss at step 0: 2.076038, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013440, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012625, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012706, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013037, lr 0.250000\n",
      "Average classification loss at step 0: 0.693511, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984018, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013325, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013019, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013593, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014923, lr 0.250000\n",
      "Average classification loss at step 0: 0.693506, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019890, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013058, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012276, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014214, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015455, lr 0.250000\n",
      "Average classification loss at step 0: 0.693500, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989504, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013923, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014128, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013920, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015471, lr 0.250000\n",
      "Average classification loss at step 0: 0.693495, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987495, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013131, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015822, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014437, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013092, lr 0.250000\n",
      "Average classification loss at step 0: 0.693490, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014598, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012855, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014223, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012939, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012583, lr 0.250000\n",
      "Average classification loss at step 0: 0.693486, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014708, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013881, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014109, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013581, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012549, lr 0.250000\n",
      "Average classification loss at step 0: 0.693481, lr 0.250000\n",
      "Average embedding loss at step 0: 2.080714, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013357, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012905, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013289, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012448, lr 0.250000\n",
      "Average classification loss at step 0: 0.693476, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029027, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012827, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013215, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014442, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013587, lr 0.250000\n",
      "Average classification loss at step 0: 0.693471, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996810, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012819, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014558, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013588, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015754, lr 0.250000\n",
      "Average classification loss at step 0: 0.693467, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001149, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012804, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015057, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013481, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012319, lr 0.250000\n",
      "Average classification loss at step 0: 0.693461, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027924, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014142, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013403, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013276, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013302, lr 0.250000\n",
      "Average classification loss at step 0: 0.693456, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985520, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012648, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012933, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012137, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014425, lr 0.250000\n",
      "Average classification loss at step 0: 0.693451, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977307, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013124, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013082, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014231, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014350, lr 0.250000\n",
      "Average classification loss at step 0: 0.693447, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044597, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013833, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013173, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014731, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014656, lr 0.250000\n",
      "Average classification loss at step 0: 0.693442, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044127, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013165, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012625, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013664, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013410, lr 0.250000\n",
      "Average classification loss at step 0: 0.693438, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996814, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013245, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014794, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012884, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013893, lr 0.250000\n",
      "Average classification loss at step 0: 0.693433, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014500, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012577, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013046, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013580, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014891, lr 0.250000\n",
      "Average classification loss at step 0: 0.693428, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038383, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013372, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014211, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013299, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013219, lr 0.250000\n",
      "Average classification loss at step 0: 0.693423, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062284, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012921, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012596, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013496, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014182, lr 0.250000\n",
      "Average classification loss at step 0: 0.693418, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998231, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015567, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014415, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013535, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011973, lr 0.250000\n",
      "Average classification loss at step 0: 0.693414, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040597, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014909, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013362, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014925, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014283, lr 0.250000\n",
      "Average classification loss at step 0: 0.693409, lr 0.250000\n",
      "Average embedding loss at step 0: 2.077064, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013468, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013728, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012082, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013459, lr 0.250000\n",
      "Average classification loss at step 0: 0.693405, lr 0.250000\n",
      "Average embedding loss at step 0: 1.958595, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014638, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013787, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014007, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014253, lr 0.250000\n",
      "Average classification loss at step 0: 0.693400, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981185, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015093, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013567, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013725, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013975, lr 0.250000\n",
      "Average classification loss at step 0: 0.693394, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022992, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012653, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012557, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013612, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013649, lr 0.250000\n",
      "Average classification loss at step 0: 0.693389, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043189, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014271, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013134, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014222, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012173, lr 0.250000\n",
      "Average classification loss at step 0: 0.693384, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031905, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013828, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013637, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014274, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013203, lr 0.250000\n",
      "Average classification loss at step 0: 0.693380, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003723, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011768, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012807, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013876, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012467, lr 0.250000\n",
      "Average classification loss at step 0: 0.693375, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035372, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013467, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014444, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013634, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013321, lr 0.250000\n",
      "Average classification loss at step 0: 0.693370, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974145, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013387, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013189, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012826, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013878, lr 0.250000\n",
      "Average classification loss at step 0: 0.693365, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993758, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014291, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012837, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011925, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013963, lr 0.250000\n",
      "Average classification loss at step 0: 0.693361, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039916, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013678, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013423, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013636, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012703, lr 0.250000\n",
      "Average classification loss at step 0: 0.693356, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981769, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013137, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012514, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013244, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014241, lr 0.250000\n",
      "Average classification loss at step 0: 0.693351, lr 0.250000\n",
      "Average embedding loss at step 0: 1.971260, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014666, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012866, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015095, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013851, lr 0.250000\n",
      "Average classification loss at step 0: 0.693346, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009128, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014061, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013116, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013237, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014845, lr 0.250000\n",
      "Average classification loss at step 0: 0.693341, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013772, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015075, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013725, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012752, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012808, lr 0.250000\n",
      "Average classification loss at step 0: 0.693336, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003887, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013424, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012523, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014335, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013640, lr 0.250000\n",
      "Average classification loss at step 0: 0.693331, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002952, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014084, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012551, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013507, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012622, lr 0.250000\n",
      "Average classification loss at step 0: 0.693326, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055645, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014093, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012752, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013614, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013684, lr 0.250000\n",
      "Average classification loss at step 0: 0.693321, lr 0.250000\n",
      "Average embedding loss at step 0: 2.117931, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013404, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014078, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014050, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012625, lr 0.250000\n",
      "Average classification loss at step 0: 0.693317, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010171, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015092, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013536, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011895, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014325, lr 0.250000\n",
      "Average classification loss at step 0: 0.693312, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017789, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012090, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013930, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013475, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012969, lr 0.250000\n",
      "Average classification loss at step 0: 0.693307, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025034, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013675, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013504, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012913, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013879, lr 0.250000\n",
      "Average classification loss at step 0: 0.693302, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046519, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014853, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012969, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012763, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013633, lr 0.250000\n",
      "Average classification loss at step 0: 0.693297, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003872, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012615, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012943, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014344, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013478, lr 0.250000\n",
      "Average classification loss at step 0: 0.693292, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026212, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011714, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011789, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013805, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012460, lr 0.250000\n",
      "Average classification loss at step 0: 0.693288, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003624, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012547, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012364, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013623, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011882, lr 0.250000\n",
      "Average classification loss at step 0: 0.693283, lr 0.250000\n",
      "Average embedding loss at step 0: 1.955276, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013050, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013802, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012409, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012621, lr 0.250000\n",
      "Average classification loss at step 0: 0.693278, lr 0.250000\n",
      "Average embedding loss at step 0: 2.048517, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012278, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011769, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012909, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013678, lr 0.250000\n",
      "Average classification loss at step 0: 0.693273, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007372, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013822, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010992, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015201, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013821, lr 0.250000\n",
      "Average classification loss at step 0: 0.693268, lr 0.250000\n",
      "Average embedding loss at step 0: 1.950174, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012535, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013378, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015259, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012314, lr 0.250000\n",
      "Average classification loss at step 0: 0.693263, lr 0.250000\n",
      "Average embedding loss at step 0: 1.919640, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013376, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013795, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012500, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012522, lr 0.250000\n",
      "Average classification loss at step 0: 0.693258, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037806, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014766, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012447, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013343, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011927, lr 0.250000\n",
      "Average classification loss at step 0: 0.693254, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064140, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014170, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013867, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014213, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012793, lr 0.250000\n",
      "Average classification loss at step 0: 0.693249, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004467, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012636, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013173, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013489, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013727, lr 0.250000\n",
      "Average classification loss at step 0: 0.693244, lr 0.250000\n",
      "Average embedding loss at step 0: 1.969417, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014066, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014310, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013249, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013977, lr 0.250000\n",
      "Average classification loss at step 0: 0.693240, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045378, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013343, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012924, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012976, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013884, lr 0.250000\n",
      "Average classification loss at step 0: 0.693235, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992417, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012400, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013755, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012793, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012326, lr 0.250000\n",
      "Average classification loss at step 0: 0.693230, lr 0.250000\n",
      "Average embedding loss at step 0: 1.946034, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013063, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013529, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014000, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014074, lr 0.250000\n",
      "Average classification loss at step 0: 0.693226, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006583, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014079, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013523, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012291, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012671, lr 0.250000\n",
      "Average classification loss at step 0: 0.693222, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031016, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012878, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012983, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.015462, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012789, lr 0.250000\n",
      "Average classification loss at step 0: 0.693217, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960802, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011739, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013621, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014624, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013047, lr 0.250000\n",
      "Average classification loss at step 0: 0.693212, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024242, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012338, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013480, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013586, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013419, lr 0.250000\n",
      "Average classification loss at step 0: 0.693208, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040413, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012795, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013297, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012798, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013178, lr 0.250000\n",
      "Average classification loss at step 0: 0.693203, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038606, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013494, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014168, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013064, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013732, lr 0.250000\n",
      "Average classification loss at step 0: 0.693198, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997948, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012446, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012419, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013460, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014830, lr 0.250000\n",
      "Average classification loss at step 0: 0.693193, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072959, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012784, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014339, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012635, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013160, lr 0.250000\n",
      "Average classification loss at step 0: 0.693188, lr 0.250000\n",
      "Average embedding loss at step 0: 1.892647, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012599, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013794, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013621, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013658, lr 0.250000\n",
      "Average classification loss at step 0: 0.693184, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005302, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013590, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012656, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012357, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015116, lr 0.250000\n",
      "Average classification loss at step 0: 0.693179, lr 0.250000\n",
      "Average embedding loss at step 0: 2.063889, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013379, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012418, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013038, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014145, lr 0.250000\n",
      "Average classification loss at step 0: 0.693175, lr 0.250000\n",
      "Average embedding loss at step 0: 1.982294, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011287, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011948, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012005, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012707, lr 0.250000\n",
      "Average classification loss at step 0: 0.693171, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981172, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013010, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012390, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013986, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012219, lr 0.250000\n",
      "Average classification loss at step 0: 0.693166, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035160, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013489, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013321, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012763, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013779, lr 0.250000\n",
      "Average classification loss at step 0: 0.693162, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976609, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014490, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013020, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012190, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013444, lr 0.250000\n",
      "Average classification loss at step 0: 0.693157, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027060, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014188, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013167, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012764, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013121, lr 0.250000\n",
      "Average classification loss at step 0: 0.693153, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035212, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013048, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014191, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013941, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013311, lr 0.250000\n",
      "Average classification loss at step 0: 0.693148, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978759, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012450, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014941, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013204, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013341, lr 0.250000\n",
      "Average classification loss at step 0: 0.693145, lr 0.250000\n",
      "Average embedding loss at step 0: 2.107137, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012374, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012506, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012940, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013554, lr 0.250000\n",
      "Average classification loss at step 0: 0.693140, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016933, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014276, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012816, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013709, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013388, lr 0.250000\n",
      "Average classification loss at step 0: 0.693136, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999945, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013373, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012636, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012277, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013446, lr 0.250000\n",
      "Average classification loss at step 0: 0.693133, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024128, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013081, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011165, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012976, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.015211, lr 0.250000\n",
      "Average classification loss at step 0: 0.693129, lr 0.250000\n",
      "Average embedding loss at step 0: 1.936828, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013358, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013445, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012706, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011528, lr 0.250000\n",
      "Average classification loss at step 0: 0.693126, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983139, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012103, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014116, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012826, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012743, lr 0.250000\n",
      "Average classification loss at step 0: 0.693122, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042826, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012901, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014300, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012948, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012230, lr 0.250000\n",
      "Average classification loss at step 0: 0.693119, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998882, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014509, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014916, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012413, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012278, lr 0.250000\n",
      "Average classification loss at step 0: 0.693115, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994161, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014417, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013222, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012364, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012998, lr 0.250000\n",
      "Average classification loss at step 0: 0.693112, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044818, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012495, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012171, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011210, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013103, lr 0.250000\n",
      "Average classification loss at step 0: 0.693110, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037718, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012041, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013534, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012616, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013511, lr 0.250000\n",
      "Average classification loss at step 0: 0.693107, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052222, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013832, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012775, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013757, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012388, lr 0.250000\n",
      "Average classification loss at step 0: 0.693104, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060920, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013674, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013915, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013385, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011899, lr 0.250000\n",
      "Average classification loss at step 0: 0.693101, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040010, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013885, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012705, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012329, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013808, lr 0.250000\n",
      "Average classification loss at step 0: 0.693098, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000262, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014158, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013260, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013049, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014634, lr 0.250000\n",
      "Average classification loss at step 0: 0.693096, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041107, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013529, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012180, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012556, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012842, lr 0.250000\n",
      "Average classification loss at step 0: 0.693093, lr 0.250000\n",
      "Average embedding loss at step 0: 2.071058, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013037, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014459, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012093, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013366, lr 0.250000\n",
      "Average classification loss at step 0: 0.693090, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004375, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011816, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013714, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013371, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013805, lr 0.250000\n",
      "Average classification loss at step 0: 0.693087, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998959, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014165, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013004, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013859, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013388, lr 0.250000\n",
      "Average classification loss at step 0: 0.693085, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998265, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014337, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012842, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013949, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012725, lr 0.250000\n",
      "Average classification loss at step 0: 0.693082, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033777, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013789, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012584, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012620, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012514, lr 0.250000\n",
      "Average classification loss at step 0: 0.693079, lr 0.250000\n",
      "Average embedding loss at step 0: 1.920564, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014575, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012955, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012994, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013544, lr 0.250000\n",
      "Average classification loss at step 0: 0.693076, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984586, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012744, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013510, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013029, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014060, lr 0.250000\n",
      "Average classification loss at step 0: 0.693074, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977267, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012610, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012855, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012721, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012728, lr 0.250000\n",
      "Average classification loss at step 0: 0.693071, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058055, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014440, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013364, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013690, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013905, lr 0.250000\n",
      "Average classification loss at step 0: 0.693068, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987367, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012996, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011861, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013955, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014378, lr 0.250000\n",
      "Average classification loss at step 0: 0.693065, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007434, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.015279, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012058, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013254, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013783, lr 0.250000\n",
      "Average classification loss at step 0: 0.693062, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027669, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012304, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012166, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013875, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012050, lr 0.250000\n",
      "Average classification loss at step 0: 0.693059, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034663, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013009, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013136, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013655, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012761, lr 0.250000\n",
      "Average classification loss at step 0: 0.693057, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011494, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014810, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013079, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011139, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012685, lr 0.250000\n",
      "Average classification loss at step 0: 0.693053, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065129, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012997, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012354, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013802, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013316, lr 0.250000\n",
      "Average classification loss at step 0: 0.693051, lr 0.250000\n",
      "Average embedding loss at step 0: 1.980131, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012772, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012991, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013285, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013032, lr 0.250000\n",
      "Average classification loss at step 0: 0.693048, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999295, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013369, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013772, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012405, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012790, lr 0.250000\n",
      "Average classification loss at step 0: 0.693046, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031223, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012435, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013296, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014099, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012457, lr 0.250000\n",
      "Average classification loss at step 0: 0.693043, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968256, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012772, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013675, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012511, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012416, lr 0.250000\n",
      "Average classification loss at step 0: 0.693040, lr 0.250000\n",
      "Average embedding loss at step 0: 2.063746, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012831, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014561, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011703, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013111, lr 0.250000\n",
      "Average classification loss at step 0: 0.693037, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992637, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013068, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012772, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014403, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013766, lr 0.250000\n",
      "Average classification loss at step 0: 0.693035, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981572, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012553, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013167, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011631, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012121, lr 0.250000\n",
      "Average classification loss at step 0: 0.693033, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039938, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012973, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013646, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013347, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013469, lr 0.250000\n",
      "Average classification loss at step 0: 0.693030, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975132, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013693, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014653, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012995, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014136, lr 0.250000\n",
      "Average classification loss at step 0: 0.693027, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024619, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013881, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013394, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013645, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013445, lr 0.250000\n",
      "Average classification loss at step 0: 0.693025, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057746, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012091, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012583, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014040, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012062, lr 0.250000\n",
      "Average classification loss at step 0: 0.693022, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028794, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012904, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013403, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012969, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013148, lr 0.250000\n",
      "Average classification loss at step 0: 0.693020, lr 0.250000\n",
      "Average embedding loss at step 0: 2.059706, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012918, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013643, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011552, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013240, lr 0.250000\n",
      "Average classification loss at step 0: 0.693017, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030180, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012326, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011742, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011871, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013983, lr 0.250000\n",
      "Average classification loss at step 0: 0.693014, lr 0.250000\n",
      "Average embedding loss at step 0: 1.940765, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013303, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014202, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013355, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013469, lr 0.250000\n",
      "Average classification loss at step 0: 0.693011, lr 0.250000\n",
      "Average embedding loss at step 0: 2.084037, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012344, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012914, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011849, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012031, lr 0.250000\n",
      "Average classification loss at step 0: 0.693008, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065734, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012290, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013835, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013323, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012515, lr 0.250000\n",
      "Average classification loss at step 0: 0.693006, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029084, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013481, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013722, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014947, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012185, lr 0.250000\n",
      "Average classification loss at step 0: 0.693003, lr 0.250000\n",
      "Average embedding loss at step 0: 1.971121, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011520, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013960, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013507, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010952, lr 0.250000\n",
      "Average classification loss at step 0: 0.693001, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987423, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013376, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011904, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011621, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010825, lr 0.250000\n",
      "Average classification loss at step 0: 0.692998, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019175, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013078, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011786, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012910, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012455, lr 0.250000\n",
      "Average classification loss at step 0: 0.692995, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018246, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012466, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011810, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012191, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013931, lr 0.250000\n",
      "Average classification loss at step 0: 0.692992, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002277, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012913, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012469, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012185, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013229, lr 0.250000\n",
      "Average classification loss at step 0: 0.692990, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044883, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014302, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013063, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012839, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012393, lr 0.250000\n",
      "Average classification loss at step 0: 0.692988, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983712, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012611, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013418, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014372, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013250, lr 0.250000\n",
      "Average classification loss at step 0: 0.692985, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006046, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013122, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013443, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011837, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014098, lr 0.250000\n",
      "Average classification loss at step 0: 0.692982, lr 0.250000\n",
      "Average embedding loss at step 0: 1.934434, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013068, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012917, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014763, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012867, lr 0.250000\n",
      "Average classification loss at step 0: 0.692980, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034235, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011099, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012648, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013569, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013502, lr 0.250000\n",
      "Average classification loss at step 0: 0.692978, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038650, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013416, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013870, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013292, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014276, lr 0.250000\n",
      "Average classification loss at step 0: 0.692975, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045502, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011943, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015235, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013030, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013999, lr 0.250000\n",
      "Average classification loss at step 0: 0.692973, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995577, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011563, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012178, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013377, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013320, lr 0.250000\n",
      "Average classification loss at step 0: 0.692970, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046080, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011796, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014049, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012855, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013168, lr 0.250000\n",
      "Average classification loss at step 0: 0.692968, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056531, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011961, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013642, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013638, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011536, lr 0.250000\n",
      "Average classification loss at step 0: 0.692966, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991436, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014590, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012296, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012741, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013589, lr 0.250000\n",
      "Average classification loss at step 0: 0.692963, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035779, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011997, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013385, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011652, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012959, lr 0.250000\n",
      "Average classification loss at step 0: 0.692960, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072220, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013234, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011344, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012476, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013416, lr 0.250000\n",
      "Average classification loss at step 0: 0.692958, lr 0.250000\n",
      "Average embedding loss at step 0: 1.925588, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013027, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014815, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012649, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012514, lr 0.250000\n",
      "Average classification loss at step 0: 0.692955, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023746, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013277, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011653, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012483, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013123, lr 0.250000\n",
      "Average classification loss at step 0: 0.692952, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038861, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012632, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012437, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013644, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013623, lr 0.250000\n",
      "Average classification loss at step 0: 0.692949, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035449, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012467, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013431, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013762, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013430, lr 0.250000\n",
      "Average classification loss at step 0: 0.692947, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025970, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012993, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012406, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012800, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013405, lr 0.250000\n",
      "Average classification loss at step 0: 0.692944, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024719, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012413, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013471, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012485, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012762, lr 0.250000\n",
      "Average classification loss at step 0: 0.692942, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030066, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013262, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012273, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013029, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012117, lr 0.250000\n",
      "Average classification loss at step 0: 0.692940, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032439, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013240, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011768, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013647, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011548, lr 0.250000\n",
      "Average classification loss at step 0: 0.692938, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068202, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013358, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012663, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012693, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011476, lr 0.250000\n",
      "Average classification loss at step 0: 0.692935, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073689, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012489, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013625, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013702, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011183, lr 0.250000\n",
      "Average classification loss at step 0: 0.692932, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017503, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013035, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011778, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012274, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013151, lr 0.250000\n",
      "Average classification loss at step 0: 0.692930, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017664, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012945, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012020, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012243, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012481, lr 0.250000\n",
      "Average classification loss at step 0: 0.692927, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012635, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012278, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013606, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014151, lr 0.250000\n",
      "Average classification loss at step 0: 0.692925, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066213, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012357, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013240, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013435, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013574, lr 0.250000\n",
      "Average classification loss at step 0: 0.692922, lr 0.250000\n",
      "Average embedding loss at step 0: 2.063102, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011153, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011982, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014453, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013439, lr 0.250000\n",
      "Average classification loss at step 0: 0.692920, lr 0.250000\n",
      "Average embedding loss at step 0: 2.091375, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012833, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011990, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012749, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014464, lr 0.250000\n",
      "Average classification loss at step 0: 0.692918, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037248, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013197, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013651, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013082, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012069, lr 0.250000\n",
      "Average classification loss at step 0: 0.692915, lr 0.250000\n",
      "Average embedding loss at step 0: 2.069228, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013092, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012798, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012369, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012840, lr 0.250000\n",
      "Average classification loss at step 0: 0.692913, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040645, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012140, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011362, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012263, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012573, lr 0.250000\n",
      "Average classification loss at step 0: 0.692911, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992723, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013335, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012839, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012040, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013077, lr 0.250000\n",
      "Average classification loss at step 0: 0.692908, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034218, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012611, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012780, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012897, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011808, lr 0.250000\n",
      "Average classification loss at step 0: 0.692906, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017762, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013064, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012632, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011618, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013563, lr 0.250000\n",
      "Average classification loss at step 0: 0.692904, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993321, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011871, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010692, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014081, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014421, lr 0.250000\n",
      "Average classification loss at step 0: 0.692901, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052260, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012726, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012370, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012508, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014118, lr 0.250000\n",
      "Average classification loss at step 0: 0.692899, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045258, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012040, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014044, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012349, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012082, lr 0.250000\n",
      "Average classification loss at step 0: 0.692897, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055701, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011841, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013360, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011676, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012280, lr 0.250000\n",
      "Average classification loss at step 0: 0.692895, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963188, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011666, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013582, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012950, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013983, lr 0.250000\n",
      "Average classification loss at step 0: 0.692892, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977354, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013851, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012864, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011728, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011471, lr 0.250000\n",
      "Average classification loss at step 0: 0.692890, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015689, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012265, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013564, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014331, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012075, lr 0.250000\n",
      "Average classification loss at step 0: 0.692888, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015601, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011413, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013728, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013458, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013963, lr 0.250000\n",
      "Average classification loss at step 0: 0.692885, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960623, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013564, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013965, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011741, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012474, lr 0.250000\n",
      "Average classification loss at step 0: 0.692882, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994662, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012885, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012385, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012955, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013289, lr 0.250000\n",
      "Average classification loss at step 0: 0.692881, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960107, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012218, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011877, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011362, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013311, lr 0.250000\n",
      "Average classification loss at step 0: 0.692879, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975395, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013006, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013321, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013142, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011223, lr 0.250000\n",
      "Average classification loss at step 0: 0.692877, lr 0.250000\n",
      "Average embedding loss at step 0: 1.988022, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012669, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011433, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013389, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013670, lr 0.250000\n",
      "Average classification loss at step 0: 0.692875, lr 0.250000\n",
      "Average embedding loss at step 0: 2.075352, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012453, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012858, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013501, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011906, lr 0.250000\n",
      "Average classification loss at step 0: 0.692873, lr 0.250000\n",
      "Average embedding loss at step 0: 1.982241, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011810, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014432, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012439, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014249, lr 0.250000\n",
      "Average classification loss at step 0: 0.692871, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051880, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011787, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012429, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013351, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014757, lr 0.250000\n",
      "Average classification loss at step 0: 0.692868, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977162, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012759, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015014, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012696, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011698, lr 0.250000\n",
      "Average classification loss at step 0: 0.692866, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999428, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012122, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012508, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013265, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012691, lr 0.250000\n",
      "Average classification loss at step 0: 0.692864, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072530, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013918, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013602, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011636, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012553, lr 0.250000\n",
      "Average classification loss at step 0: 0.692862, lr 0.250000\n",
      "Average embedding loss at step 0: 2.048418, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013215, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012608, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013423, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012906, lr 0.250000\n",
      "Average classification loss at step 0: 0.692860, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977153, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012686, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013414, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011893, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011984, lr 0.250000\n",
      "Average classification loss at step 0: 0.692857, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984442, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012130, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013575, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011412, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012517, lr 0.250000\n",
      "Average classification loss at step 0: 0.692855, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984197, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013671, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013801, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013510, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013549, lr 0.250000\n",
      "Average classification loss at step 0: 0.692853, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060004, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012155, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012339, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013056, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012042, lr 0.250000\n",
      "Average classification loss at step 0: 0.692851, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017785, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013337, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012180, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013415, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011760, lr 0.250000\n",
      "Average classification loss at step 0: 0.692848, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021005, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013058, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012045, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013785, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012417, lr 0.250000\n",
      "Average classification loss at step 0: 0.692846, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033612, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012359, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011451, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011957, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012790, lr 0.250000\n",
      "Average classification loss at step 0: 0.692845, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061231, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013042, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014446, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011977, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012567, lr 0.250000\n",
      "Average classification loss at step 0: 0.692842, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983084, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012458, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013183, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013344, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012804, lr 0.250000\n",
      "Average classification loss at step 0: 0.692840, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973141, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012273, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013424, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012291, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012954, lr 0.250000\n",
      "Average classification loss at step 0: 0.692839, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016691, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013394, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012557, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012545, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010972, lr 0.250000\n",
      "Average classification loss at step 0: 0.692837, lr 0.250000\n",
      "Average embedding loss at step 0: 2.077410, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011721, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011786, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012828, lr 0.250000\n",
      "Average classification loss at step 0: 0.692834, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976233, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012978, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011714, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014162, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013362, lr 0.250000\n",
      "Average classification loss at step 0: 0.692832, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031929, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013407, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012594, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013275, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013898, lr 0.250000\n",
      "Average classification loss at step 0: 0.692830, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040734, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014256, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013148, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011152, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013032, lr 0.250000\n",
      "Average classification loss at step 0: 0.692828, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996419, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012377, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012434, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013313, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013301, lr 0.250000\n",
      "Average classification loss at step 0: 0.692826, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994117, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013022, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013175, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012457, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012393, lr 0.250000\n",
      "Average classification loss at step 0: 0.692824, lr 0.250000\n",
      "Average embedding loss at step 0: 2.086382, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011998, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011889, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012250, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011916, lr 0.250000\n",
      "Average classification loss at step 0: 0.692822, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034807, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010915, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012961, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012609, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013122, lr 0.250000\n",
      "Average classification loss at step 0: 0.692820, lr 0.250000\n",
      "Average embedding loss at step 0: 2.071204, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012435, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012185, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011835, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012670, lr 0.250000\n",
      "Average classification loss at step 0: 0.692818, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967954, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012181, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012224, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012297, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012753, lr 0.250000\n",
      "Average classification loss at step 0: 0.692816, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073509, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013086, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013255, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012304, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013391, lr 0.250000\n",
      "Average classification loss at step 0: 0.692814, lr 0.250000\n",
      "Average embedding loss at step 0: 1.954363, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014495, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012552, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012712, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013174, lr 0.250000\n",
      "Average classification loss at step 0: 0.692811, lr 0.250000\n",
      "Average embedding loss at step 0: 1.982522, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011562, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012316, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012779, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012841, lr 0.250000\n",
      "Average classification loss at step 0: 0.692810, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965897, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011085, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011122, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013627, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012010, lr 0.250000\n",
      "Average classification loss at step 0: 0.692808, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041352, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011316, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012288, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013161, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011977, lr 0.250000\n",
      "Average classification loss at step 0: 0.692806, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010830, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012249, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012989, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012351, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012086, lr 0.250000\n",
      "Average classification loss at step 0: 0.692804, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003454, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012727, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012572, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013079, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012936, lr 0.250000\n",
      "Average classification loss at step 0: 0.692802, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052720, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012411, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010905, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013543, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012242, lr 0.250000\n",
      "Average classification loss at step 0: 0.692800, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041944, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012196, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012530, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014511, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012031, lr 0.250000\n",
      "Average classification loss at step 0: 0.692798, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037136, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012316, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012071, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013035, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012408, lr 0.250000\n",
      "Average classification loss at step 0: 0.692796, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030869, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013668, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011490, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012572, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011969, lr 0.250000\n",
      "Average classification loss at step 0: 0.692794, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000712, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013672, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012487, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014202, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012891, lr 0.250000\n",
      "Average classification loss at step 0: 0.692792, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978670, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012867, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012784, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012590, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013072, lr 0.250000\n",
      "Average classification loss at step 0: 0.692790, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056267, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012549, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012743, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010909, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012854, lr 0.250000\n",
      "Average classification loss at step 0: 0.692788, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995527, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012673, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013296, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012975, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012605, lr 0.250000\n",
      "Average classification loss at step 0: 0.692786, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002228, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011866, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011592, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012634, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013056, lr 0.250000\n",
      "Average classification loss at step 0: 0.692784, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015541, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011982, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012153, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013376, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012509, lr 0.250000\n",
      "Average classification loss at step 0: 0.692782, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015039, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013441, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011698, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011379, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013019, lr 0.250000\n",
      "Average classification loss at step 0: 0.692781, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017907, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012544, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012442, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014490, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012435, lr 0.250000\n",
      "Average classification loss at step 0: 0.692779, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981152, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010804, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014755, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012948, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012560, lr 0.250000\n",
      "Average classification loss at step 0: 0.692777, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013100, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011567, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013492, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013363, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011788, lr 0.250000\n",
      "Average classification loss at step 0: 0.692775, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983983, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012655, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014323, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012268, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012263, lr 0.250000\n",
      "Average classification loss at step 0: 0.692773, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010702, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012653, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013062, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013036, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012804, lr 0.250000\n",
      "Average classification loss at step 0: 0.692771, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018765, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014086, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012665, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012316, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011835, lr 0.250000\n",
      "Average classification loss at step 0: 0.692769, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066878, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011815, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012270, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012601, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012583, lr 0.250000\n",
      "Average classification loss at step 0: 0.692768, lr 0.250000\n",
      "Average embedding loss at step 0: 1.988661, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011954, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013388, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013155, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013245, lr 0.250000\n",
      "Average classification loss at step 0: 0.692766, lr 0.250000\n",
      "Average embedding loss at step 0: 1.931732, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012432, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012065, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012186, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012861, lr 0.250000\n",
      "Average classification loss at step 0: 0.692764, lr 0.250000\n",
      "Average embedding loss at step 0: 1.988511, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011543, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012505, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012508, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013115, lr 0.250000\n",
      "Average classification loss at step 0: 0.692762, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993271, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012966, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012594, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012921, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011440, lr 0.250000\n",
      "Average classification loss at step 0: 0.692760, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034417, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013254, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011711, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012859, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011509, lr 0.250000\n",
      "Average classification loss at step 0: 0.692759, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973561, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012805, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014029, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013211, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013271, lr 0.250000\n",
      "Average classification loss at step 0: 0.692757, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006051, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013258, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.009917, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012733, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012676, lr 0.250000\n",
      "Average classification loss at step 0: 0.692755, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963157, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012668, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012753, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011770, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012675, lr 0.250000\n",
      "Average classification loss at step 0: 0.692752, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016160, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013369, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012844, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012323, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013172, lr 0.250000\n",
      "Average classification loss at step 0: 0.692751, lr 0.250000\n",
      "Average embedding loss at step 0: 1.963752, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012829, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013306, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013194, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011476, lr 0.250000\n",
      "Average classification loss at step 0: 0.692749, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035885, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012595, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011873, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011501, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013923, lr 0.250000\n",
      "Average classification loss at step 0: 0.692747, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022884, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012588, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013128, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012241, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012036, lr 0.250000\n",
      "Average classification loss at step 0: 0.692745, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040327, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013207, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011195, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013156, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011209, lr 0.250000\n",
      "Average classification loss at step 0: 0.692743, lr 0.250000\n",
      "Average embedding loss at step 0: 1.923153, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011807, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011630, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012045, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014029, lr 0.250000\n",
      "Average classification loss at step 0: 0.692741, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985738, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012459, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012747, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012723, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012026, lr 0.250000\n",
      "Average classification loss at step 0: 0.692740, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001650, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012071, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012807, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011918, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012042, lr 0.250000\n",
      "Average classification loss at step 0: 0.692738, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978332, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012072, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012430, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012403, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011971, lr 0.250000\n",
      "Average classification loss at step 0: 0.692736, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990529, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013375, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012932, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010500, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012474, lr 0.250000\n",
      "Average classification loss at step 0: 0.692735, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027721, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013012, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012730, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011464, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012181, lr 0.250000\n",
      "Average classification loss at step 0: 0.692733, lr 0.250000\n",
      "Average embedding loss at step 0: 1.931950, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013303, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011848, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.009438, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012606, lr 0.250000\n",
      "Average classification loss at step 0: 0.692731, lr 0.250000\n",
      "Average embedding loss at step 0: 1.955320, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012093, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012358, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011996, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012686, lr 0.250000\n",
      "Average classification loss at step 0: 0.692729, lr 0.250000\n",
      "Average embedding loss at step 0: 1.980806, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013406, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012333, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013680, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011250, lr 0.250000\n",
      "Average classification loss at step 0: 0.692727, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968126, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011899, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011359, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011416, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010785, lr 0.250000\n",
      "Average classification loss at step 0: 0.692726, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032766, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012632, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012268, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011215, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012155, lr 0.250000\n",
      "Average classification loss at step 0: 0.692724, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978315, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013605, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012458, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012520, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013917, lr 0.250000\n",
      "Average classification loss at step 0: 0.692723, lr 0.250000\n",
      "Average embedding loss at step 0: 1.966578, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012146, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011661, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012423, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012359, lr 0.250000\n",
      "Average classification loss at step 0: 0.692722, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014000, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012409, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013839, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011114, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013065, lr 0.250000\n",
      "Average classification loss at step 0: 0.692720, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977216, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011393, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012318, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012895, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012620, lr 0.250000\n",
      "Average classification loss at step 0: 0.692719, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967741, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012571, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012860, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011634, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012471, lr 0.250000\n",
      "Average classification loss at step 0: 0.692717, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016794, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013599, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013238, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013058, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011113, lr 0.250000\n",
      "Average classification loss at step 0: 0.692715, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009778, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012428, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012685, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011565, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011967, lr 0.250000\n",
      "Average classification loss at step 0: 0.692714, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031478, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013306, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012919, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012391, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012362, lr 0.250000\n",
      "Average classification loss at step 0: 0.692712, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019850, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013075, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012882, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013617, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013408, lr 0.250000\n",
      "Average classification loss at step 0: 0.692710, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991536, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013303, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012088, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011390, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011663, lr 0.250000\n",
      "Average classification loss at step 0: 0.692709, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015186, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011936, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012300, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014036, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014040, lr 0.250000\n",
      "Average classification loss at step 0: 0.692707, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054849, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012660, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011640, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012633, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014217, lr 0.250000\n",
      "Average classification loss at step 0: 0.692705, lr 0.250000\n",
      "Average embedding loss at step 0: 1.949391, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013044, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011570, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011841, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013041, lr 0.250000\n",
      "Average classification loss at step 0: 0.692703, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026063, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011408, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010882, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012719, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011028, lr 0.250000\n",
      "Average classification loss at step 0: 0.692702, lr 0.250000\n",
      "Average embedding loss at step 0: 1.944704, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011849, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012330, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013504, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013625, lr 0.250000\n",
      "Average classification loss at step 0: 0.692701, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029218, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013600, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012340, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.009931, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012229, lr 0.250000\n",
      "Average classification loss at step 0: 0.692699, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000973, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013362, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012135, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012713, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012806, lr 0.250000\n",
      "Average classification loss at step 0: 0.692698, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011541, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011350, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012135, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012592, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013423, lr 0.250000\n",
      "Average classification loss at step 0: 0.692696, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025584, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012553, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013708, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012768, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012366, lr 0.250000\n",
      "Average classification loss at step 0: 0.692695, lr 0.250000\n",
      "Average embedding loss at step 0: 2.006611, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011864, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012817, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012661, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012259, lr 0.250000\n",
      "Average classification loss at step 0: 0.692693, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016863, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012824, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011178, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010963, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012241, lr 0.250000\n",
      "Average classification loss at step 0: 0.692692, lr 0.250000\n",
      "Average embedding loss at step 0: 1.920230, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012490, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012688, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011563, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012682, lr 0.250000\n",
      "Average classification loss at step 0: 0.692690, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990613, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012617, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012288, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013107, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012645, lr 0.250000\n",
      "Average classification loss at step 0: 0.692689, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014830, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011213, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012103, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011176, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011871, lr 0.250000\n",
      "Average classification loss at step 0: 0.692687, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027085, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012832, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013083, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012621, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011809, lr 0.250000\n",
      "Average classification loss at step 0: 0.692686, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981096, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013503, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013043, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013295, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012297, lr 0.250000\n",
      "Average classification loss at step 0: 0.692684, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012218, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013381, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012505, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012856, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012686, lr 0.250000\n",
      "Average classification loss at step 0: 0.692683, lr 0.250000\n",
      "Average embedding loss at step 0: 1.962129, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012777, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012298, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012043, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011464, lr 0.250000\n",
      "Average classification loss at step 0: 0.692681, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037429, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013873, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013059, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012015, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011666, lr 0.250000\n",
      "Average classification loss at step 0: 0.692680, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039889, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011146, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012338, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011394, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013148, lr 0.250000\n",
      "Average classification loss at step 0: 0.692679, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015628, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013066, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012961, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.009923, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011508, lr 0.250000\n",
      "Average classification loss at step 0: 0.692677, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025460, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011414, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012998, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012402, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013114, lr 0.250000\n",
      "Average classification loss at step 0: 0.692676, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041778, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012957, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012848, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012353, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011536, lr 0.250000\n",
      "Average classification loss at step 0: 0.692674, lr 0.250000\n",
      "Average embedding loss at step 0: 2.084706, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011219, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013161, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012971, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012196, lr 0.250000\n",
      "Average classification loss at step 0: 0.692673, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996270, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012755, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012790, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013903, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012208, lr 0.250000\n",
      "Average classification loss at step 0: 0.692671, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062295, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011471, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013217, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013113, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010574, lr 0.250000\n",
      "Average classification loss at step 0: 0.692670, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011673, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012072, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011935, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011870, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011162, lr 0.250000\n",
      "Average classification loss at step 0: 0.692668, lr 0.250000\n",
      "Average embedding loss at step 0: 2.008759, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012778, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010983, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012750, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012153, lr 0.250000\n",
      "Average classification loss at step 0: 0.692667, lr 0.250000\n",
      "Average embedding loss at step 0: 1.955095, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012005, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012739, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011153, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012900, lr 0.250000\n",
      "Average classification loss at step 0: 0.692665, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991780, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012878, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011863, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011941, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012129, lr 0.250000\n",
      "Average classification loss at step 0: 0.692664, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011365, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012368, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012436, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011687, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011568, lr 0.250000\n",
      "Average classification loss at step 0: 0.692662, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001254, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013407, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011861, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014313, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012809, lr 0.250000\n",
      "Average classification loss at step 0: 0.692661, lr 0.250000\n",
      "Average embedding loss at step 0: 2.119210, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012853, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012612, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011591, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012170, lr 0.250000\n",
      "Average classification loss at step 0: 0.692660, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035286, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012709, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011359, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012358, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011725, lr 0.250000\n",
      "Average classification loss at step 0: 0.692659, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019092, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010419, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012111, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013438, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012145, lr 0.250000\n",
      "Average classification loss at step 0: 0.692657, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965767, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013095, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012893, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012171, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013075, lr 0.250000\n",
      "Average classification loss at step 0: 0.692656, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037959, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011552, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014651, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012683, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013413, lr 0.250000\n",
      "Average classification loss at step 0: 0.692655, lr 0.250000\n",
      "Average embedding loss at step 0: 2.056124, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011533, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012051, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011991, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012483, lr 0.250000\n",
      "Average classification loss at step 0: 0.692654, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026026, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011659, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013205, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012953, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011563, lr 0.250000\n",
      "Average classification loss at step 0: 0.692652, lr 0.250000\n",
      "Average embedding loss at step 0: 1.931722, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012306, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013266, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013775, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010571, lr 0.250000\n",
      "Average classification loss at step 0: 0.692651, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017086, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013546, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012700, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011594, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013843, lr 0.250000\n",
      "Average classification loss at step 0: 0.692649, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012914, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011737, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012848, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011069, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012995, lr 0.250000\n",
      "Average classification loss at step 0: 0.692648, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043906, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012889, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010863, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012453, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012500, lr 0.250000\n",
      "Average classification loss at step 0: 0.692647, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020689, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012099, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013912, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011337, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012297, lr 0.250000\n",
      "Average classification loss at step 0: 0.692645, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031437, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012922, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011531, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011699, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011884, lr 0.250000\n",
      "Average classification loss at step 0: 0.692644, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992566, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011827, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012386, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013113, lr 0.250000\n",
      "Average classification loss at step 0: 0.692643, lr 0.250000\n",
      "Average embedding loss at step 0: 2.003028, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011750, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012998, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012713, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011627, lr 0.250000\n",
      "Average classification loss at step 0: 0.692641, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974797, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013710, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011996, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010684, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012541, lr 0.250000\n",
      "Average classification loss at step 0: 0.692640, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034523, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011978, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012708, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011091, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012064, lr 0.250000\n",
      "Average classification loss at step 0: 0.692639, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031859, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012429, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012106, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012439, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012572, lr 0.250000\n",
      "Average classification loss at step 0: 0.692638, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061229, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012004, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011499, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013637, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011639, lr 0.250000\n",
      "Average classification loss at step 0: 0.692637, lr 0.250000\n",
      "Average embedding loss at step 0: 2.107498, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014245, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011282, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011725, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012553, lr 0.250000\n",
      "Average classification loss at step 0: 0.692635, lr 0.250000\n",
      "Average embedding loss at step 0: 1.928181, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012888, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011524, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.014060, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011177, lr 0.250000\n",
      "Average classification loss at step 0: 0.692634, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027759, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011562, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010736, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012103, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013454, lr 0.250000\n",
      "Average classification loss at step 0: 0.692633, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000860, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012368, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011754, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010048, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011720, lr 0.250000\n",
      "Average classification loss at step 0: 0.692632, lr 0.250000\n",
      "Average embedding loss at step 0: 2.008621, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013306, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012406, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012891, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013264, lr 0.250000\n",
      "Average classification loss at step 0: 0.692631, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993123, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012655, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012973, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012645, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012925, lr 0.250000\n",
      "Average classification loss at step 0: 0.692629, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041849, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012445, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013488, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012266, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012412, lr 0.250000\n",
      "Average classification loss at step 0: 0.692628, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051445, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012044, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011987, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012498, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012526, lr 0.250000\n",
      "Average classification loss at step 0: 0.692627, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010523, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011767, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012678, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011953, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011929, lr 0.250000\n",
      "Average classification loss at step 0: 0.692626, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068561, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012028, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012118, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012484, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012300, lr 0.250000\n",
      "Average classification loss at step 0: 0.692625, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041768, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011710, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011754, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012719, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010674, lr 0.250000\n",
      "Average classification loss at step 0: 0.692624, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981471, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012576, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013061, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012801, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012850, lr 0.250000\n",
      "Average classification loss at step 0: 0.692622, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015670, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012736, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011190, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012172, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010211, lr 0.250000\n",
      "Average classification loss at step 0: 0.692621, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013236, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012339, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011604, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013261, lr 0.250000\n",
      "Average classification loss at step 0: 0.692620, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001841, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012020, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011664, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011490, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014390, lr 0.250000\n",
      "Average classification loss at step 0: 0.692619, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014454, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012919, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011101, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011832, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013499, lr 0.250000\n",
      "Average classification loss at step 0: 0.692617, lr 0.250000\n",
      "Average embedding loss at step 0: 2.087850, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013426, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012551, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011544, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012199, lr 0.250000\n",
      "Average classification loss at step 0: 0.692616, lr 0.250000\n",
      "Average embedding loss at step 0: 1.973022, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012330, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014704, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011064, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012580, lr 0.250000\n",
      "Average classification loss at step 0: 0.692615, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000710, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012002, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012074, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011261, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012732, lr 0.250000\n",
      "Average classification loss at step 0: 0.692614, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021929, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012483, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011194, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011941, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010547, lr 0.250000\n",
      "Average classification loss at step 0: 0.692613, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034943, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011206, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011812, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013322, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010935, lr 0.250000\n",
      "Average classification loss at step 0: 0.692612, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983308, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011911, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012573, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011339, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014125, lr 0.250000\n",
      "Average classification loss at step 0: 0.692611, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073360, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013052, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013155, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011899, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011653, lr 0.250000\n",
      "Average classification loss at step 0: 0.692609, lr 0.250000\n",
      "Average embedding loss at step 0: 1.957875, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012144, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013073, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013268, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011490, lr 0.250000\n",
      "Average classification loss at step 0: 0.692608, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032665, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011677, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011547, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011515, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012640, lr 0.250000\n",
      "Average classification loss at step 0: 0.692607, lr 0.250000\n",
      "Average embedding loss at step 0: 2.047810, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011387, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012489, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012024, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010899, lr 0.250000\n",
      "Average classification loss at step 0: 0.692606, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017480, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012633, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010900, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013525, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012510, lr 0.250000\n",
      "Average classification loss at step 0: 0.692605, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052952, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012506, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011589, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012915, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011646, lr 0.250000\n",
      "Average classification loss at step 0: 0.692604, lr 0.250000\n",
      "Average embedding loss at step 0: 2.065597, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012308, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012777, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013245, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013712, lr 0.250000\n",
      "Average classification loss at step 0: 0.692603, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044291, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011360, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011868, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012772, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013573, lr 0.250000\n",
      "Average classification loss at step 0: 0.692602, lr 0.250000\n",
      "Average embedding loss at step 0: 2.015635, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013383, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013454, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012743, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011016, lr 0.250000\n",
      "Average classification loss at step 0: 0.692601, lr 0.250000\n",
      "Average embedding loss at step 0: 2.053059, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011856, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011584, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013070, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012404, lr 0.250000\n",
      "Average classification loss at step 0: 0.692600, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993033, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012961, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012580, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011144, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012223, lr 0.250000\n",
      "Average classification loss at step 0: 0.692599, lr 0.250000\n",
      "Average embedding loss at step 0: 2.081724, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013418, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011139, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013911, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012042, lr 0.250000\n",
      "Average classification loss at step 0: 0.692598, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998649, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011269, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012023, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011212, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012246, lr 0.250000\n",
      "Average classification loss at step 0: 0.692596, lr 0.250000\n",
      "Average embedding loss at step 0: 1.919043, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011313, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012212, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011321, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012452, lr 0.250000\n",
      "Average classification loss at step 0: 0.692595, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061482, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013343, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011631, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012383, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013471, lr 0.250000\n",
      "Average classification loss at step 0: 0.692594, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026343, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011664, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012212, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012388, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011463, lr 0.250000\n",
      "Average classification loss at step 0: 0.692593, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000737, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012250, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011640, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012866, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011748, lr 0.250000\n",
      "Average classification loss at step 0: 0.692592, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024755, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013331, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012401, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012606, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011414, lr 0.250000\n",
      "Average classification loss at step 0: 0.692592, lr 0.250000\n",
      "Average embedding loss at step 0: 1.985931, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010522, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011968, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011040, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012693, lr 0.250000\n",
      "Average classification loss at step 0: 0.692590, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039737, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010977, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.015064, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011464, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013239, lr 0.250000\n",
      "Average classification loss at step 0: 0.692589, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976052, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012292, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011464, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011620, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011732, lr 0.250000\n",
      "Average classification loss at step 0: 0.692588, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024613, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012673, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011627, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010670, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011943, lr 0.250000\n",
      "Average classification loss at step 0: 0.692587, lr 0.250000\n",
      "Average embedding loss at step 0: 2.100481, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012279, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013342, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011291, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011908, lr 0.250000\n",
      "Average classification loss at step 0: 0.692586, lr 0.250000\n",
      "Average embedding loss at step 0: 1.990061, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011975, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011220, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011709, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013093, lr 0.250000\n",
      "Average classification loss at step 0: 0.692585, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025629, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014274, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011051, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013039, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012194, lr 0.250000\n",
      "Average classification loss at step 0: 0.692584, lr 0.250000\n",
      "Average embedding loss at step 0: 2.068952, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013026, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012447, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013442, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011845, lr 0.250000\n",
      "Average classification loss at step 0: 0.692583, lr 0.250000\n",
      "Average embedding loss at step 0: 2.077707, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012685, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013172, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012736, lr 0.250000\n",
      "Average classification loss at step 0: 0.692582, lr 0.250000\n",
      "Average embedding loss at step 0: 2.041991, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012401, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011151, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013159, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013096, lr 0.250000\n",
      "Average classification loss at step 0: 0.692581, lr 0.250000\n",
      "Average embedding loss at step 0: 1.959025, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012112, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012275, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012032, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012298, lr 0.250000\n",
      "Average classification loss at step 0: 0.692580, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960669, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011353, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011749, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011852, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012658, lr 0.250000\n",
      "Average classification loss at step 0: 0.692579, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983189, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012834, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012167, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012805, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011467, lr 0.250000\n",
      "Average classification loss at step 0: 0.692578, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028063, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012405, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012623, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011939, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012553, lr 0.250000\n",
      "Average classification loss at step 0: 0.692577, lr 0.250000\n",
      "Average embedding loss at step 0: 2.103906, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012994, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012323, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012081, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012220, lr 0.250000\n",
      "Average classification loss at step 0: 0.692576, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011312, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012209, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011722, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011337, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012854, lr 0.250000\n",
      "Average classification loss at step 0: 0.692576, lr 0.250000\n",
      "Average embedding loss at step 0: 1.997973, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013180, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012606, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011719, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012247, lr 0.250000\n",
      "Average classification loss at step 0: 0.692575, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995927, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010949, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012041, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012992, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011636, lr 0.250000\n",
      "Average classification loss at step 0: 0.692574, lr 0.250000\n",
      "Average embedding loss at step 0: 2.067777, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010685, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010159, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012550, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012087, lr 0.250000\n",
      "Average classification loss at step 0: 0.692573, lr 0.250000\n",
      "Average embedding loss at step 0: 1.942583, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011914, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012058, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011995, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013127, lr 0.250000\n",
      "Average classification loss at step 0: 0.692572, lr 0.250000\n",
      "Average embedding loss at step 0: 1.957731, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013250, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013137, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011132, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012092, lr 0.250000\n",
      "Average classification loss at step 0: 0.692571, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039826, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011651, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011554, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010780, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012227, lr 0.250000\n",
      "Average classification loss at step 0: 0.692570, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011457, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012990, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.009373, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013239, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011548, lr 0.250000\n",
      "Average classification loss at step 0: 0.692569, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022237, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011330, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013119, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013137, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011180, lr 0.250000\n",
      "Average classification loss at step 0: 0.692568, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974139, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011737, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012595, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012069, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012000, lr 0.250000\n",
      "Average classification loss at step 0: 0.692567, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058796, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012229, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011214, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010834, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011411, lr 0.250000\n",
      "Average classification loss at step 0: 0.692567, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992471, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012150, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011498, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013399, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012764, lr 0.250000\n",
      "Average classification loss at step 0: 0.692566, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051105, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012131, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012627, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011862, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011781, lr 0.250000\n",
      "Average classification loss at step 0: 0.692565, lr 0.250000\n",
      "Average embedding loss at step 0: 2.052207, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012580, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012766, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011860, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012474, lr 0.250000\n",
      "Average classification loss at step 0: 0.692564, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965314, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012451, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012340, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013222, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011292, lr 0.250000\n",
      "Average classification loss at step 0: 0.692563, lr 0.250000\n",
      "Average embedding loss at step 0: 2.074840, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011176, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011947, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012579, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012451, lr 0.250000\n",
      "Average classification loss at step 0: 0.692562, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974673, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011925, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011453, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013200, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012894, lr 0.250000\n",
      "Average classification loss at step 0: 0.692561, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023026, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012489, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011975, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010919, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011138, lr 0.250000\n",
      "Average classification loss at step 0: 0.692560, lr 0.250000\n",
      "Average embedding loss at step 0: 2.064203, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010752, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013724, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013175, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011054, lr 0.250000\n",
      "Average classification loss at step 0: 0.692559, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022238, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011180, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013673, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012099, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012369, lr 0.250000\n",
      "Average classification loss at step 0: 0.692558, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999760, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011594, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011767, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012337, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011299, lr 0.250000\n",
      "Average classification loss at step 0: 0.692557, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974252, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011742, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012380, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011356, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011824, lr 0.250000\n",
      "Average classification loss at step 0: 0.692556, lr 0.250000\n",
      "Average embedding loss at step 0: 1.964894, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012190, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013292, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012746, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012507, lr 0.250000\n",
      "Average classification loss at step 0: 0.692556, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013658, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013308, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011302, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011809, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013418, lr 0.250000\n",
      "Average classification loss at step 0: 0.692555, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009497, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011280, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012448, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011511, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013095, lr 0.250000\n",
      "Average classification loss at step 0: 0.692554, lr 0.250000\n",
      "Average embedding loss at step 0: 2.012952, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011823, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011870, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013280, lr 0.250000\n",
      "Average classification loss at step 0: 0.692554, lr 0.250000\n",
      "Average embedding loss at step 0: 1.992377, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011140, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011893, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010722, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012507, lr 0.250000\n",
      "Average classification loss at step 0: 0.692553, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989212, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011489, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010625, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013560, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011630, lr 0.250000\n",
      "Average classification loss at step 0: 0.692552, lr 0.250000\n",
      "Average embedding loss at step 0: 1.939086, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011294, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011685, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011861, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012069, lr 0.250000\n",
      "Average classification loss at step 0: 0.692551, lr 0.250000\n",
      "Average embedding loss at step 0: 1.975260, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012938, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011394, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013186, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011281, lr 0.250000\n",
      "Average classification loss at step 0: 0.692550, lr 0.250000\n",
      "Average embedding loss at step 0: 2.097724, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012523, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012044, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012463, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011871, lr 0.250000\n",
      "Average classification loss at step 0: 0.692549, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043696, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012715, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011046, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012378, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010926, lr 0.250000\n",
      "Average classification loss at step 0: 0.692549, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001630, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012995, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012141, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011817, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012419, lr 0.250000\n",
      "Average classification loss at step 0: 0.692548, lr 0.250000\n",
      "Average embedding loss at step 0: 1.961410, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013561, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013283, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013186, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013348, lr 0.250000\n",
      "Average classification loss at step 0: 0.692547, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976194, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012078, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012493, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013158, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011213, lr 0.250000\n",
      "Average classification loss at step 0: 0.692546, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010090, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012952, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010849, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010558, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012725, lr 0.250000\n",
      "Average classification loss at step 0: 0.692545, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014442, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012120, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010712, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011577, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012000, lr 0.250000\n",
      "Average classification loss at step 0: 0.692545, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025563, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012762, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011582, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012664, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011697, lr 0.250000\n",
      "Average classification loss at step 0: 0.692544, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051519, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010980, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011955, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011295, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013514, lr 0.250000\n",
      "Average classification loss at step 0: 0.692543, lr 0.250000\n",
      "Average embedding loss at step 0: 1.968415, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012773, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012563, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011553, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010590, lr 0.250000\n",
      "Average classification loss at step 0: 0.692542, lr 0.250000\n",
      "Average embedding loss at step 0: 2.010951, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011150, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012564, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011227, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011507, lr 0.250000\n",
      "Average classification loss at step 0: 0.692542, lr 0.250000\n",
      "Average embedding loss at step 0: 1.974320, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012833, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012323, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011636, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011496, lr 0.250000\n",
      "Average classification loss at step 0: 0.692541, lr 0.250000\n",
      "Average embedding loss at step 0: 1.943634, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012829, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013313, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011771, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012484, lr 0.250000\n",
      "Average classification loss at step 0: 0.692540, lr 0.250000\n",
      "Average embedding loss at step 0: 2.073654, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012919, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011353, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011803, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012960, lr 0.250000\n",
      "Average classification loss at step 0: 0.692539, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998913, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012306, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010686, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.009751, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012382, lr 0.250000\n",
      "Average classification loss at step 0: 0.692539, lr 0.250000\n",
      "Average embedding loss at step 0: 2.079319, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010570, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012328, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011352, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012872, lr 0.250000\n",
      "Average classification loss at step 0: 0.692538, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035788, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013347, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011340, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012790, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011449, lr 0.250000\n",
      "Average classification loss at step 0: 0.692537, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016588, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012295, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010289, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011694, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011899, lr 0.250000\n",
      "Average classification loss at step 0: 0.692537, lr 0.250000\n",
      "Average embedding loss at step 0: 2.061749, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012810, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011812, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010393, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012591, lr 0.250000\n",
      "Average classification loss at step 0: 0.692536, lr 0.250000\n",
      "Average embedding loss at step 0: 1.987370, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013421, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011536, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012519, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013211, lr 0.250000\n",
      "Average classification loss at step 0: 0.692535, lr 0.250000\n",
      "Average embedding loss at step 0: 1.924791, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011318, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010405, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011609, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011520, lr 0.250000\n",
      "Average classification loss at step 0: 0.692534, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976174, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011665, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012524, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011342, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013344, lr 0.250000\n",
      "Average classification loss at step 0: 0.692534, lr 0.250000\n",
      "Average embedding loss at step 0: 2.069523, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011298, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012360, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012565, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012858, lr 0.250000\n",
      "Average classification loss at step 0: 0.692533, lr 0.250000\n",
      "Average embedding loss at step 0: 1.957543, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012145, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011348, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012289, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011693, lr 0.250000\n",
      "Average classification loss at step 0: 0.692532, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000790, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012141, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012542, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011731, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011605, lr 0.250000\n",
      "Average classification loss at step 0: 0.692532, lr 0.250000\n",
      "Average embedding loss at step 0: 2.066501, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013542, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011637, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012362, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011965, lr 0.250000\n",
      "Average classification loss at step 0: 0.692531, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017262, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013239, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012379, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010701, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010797, lr 0.250000\n",
      "Average classification loss at step 0: 0.692531, lr 0.250000\n",
      "Average embedding loss at step 0: 1.961692, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012612, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012691, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011326, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012353, lr 0.250000\n",
      "Average classification loss at step 0: 0.692530, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989511, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012716, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012628, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011340, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012427, lr 0.250000\n",
      "Average classification loss at step 0: 0.692529, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027523, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012350, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010689, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013011, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013859, lr 0.250000\n",
      "Average classification loss at step 0: 0.692528, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007985, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012441, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010358, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012069, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014224, lr 0.250000\n",
      "Average classification loss at step 0: 0.692527, lr 0.250000\n",
      "Average embedding loss at step 0: 2.022565, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013686, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011042, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011780, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012915, lr 0.250000\n",
      "Average classification loss at step 0: 0.692527, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011567, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.009924, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010602, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011978, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010729, lr 0.250000\n",
      "Average classification loss at step 0: 0.692526, lr 0.250000\n",
      "Average embedding loss at step 0: 1.996091, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011691, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011908, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012856, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012137, lr 0.250000\n",
      "Average classification loss at step 0: 0.692525, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009090, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013452, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011906, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011273, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011463, lr 0.250000\n",
      "Average classification loss at step 0: 0.692525, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017226, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012137, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012217, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012568, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011847, lr 0.250000\n",
      "Average classification loss at step 0: 0.692524, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002704, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011424, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011747, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013220, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012123, lr 0.250000\n",
      "Average classification loss at step 0: 0.692524, lr 0.250000\n",
      "Average embedding loss at step 0: 1.967351, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011892, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013285, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011927, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011680, lr 0.250000\n",
      "Average classification loss at step 0: 0.692523, lr 0.250000\n",
      "Average embedding loss at step 0: 1.977810, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011120, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012396, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012275, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012137, lr 0.250000\n",
      "Average classification loss at step 0: 0.692522, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998970, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010667, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011589, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011368, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013073, lr 0.250000\n",
      "Average classification loss at step 0: 0.692521, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978077, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013555, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012971, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011995, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011951, lr 0.250000\n",
      "Average classification loss at step 0: 0.692521, lr 0.250000\n",
      "Average embedding loss at step 0: 2.085442, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011833, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012008, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013917, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011558, lr 0.250000\n",
      "Average classification loss at step 0: 0.692520, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036687, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011154, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013012, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010428, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010712, lr 0.250000\n",
      "Average classification loss at step 0: 0.692520, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038375, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012474, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013210, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012118, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011848, lr 0.250000\n",
      "Average classification loss at step 0: 0.692519, lr 0.250000\n",
      "Average embedding loss at step 0: 2.005027, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013142, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012261, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012372, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013495, lr 0.250000\n",
      "Average classification loss at step 0: 0.692518, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004080, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013373, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011393, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012438, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011621, lr 0.250000\n",
      "Average classification loss at step 0: 0.692518, lr 0.250000\n",
      "Average embedding loss at step 0: 2.082172, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012249, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012140, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012265, lr 0.250000\n",
      "Average classification loss at step 0: 0.692517, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021705, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012528, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012543, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012409, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011714, lr 0.250000\n",
      "Average classification loss at step 0: 0.692516, lr 0.250000\n",
      "Average embedding loss at step 0: 2.062491, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011847, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012077, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011314, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010639, lr 0.250000\n",
      "Average classification loss at step 0: 0.692516, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057624, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012167, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012781, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010622, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011327, lr 0.250000\n",
      "Average classification loss at step 0: 0.692515, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989311, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012095, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011836, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012532, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012656, lr 0.250000\n",
      "Average classification loss at step 0: 0.692515, lr 0.250000\n",
      "Average embedding loss at step 0: 1.998182, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012347, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012224, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011024, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010968, lr 0.250000\n",
      "Average classification loss at step 0: 0.692514, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035546, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012588, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012575, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010964, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011137, lr 0.250000\n",
      "Average classification loss at step 0: 0.692513, lr 0.250000\n",
      "Average embedding loss at step 0: 2.027183, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012182, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013569, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012808, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010433, lr 0.250000\n",
      "Average classification loss at step 0: 0.692513, lr 0.250000\n",
      "Average embedding loss at step 0: 1.993815, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010514, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013059, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011672, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011115, lr 0.250000\n",
      "Average classification loss at step 0: 0.692512, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026511, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012051, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011514, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010844, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.009597, lr 0.250000\n",
      "Average classification loss at step 0: 0.692511, lr 0.250000\n",
      "Average embedding loss at step 0: 2.004727, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012477, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010733, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011586, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011887, lr 0.250000\n",
      "Average classification loss at step 0: 0.692511, lr 0.250000\n",
      "Average embedding loss at step 0: 2.039579, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010954, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012142, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010648, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013458, lr 0.250000\n",
      "Average classification loss at step 0: 0.692510, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025559, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011392, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012017, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012465, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012278, lr 0.250000\n",
      "Average classification loss at step 0: 0.692510, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007717, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012076, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012049, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012281, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012213, lr 0.250000\n",
      "Average classification loss at step 0: 0.692509, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018567, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013105, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011443, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012994, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012535, lr 0.250000\n",
      "Average classification loss at step 0: 0.692509, lr 0.250000\n",
      "Average embedding loss at step 0: 1.972715, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012193, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012742, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010451, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011925, lr 0.250000\n",
      "Average classification loss at step 0: 0.692508, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981488, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012111, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012894, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011992, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011706, lr 0.250000\n",
      "Average classification loss at step 0: 0.692507, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019158, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010292, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011504, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011319, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012003, lr 0.250000\n",
      "Average classification loss at step 0: 0.692507, lr 0.250000\n",
      "Average embedding loss at step 0: 2.028246, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012131, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012812, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011751, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011461, lr 0.250000\n",
      "Average classification loss at step 0: 0.692506, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023525, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011501, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013967, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011563, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013640, lr 0.250000\n",
      "Average classification loss at step 0: 0.692506, lr 0.250000\n",
      "Average embedding loss at step 0: 2.069872, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010719, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012362, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012097, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011569, lr 0.250000\n",
      "Average classification loss at step 0: 0.692506, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999321, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011175, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014013, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012065, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011110, lr 0.250000\n",
      "Average classification loss at step 0: 0.692505, lr 0.250000\n",
      "Average embedding loss at step 0: 1.971757, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012797, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011744, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012515, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010049, lr 0.250000\n",
      "Average classification loss at step 0: 0.692504, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978032, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012535, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011186, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012228, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013239, lr 0.250000\n",
      "Average classification loss at step 0: 0.692504, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054707, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011472, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013545, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011123, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011559, lr 0.250000\n",
      "Average classification loss at step 0: 0.692503, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965942, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013043, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011092, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011597, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012302, lr 0.250000\n",
      "Average classification loss at step 0: 0.692503, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029214, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011906, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013676, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011228, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012142, lr 0.250000\n",
      "Average classification loss at step 0: 0.692502, lr 0.250000\n",
      "Average embedding loss at step 0: 1.926362, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013221, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010807, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010245, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012266, lr 0.250000\n",
      "Average classification loss at step 0: 0.692502, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036126, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011614, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011625, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010945, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012793, lr 0.250000\n",
      "Average classification loss at step 0: 0.692501, lr 0.250000\n",
      "Average embedding loss at step 0: 1.960544, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011617, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012662, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012961, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010928, lr 0.250000\n",
      "Average classification loss at step 0: 0.692500, lr 0.250000\n",
      "Average embedding loss at step 0: 2.035913, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013139, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012074, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011836, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011948, lr 0.250000\n",
      "Average classification loss at step 0: 0.692500, lr 0.250000\n",
      "Average embedding loss at step 0: 1.930598, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011434, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011978, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010310, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011261, lr 0.250000\n",
      "Average classification loss at step 0: 0.692499, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032104, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011797, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011940, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011909, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011668, lr 0.250000\n",
      "Average classification loss at step 0: 0.692499, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023501, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013214, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010105, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012727, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011459, lr 0.250000\n",
      "Average classification loss at step 0: 0.692499, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995121, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.014305, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011116, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011161, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010443, lr 0.250000\n",
      "Average classification loss at step 0: 0.692498, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994896, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011759, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010777, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012186, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011062, lr 0.250000\n",
      "Average classification loss at step 0: 0.692497, lr 0.250000\n",
      "Average embedding loss at step 0: 2.050875, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013015, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010760, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011151, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013000, lr 0.250000\n",
      "Average classification loss at step 0: 0.692497, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029506, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012455, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010753, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010665, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012340, lr 0.250000\n",
      "Average classification loss at step 0: 0.692496, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013612, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012348, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011935, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012112, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011797, lr 0.250000\n",
      "Average classification loss at step 0: 0.692496, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019518, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010545, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013138, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012615, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012963, lr 0.250000\n",
      "Average classification loss at step 0: 0.692495, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025954, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012534, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012010, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012501, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012433, lr 0.250000\n",
      "Average classification loss at step 0: 0.692495, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057061, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013011, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011375, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011803, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013173, lr 0.250000\n",
      "Average classification loss at step 0: 0.692494, lr 0.250000\n",
      "Average embedding loss at step 0: 1.984122, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011023, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012599, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012672, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011585, lr 0.250000\n",
      "Average classification loss at step 0: 0.692494, lr 0.250000\n",
      "Average embedding loss at step 0: 2.000769, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011406, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011274, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011313, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011504, lr 0.250000\n",
      "Average classification loss at step 0: 0.692493, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016488, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011816, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010898, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012100, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011854, lr 0.250000\n",
      "Average classification loss at step 0: 0.692493, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983827, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012459, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011458, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011893, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011405, lr 0.250000\n",
      "Average classification loss at step 0: 0.692493, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981757, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011640, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012381, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011571, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011320, lr 0.250000\n",
      "Average classification loss at step 0: 0.692492, lr 0.250000\n",
      "Average embedding loss at step 0: 2.057235, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011687, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011182, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013016, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013084, lr 0.250000\n",
      "Average classification loss at step 0: 0.692491, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978139, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012244, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012273, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011488, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013137, lr 0.250000\n",
      "Average classification loss at step 0: 0.692491, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007550, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011432, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011374, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011540, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.014441, lr 0.250000\n",
      "Average classification loss at step 0: 0.692490, lr 0.250000\n",
      "Average embedding loss at step 0: 2.032107, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012278, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013388, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012377, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011675, lr 0.250000\n",
      "Average classification loss at step 0: 0.692490, lr 0.250000\n",
      "Average embedding loss at step 0: 2.048431, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011520, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011597, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010095, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011911, lr 0.250000\n",
      "Average classification loss at step 0: 0.692489, lr 0.250000\n",
      "Average embedding loss at step 0: 2.043989, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011313, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012647, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011023, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012118, lr 0.250000\n",
      "Average classification loss at step 0: 0.692489, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042506, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011910, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011141, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012824, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010797, lr 0.250000\n",
      "Average classification loss at step 0: 0.692488, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044663, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011335, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011876, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012131, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011669, lr 0.250000\n",
      "Average classification loss at step 0: 0.692488, lr 0.250000\n",
      "Average embedding loss at step 0: 2.034113, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011115, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012144, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011046, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013363, lr 0.250000\n",
      "Average classification loss at step 0: 0.692487, lr 0.250000\n",
      "Average embedding loss at step 0: 2.042146, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013471, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013364, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011970, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011011, lr 0.250000\n",
      "Average classification loss at step 0: 0.692487, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995762, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011941, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011388, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011807, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012288, lr 0.250000\n",
      "Average classification loss at step 0: 0.692486, lr 0.250000\n",
      "Average embedding loss at step 0: 2.013358, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012095, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010932, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011316, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012485, lr 0.250000\n",
      "Average classification loss at step 0: 0.692486, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009294, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011112, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012603, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012314, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011475, lr 0.250000\n",
      "Average classification loss at step 0: 0.692485, lr 0.250000\n",
      "Average embedding loss at step 0: 2.055634, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011748, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010716, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013140, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012614, lr 0.250000\n",
      "Average classification loss at step 0: 0.692485, lr 0.250000\n",
      "Average embedding loss at step 0: 2.016683, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012015, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011039, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011567, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010504, lr 0.250000\n",
      "Average classification loss at step 0: 0.692485, lr 0.250000\n",
      "Average embedding loss at step 0: 2.031887, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012137, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012459, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011621, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013159, lr 0.250000\n",
      "Average classification loss at step 0: 0.692484, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002633, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011735, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012114, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013291, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012289, lr 0.250000\n",
      "Average classification loss at step 0: 0.692484, lr 0.250000\n",
      "Average embedding loss at step 0: 2.084655, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011784, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012169, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011230, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011753, lr 0.250000\n",
      "Average classification loss at step 0: 0.692483, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060325, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011614, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011322, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012730, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011522, lr 0.250000\n",
      "Average classification loss at step 0: 0.692483, lr 0.250000\n",
      "Average embedding loss at step 0: 1.971074, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013965, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012748, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011386, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011422, lr 0.250000\n",
      "Average classification loss at step 0: 0.692483, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026703, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012493, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010017, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013454, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011343, lr 0.250000\n",
      "Average classification loss at step 0: 0.692482, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030968, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012276, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012368, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011126, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010903, lr 0.250000\n",
      "Average classification loss at step 0: 0.692482, lr 0.250000\n",
      "Average embedding loss at step 0: 1.946581, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010825, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010479, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011230, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012697, lr 0.250000\n",
      "Average classification loss at step 0: 0.692481, lr 0.250000\n",
      "Average embedding loss at step 0: 1.961774, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012550, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011406, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011720, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012657, lr 0.250000\n",
      "Average classification loss at step 0: 0.692481, lr 0.250000\n",
      "Average embedding loss at step 0: 2.078698, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012054, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011701, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012817, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010137, lr 0.250000\n",
      "Average classification loss at step 0: 0.692480, lr 0.250000\n",
      "Average embedding loss at step 0: 1.965014, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013265, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011207, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011909, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010464, lr 0.250000\n",
      "Average classification loss at step 0: 0.692480, lr 0.250000\n",
      "Average embedding loss at step 0: 2.046288, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013241, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011007, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011759, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012037, lr 0.250000\n",
      "Average classification loss at step 0: 0.692480, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051458, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011718, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012168, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011307, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013357, lr 0.250000\n",
      "Average classification loss at step 0: 0.692479, lr 0.250000\n",
      "Average embedding loss at step 0: 2.021134, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011830, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.014075, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011672, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011425, lr 0.250000\n",
      "Average classification loss at step 0: 0.692479, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001624, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011316, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011442, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012053, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012350, lr 0.250000\n",
      "Average classification loss at step 0: 0.692479, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051579, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012138, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011647, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010595, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012827, lr 0.250000\n",
      "Average classification loss at step 0: 0.692478, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017875, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012503, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011761, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011350, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011382, lr 0.250000\n",
      "Average classification loss at step 0: 0.692478, lr 0.250000\n",
      "Average embedding loss at step 0: 1.938103, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011135, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010056, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011490, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012027, lr 0.250000\n",
      "Average classification loss at step 0: 0.692477, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007646, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012170, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011769, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011868, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012824, lr 0.250000\n",
      "Average classification loss at step 0: 0.692477, lr 0.250000\n",
      "Average embedding loss at step 0: 2.024683, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012134, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012181, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012386, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012278, lr 0.250000\n",
      "Average classification loss at step 0: 0.692477, lr 0.250000\n",
      "Average embedding loss at step 0: 2.036515, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012360, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012132, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010670, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011727, lr 0.250000\n",
      "Average classification loss at step 0: 0.692476, lr 0.250000\n",
      "Average embedding loss at step 0: 2.038361, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010570, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010948, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012611, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012797, lr 0.250000\n",
      "Average classification loss at step 0: 0.692476, lr 0.250000\n",
      "Average embedding loss at step 0: 1.995126, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011441, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012021, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012250, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011285, lr 0.250000\n",
      "Average classification loss at step 0: 0.692476, lr 0.250000\n",
      "Average embedding loss at step 0: 1.905055, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012057, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011703, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010726, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012016, lr 0.250000\n",
      "Average classification loss at step 0: 0.692475, lr 0.250000\n",
      "Average embedding loss at step 0: 2.060345, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011404, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012675, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011526, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011877, lr 0.250000\n",
      "Average classification loss at step 0: 0.692475, lr 0.250000\n",
      "Average embedding loss at step 0: 1.972223, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012464, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011585, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011651, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011437, lr 0.250000\n",
      "Average classification loss at step 0: 0.692474, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044752, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011251, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011322, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010771, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011580, lr 0.250000\n",
      "Average classification loss at step 0: 0.692474, lr 0.250000\n",
      "Average embedding loss at step 0: 2.007205, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010464, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011591, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010112, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013016, lr 0.250000\n",
      "Average classification loss at step 0: 0.692473, lr 0.250000\n",
      "Average embedding loss at step 0: 2.045026, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013130, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012139, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011066, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011511, lr 0.250000\n",
      "Average classification loss at step 0: 0.692473, lr 0.250000\n",
      "Average embedding loss at step 0: 2.074999, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011434, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012137, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012548, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011427, lr 0.250000\n",
      "Average classification loss at step 0: 0.692473, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033713, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010330, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010826, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012631, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011660, lr 0.250000\n",
      "Average classification loss at step 0: 0.692472, lr 0.250000\n",
      "Average embedding loss at step 0: 2.134176, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012285, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010373, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013269, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011682, lr 0.250000\n",
      "Average classification loss at step 0: 0.692472, lr 0.250000\n",
      "Average embedding loss at step 0: 2.037735, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011510, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011152, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010952, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011625, lr 0.250000\n",
      "Average classification loss at step 0: 0.692472, lr 0.250000\n",
      "Average embedding loss at step 0: 2.018331, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010592, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011057, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012024, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010760, lr 0.250000\n",
      "Average classification loss at step 0: 0.692471, lr 0.250000\n",
      "Average embedding loss at step 0: 2.001314, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010997, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010189, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012789, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010379, lr 0.250000\n",
      "Average classification loss at step 0: 0.692471, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033955, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010634, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012345, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013941, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010709, lr 0.250000\n",
      "Average classification loss at step 0: 0.692470, lr 0.250000\n",
      "Average embedding loss at step 0: 1.966748, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011788, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011569, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011539, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012390, lr 0.250000\n",
      "Average classification loss at step 0: 0.692470, lr 0.250000\n",
      "Average embedding loss at step 0: 2.072834, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012500, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010161, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011302, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010549, lr 0.250000\n",
      "Average classification loss at step 0: 0.692470, lr 0.250000\n",
      "Average embedding loss at step 0: 2.017673, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012819, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011643, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012626, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011516, lr 0.250000\n",
      "Average classification loss at step 0: 0.692469, lr 0.250000\n",
      "Average embedding loss at step 0: 1.925648, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011534, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011527, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011370, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011143, lr 0.250000\n",
      "Average classification loss at step 0: 0.692469, lr 0.250000\n",
      "Average embedding loss at step 0: 1.989068, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011482, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012234, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012221, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011650, lr 0.250000\n",
      "Average classification loss at step 0: 0.692468, lr 0.250000\n",
      "Average embedding loss at step 0: 1.976247, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012171, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011509, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012261, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010953, lr 0.250000\n",
      "Average classification loss at step 0: 0.692468, lr 0.250000\n",
      "Average embedding loss at step 0: 2.011832, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011940, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011977, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010924, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010592, lr 0.250000\n",
      "Average classification loss at step 0: 0.692468, lr 0.250000\n",
      "Average embedding loss at step 0: 2.067075, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010678, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011004, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013831, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012629, lr 0.250000\n",
      "Average classification loss at step 0: 0.692468, lr 0.250000\n",
      "Average embedding loss at step 0: 2.026309, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012840, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011296, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011575, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012612, lr 0.250000\n",
      "Average classification loss at step 0: 0.692467, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025080, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010825, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013331, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012669, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010845, lr 0.250000\n",
      "Average classification loss at step 0: 0.692467, lr 0.250000\n",
      "Average embedding loss at step 0: 1.939243, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010292, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013069, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010981, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012568, lr 0.250000\n",
      "Average classification loss at step 0: 0.692466, lr 0.250000\n",
      "Average embedding loss at step 0: 1.939316, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010885, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011575, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011339, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011474, lr 0.250000\n",
      "Average classification loss at step 0: 0.692466, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991598, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011361, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012226, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011072, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011089, lr 0.250000\n",
      "Average classification loss at step 0: 0.692466, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019381, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012234, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012513, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011884, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012658, lr 0.250000\n",
      "Average classification loss at step 0: 0.692465, lr 0.250000\n",
      "Average embedding loss at step 0: 1.981109, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012967, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.009860, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011451, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012478, lr 0.250000\n",
      "Average classification loss at step 0: 0.692465, lr 0.250000\n",
      "Average embedding loss at step 0: 2.009341, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011987, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.013426, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011947, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012174, lr 0.250000\n",
      "Average classification loss at step 0: 0.692465, lr 0.250000\n",
      "Average embedding loss at step 0: 2.051386, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011591, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012723, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012685, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012185, lr 0.250000\n",
      "Average classification loss at step 0: 0.692464, lr 0.250000\n",
      "Average embedding loss at step 0: 2.030013, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011387, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010552, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011225, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011172, lr 0.250000\n",
      "Average classification loss at step 0: 0.692464, lr 0.250000\n",
      "Average embedding loss at step 0: 2.054084, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012234, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011784, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012514, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010856, lr 0.250000\n",
      "Average classification loss at step 0: 0.692464, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002908, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010609, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011283, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011771, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012210, lr 0.250000\n",
      "Average classification loss at step 0: 0.692464, lr 0.250000\n",
      "Average embedding loss at step 0: 2.029877, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013027, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011629, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011692, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010405, lr 0.250000\n",
      "Average classification loss at step 0: 0.692463, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020017, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011511, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011242, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011811, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010573, lr 0.250000\n",
      "Average classification loss at step 0: 0.692463, lr 0.250000\n",
      "Average embedding loss at step 0: 2.040427, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011463, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010428, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011577, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010878, lr 0.250000\n",
      "Average classification loss at step 0: 0.692463, lr 0.250000\n",
      "Average embedding loss at step 0: 1.991089, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012952, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012072, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011370, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012526, lr 0.250000\n",
      "Average classification loss at step 0: 0.692462, lr 0.250000\n",
      "Average embedding loss at step 0: 2.008363, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011880, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012381, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011259, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012096, lr 0.250000\n",
      "Average classification loss at step 0: 0.692462, lr 0.250000\n",
      "Average embedding loss at step 0: 2.020678, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012203, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012232, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012725, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010868, lr 0.250000\n",
      "Average classification loss at step 0: 0.692462, lr 0.250000\n",
      "Average embedding loss at step 0: 2.014968, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012074, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011027, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.009841, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012669, lr 0.250000\n",
      "Average classification loss at step 0: 0.692461, lr 0.250000\n",
      "Average embedding loss at step 0: 2.025037, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010973, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011387, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011462, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010653, lr 0.250000\n",
      "Average classification loss at step 0: 0.692461, lr 0.250000\n",
      "Average embedding loss at step 0: 2.103743, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011263, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011992, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010928, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010885, lr 0.250000\n",
      "Average classification loss at step 0: 0.692461, lr 0.250000\n",
      "Average embedding loss at step 0: 1.978282, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011921, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011235, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011775, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013608, lr 0.250000\n",
      "Average classification loss at step 0: 0.692460, lr 0.250000\n",
      "Average embedding loss at step 0: 2.044542, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012194, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011502, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012254, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011710, lr 0.250000\n",
      "Average classification loss at step 0: 0.692460, lr 0.250000\n",
      "Average embedding loss at step 0: 2.053017, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011118, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011497, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010865, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011842, lr 0.250000\n",
      "Average classification loss at step 0: 0.692460, lr 0.250000\n",
      "Average embedding loss at step 0: 2.019778, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011922, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011934, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012013, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010606, lr 0.250000\n",
      "Average classification loss at step 0: 0.692460, lr 0.250000\n",
      "Average embedding loss at step 0: 1.994774, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011675, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012728, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.012005, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012497, lr 0.250000\n",
      "Average classification loss at step 0: 0.692459, lr 0.250000\n",
      "Average embedding loss at step 0: 1.999010, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011939, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011879, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010405, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012531, lr 0.250000\n",
      "Average classification loss at step 0: 0.692459, lr 0.250000\n",
      "Average embedding loss at step 0: 2.023285, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011660, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011091, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010165, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011590, lr 0.250000\n",
      "Average classification loss at step 0: 0.692459, lr 0.250000\n",
      "Average embedding loss at step 0: 1.946845, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010298, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010739, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010571, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.012151, lr 0.250000\n",
      "Average classification loss at step 0: 0.692458, lr 0.250000\n",
      "Average embedding loss at step 0: 2.002027, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011932, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010537, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013440, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.009821, lr 0.250000\n",
      "Average classification loss at step 0: 0.692458, lr 0.250000\n",
      "Average embedding loss at step 0: 1.983120, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.010918, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011214, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011026, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011221, lr 0.250000\n",
      "Average classification loss at step 0: 0.692458, lr 0.250000\n",
      "Average embedding loss at step 0: 2.033004, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.013182, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011366, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010877, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011320, lr 0.250000\n",
      "Average classification loss at step 0: 0.692458, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058552, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.012333, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.011919, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.013096, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.013288, lr 0.250000\n",
      "Average classification loss at step 0: 0.692457, lr 0.250000\n",
      "Average embedding loss at step 0: 1.953636, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011641, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.010856, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.011759, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.010766, lr 0.250000\n",
      "Average classification loss at step 0: 0.692457, lr 0.250000\n",
      "Average embedding loss at step 0: 2.058074, lr 0.250000\n",
      "Average embedding loss at step 2000: 2.011671, lr 0.250000\n",
      "Average embedding loss at step 4000: 2.012250, lr 0.250000\n",
      "Average embedding loss at step 6000: 2.010419, lr 0.250000\n",
      "Average embedding loss at step 8000: 2.011441, lr 0.250000\n",
      "Average classification loss at step 0: 0.692457, lr 0.250000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-444-4afef91eedcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclf_idx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_y\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0maverage_clf_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_feature = False\n",
    "emb_steps = 10000 #50000001\n",
    "clf_steps = 1000\n",
    "\n",
    "total_step = 0\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    while (True):\n",
    "        average_emb_loss = 0\n",
    "        average_clf_loss = 0\n",
    "        for step in range(emb_steps):\n",
    "            batch_data, batch_labels, batch_path, batch_path_id, w_p2p = generate_batch(\n",
    "                batch_path_size, num_skips, skip_window)\n",
    "            feed_dict = {train_dataset : batch_data,\n",
    "                         train_labels : batch_labels, \n",
    "                         path_dataset : batch_path,\n",
    "                         path_id : batch_path_id,\n",
    "                         w_path2pair : w_p2p}\n",
    "            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_emb_loss += l\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_emb_loss = average_emb_loss / 2000.0\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average embedding loss at step %d: %f, lr %f' % (step, average_emb_loss, 0.25))\n",
    "                average_emb_loss = 0\n",
    "                \n",
    "        for step in range(clf_steps):\n",
    "            if (use_feature):\n",
    "                # for datasets in the icml paper\n",
    "                feed_dict = {clf_idx : trn_idx, clf_y : trn_y, feature_dataset : trn_f}\n",
    "            else:\n",
    "                feed_dict = {clf_idx : trn_idx, clf_y : trn_y}\n",
    "\n",
    "            _, l = session.run([clf_optimizer, clf_loss], feed_dict=feed_dict)\n",
    "            average_clf_loss += l\n",
    "            if step % 1000 == 0:\n",
    "                if step > 0:\n",
    "                    average_clf_loss = average_clf_loss / 1000.0\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average classification loss at step %d: %f, lr %f' % (step, average_clf_loss, 0.25))\n",
    "                average_clf_loss = 0\n",
    "                \n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#         y_p = tf.argmax(logit_y, 1)\n",
    "#         feed_dict = {clf_x : tst_x, clf_y : tst_y}\n",
    "#         _, l, res_logit_y = session.run([clf_optimizer, clf_loss, logit_y], feed_dict=feed_dict)\n",
    "#         y_true = np.argmax(tst_y,1)\n",
    "#         print(\"micro: \", f1_score(y_true, res_pred_y, average='micro'))\n",
    "#         print(\"macro: \", f1_score(y_true, res_pred_y, average='macro'))\n",
    "\n",
    "        res_y_pred = tf.argmax(logit_y, 1)\n",
    "        res_y_true = tf.argmax(clf_y, 1)\n",
    "        if (use_feature):\n",
    "            trn_y_pred = res_y_pred.eval({clf_idx : trn_idx, clf_y: trn_y, feature_dataset : trn_f})\n",
    "            trn_y_ture = res_y_true.eval({clf_idx : trn_idx, clf_y: trn_y, feature_dataset : trn_f})\n",
    "            tst_y_pred = res_y_pred.eval({clf_idx : tst_idx, clf_y: tst_y, feature_dataset : tst_f})\n",
    "            tst_y_ture = res_y_true.eval({clf_idx : tst_idx, clf_y: tst_y, feature_dataset : tst_f})\n",
    "            print(\"Epoch %d, trn acc %.6f acc %.6f:\" % (total_step,\n",
    "                                                        accuracy_score(trn_y_ture, trn_y_pred.flatten()),\n",
    "                                                        accuracy_score(tst_y_ture, tst_y_pred.flatten())))\n",
    "        else:\n",
    "            pass\n",
    "#             trn_y_pred = res_y_pred.eval({clf_idx : trn_idx, clf_y: trn_y})\n",
    "#             trn_y_ture = res_y_true.eval({clf_idx : trn_idx, clf_y: trn_y})\n",
    "#             tst_y_pred = res_y_pred.eval({clf_idx : tst_idx, clf_y: tst_y})\n",
    "#             tst_y_ture = res_y_true.eval({clf_idx : tst_idx, clf_y: tst_y})\n",
    "#             print(trn_y_pred.tolist())\n",
    "#             print(trn_y_ture.tolist())\n",
    "        \n",
    "        \n",
    "        if total_step % 1 == 0:\n",
    "            embedding_filename = '/hdd2/graph_embedding/customized/results/exp_blogcatalog_semi_avg1/blog_embeddings_iter%d.txt' %total_step\n",
    "            not_normal_embeddings = embeddings.eval()\n",
    "            ordered_embeddings = [not_normal_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "            np.savetxt(embedding_filename, ordered_embeddings)\n",
    "        \n",
    "        total_step += 1\n",
    "            \n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    not_normal_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "#     sess.run(init_op)\n",
    "    # Do some work with the model.\n",
    "#     dec_v2.op.run()\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, \"//hdd2/graph_embedding/customized/results/exp_blogcatalog_semi_avg1/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-4da9b1bed20e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/hdd2/graph_embedding/customized/blog_embeddings_1.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mordered_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings_1.txt'\n",
    "ordered_embeddings = [final_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "np.savetxt(embedding_filename, ordered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47477195,  0.47871515,  0.47899124,  0.4759534 ,  0.47790405,\n",
       "        0.47898862,  0.47662544,  0.48324525,  0.47535485,  0.47714397,\n",
       "        0.47675049,  0.47513527,  0.47733408,  0.4759779 ,  0.47573614,\n",
       "        0.47668341,  0.47703272,  0.47557175,  0.47846189,  0.47588319,\n",
       "        0.47464845,  0.47754869,  0.47674361,  0.48117602,  0.47582838,\n",
       "        0.47704467,  0.47620195,  0.47587633,  0.47659436,  0.47654676,\n",
       "        0.47580355,  0.47573981,  0.47579864,  0.47626203,  0.47682789,\n",
       "        0.47790003,  0.47652382,  0.47664452,  0.47489908], dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_logit_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(embedding_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58.47768402, -14.06970406, -11.84331131,   3.77727461,\n",
       "        27.18597221, -25.0004673 ,  65.72541809, -65.32319641,\n",
       "        46.84730148, -17.94046402,  -9.88634205,   4.96879911,\n",
       "       -39.52791977, -41.69575119,  74.56922913,   7.61631155,\n",
       "        23.03376389,  50.97651291, -33.59208679,  31.2266407 ,\n",
       "        81.93914032,  15.72993279,  -5.28370953,  17.18107605,\n",
       "       -75.29068756,  14.2514925 ,   7.94441748,  -4.89663982,\n",
       "       -51.30142212, -33.08482361, -32.42542267,   1.93974102,\n",
       "        46.80765533,   8.50759125,  10.44501019, -40.15472794,\n",
       "        -5.02595329, -36.50287247, -45.88523102, -79.72190094,\n",
       "       -17.26217461,   8.80545712, -52.35463333, -16.01053047,\n",
       "       -43.30438232, -39.79665756, -16.72794151,  34.34871674,\n",
       "        57.16449738,  -4.06061411, -72.57341003,  -7.9510541 ,\n",
       "        63.76834869,  43.05852509,   2.076653  , -98.20155334,\n",
       "        16.07836723,  46.57059479,  68.2883606 ,  -1.08950758,\n",
       "         5.00839376,   9.26826859,   8.00768375,  18.40788651,\n",
       "        57.93119049,  69.93483734, -30.36079407, -24.08821869,\n",
       "       -98.82420349,  56.28701782,  -6.49470615, -29.10713005,\n",
       "         9.16787815,  29.59499741,  82.98940277,  83.68479919,\n",
       "       -54.79275894,  25.96276855,  22.00436592, -61.77494812,\n",
       "       -33.84917831, -51.7205925 , -47.96949768, -18.86766052,\n",
       "       -10.83108044,   3.68754077,  65.11179352,  28.03749084,\n",
       "       -43.74489975,  14.33013916,  11.00716972,  48.05020142,\n",
       "       -17.2221508 ,  46.67128754,   9.17617607, -66.37744141,\n",
       "        28.96920204, -23.5886116 ,  48.79130173,   3.59684467,\n",
       "       -10.80480194,  -7.6785965 ,  23.84983635, -46.33642578,\n",
       "       -20.64024162,   0.37871927,  22.73223305,  52.55372238,\n",
       "        10.19163036, -11.5577383 ,  -4.63438082,   3.70495296,\n",
       "         7.59680653,  27.99442673,  29.33081055, -51.17831421,\n",
       "        59.33683395,  11.71917534,   9.37813663, -17.67382622,\n",
       "       -19.4407177 , -19.11290359, -16.30101204, -26.50016785,\n",
       "        37.48277664,   3.48921919,  24.05833054,  27.21180725])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 10308, 10309, 10310])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00, ...,\n",
       "         1.03090000e+04,   1.03100000e+04,   1.03110000e+04])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10312"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10311"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocabulary_size)\n",
    "couples, labels = skipgrams(data, vocabulary_size, window_size=10, sampling_table=sampling_table, negative_samples=5)\n",
    "# word_target, word_context = zip(*couples)\n",
    "# word_target = np.array(word_target, dtype=\"int32\")\n",
    "# word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "# print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824960\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in walks_corpus:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Skipgram(sentences=walks_corpus, vocabulary_counts=vocabulary_size, size=128,\n",
    "#                  window=10, min_count=0, trim_rule=None, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "#     embeddings = np.loadtxt(embeddings_file)\n",
    "    \n",
    "    ## for original deepwalk\n",
    "    #model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    \n",
    "    ## for external word2vec lib\n",
    "    model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "#     features_matrix = embeddings\n",
    "\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "\n",
    "    # use other word2vec lib\n",
    "    features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8496e862a2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_filename_other_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-e951e3ee7ec9>\u001b[0m in \u001b[0;36mscoring\u001b[0;34m(emb_filename, matfile)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m## for external word2vec lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 2. Load labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "embedding_filename_other_lib = '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\n",
    "\n",
    "scoring('', matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "model = KeyedVectors.load_word2vec_format(embedding_filename_original, binary=False)\n",
    "                                          \n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embeddings = np.loadtxt(embedding_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.110529  ,  0.19354101,  0.3969    ,  0.16305301, -0.198276  ,\n",
       "       -0.219273  ,  0.045498  ,  0.138824  , -0.174495  ,  0.20618799,\n",
       "        0.48984301,  0.302856  , -0.46013999,  0.227097  ,  0.071323  ,\n",
       "       -0.43010399,  0.015921  , -0.021136  ,  0.28175899, -0.26225701,\n",
       "        0.102745  , -0.31963301,  0.058964  ,  0.132149  ,  0.176946  ,\n",
       "       -0.155433  ,  0.160973  , -0.282166  ,  0.030017  , -0.165079  ,\n",
       "        0.25168899, -0.52189702,  0.005448  ,  0.18592501, -0.013992  ,\n",
       "       -0.070675  ,  0.033961  ,  0.117675  , -0.073083  ,  0.068748  ,\n",
       "       -0.10755   ,  0.068676  ,  0.17162   , -0.136898  ,  0.17979699,\n",
       "       -0.106551  , -0.212037  ,  0.103523  , -0.242975  , -0.46731299,\n",
       "        0.182107  ,  0.092075  , -0.141946  ,  0.051342  ,  0.31653801,\n",
       "        0.28085399, -0.029812  ,  0.19881   ,  0.31846499, -0.12293   ,\n",
       "        0.22415499, -0.21315201, -0.220193  , -0.15813901,  0.104372  ,\n",
       "       -0.038969  ,  0.020534  , -0.18385699, -0.049377  ,  0.241363  ,\n",
       "        0.15504301,  0.27251899, -0.20202599,  0.074011  ,  0.248133  ,\n",
       "       -0.015073  ,  0.046193  , -0.124894  , -0.06814   , -0.128296  ,\n",
       "        0.020906  , -0.07755   , -0.172672  ,  0.032093  ,  0.156846  ,\n",
       "       -0.019415  ,  0.051345  , -0.13377801, -0.195548  ,  0.128029  ,\n",
       "        0.066909  ,  0.13171799, -0.214425  ,  0.097262  , -0.20645601,\n",
       "       -0.114667  , -0.193297  , -0.218493  , -0.14356001, -0.06486   ,\n",
       "        0.078362  ,  0.043282  , -0.195365  ,  0.035029  , -0.100952  ,\n",
       "        0.028167  , -0.254026  ,  0.080769  ,  0.138097  , -0.118832  ,\n",
       "        0.22276799, -0.099504  ,  0.045412  ,  0.141414  ,  0.123148  ,\n",
       "        0.13963801, -0.004271  ,  0.172041  ,  0.12623601,  0.24216899,\n",
       "       -0.219863  ,  0.27274099,  0.026154  ,  0.10464   , -0.023742  ,\n",
       "       -0.206168  , -0.33293399, -0.034799  ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.47322476e-01,   2.67052501e-02,   1.52672976e-02,\n",
       "        -1.37635857e-01,  -6.47586957e-02,  -2.46349778e-02,\n",
       "        -8.18009302e-02,   6.91710338e-02,  -9.66271833e-02,\n",
       "        -9.89247933e-02,   5.70955575e-02,   1.53366596e-01,\n",
       "         8.30798447e-02,  -1.44452751e-01,   9.11711424e-04,\n",
       "         1.01996966e-01,   4.97803837e-02,  -4.91250493e-02,\n",
       "         1.03872299e-01,  -1.11238003e-01,   3.35200056e-02,\n",
       "         6.37169033e-02,  -3.92291024e-02,  -2.92820130e-02,\n",
       "        -2.85616820e-03,  -3.53677608e-02,  -5.77375665e-02,\n",
       "        -3.55093554e-02,   4.43368331e-02,  -1.63600937e-01,\n",
       "        -5.06901741e-02,   1.09737791e-01,  -6.03695633e-03,\n",
       "        -9.09792027e-04,  -9.44678709e-02,  -5.35581484e-02,\n",
       "         5.67527264e-02,  -2.60556079e-02,   1.89456977e-02,\n",
       "         2.38609854e-02,   4.84210551e-02,   1.34210438e-01,\n",
       "        -1.30895942e-01,   6.51589632e-02,   6.92558140e-02,\n",
       "        -5.88307390e-03,  -1.87402926e-02,   1.52439341e-01,\n",
       "        -7.65966475e-02,  -1.14347152e-02,  -5.93073331e-02,\n",
       "         3.97681519e-02,  -2.26105414e-02,  -1.00840345e-01,\n",
       "        -2.73720361e-02,  -1.35795742e-01,  -7.55931512e-02,\n",
       "         6.89051822e-02,  -9.89682674e-02,   8.19836278e-03,\n",
       "         9.56752300e-02,   1.11364149e-01,  -5.73285446e-02,\n",
       "        -7.95298442e-02,  -6.94709942e-02,   1.97762456e-02,\n",
       "        -7.93395117e-02,   5.48537821e-02,  -7.36792460e-02,\n",
       "         4.13507484e-02,   1.48582190e-01,  -1.37633294e-01,\n",
       "        -1.10649310e-01,   5.96768968e-02,   9.31203440e-02,\n",
       "         1.78780377e-01,  -1.70841515e-01,   9.95152742e-02,\n",
       "         9.34500918e-02,   1.38932645e-01,  -1.15846768e-01,\n",
       "         3.72944437e-02,  -1.21906027e-01,  -1.49959236e-01,\n",
       "        -6.60825595e-02,   8.33349824e-02,   2.63328534e-02,\n",
       "         3.53457741e-02,   1.41411439e-01,  -7.07744882e-02,\n",
       "         1.21387757e-01,   5.68825640e-02,  -1.21080384e-01,\n",
       "         1.42978668e-01,  -1.67644396e-01,   1.68275498e-02,\n",
       "        -9.96617302e-02,   4.47709225e-02,   2.08472624e-01,\n",
       "        -1.08954571e-01,  -8.83026272e-02,  -1.23706110e-01,\n",
       "        -5.51782846e-02,   1.30027205e-01,   5.61708212e-02,\n",
       "         7.12516978e-02,   3.37933712e-02,   8.94594342e-02,\n",
       "         1.06588854e-02,  -2.62339506e-02,   4.57158871e-02,\n",
       "        -2.36465354e-02,   1.83984548e-01,  -5.11433706e-02,\n",
       "         1.30957412e-02,   1.15885787e-01,   1.39660118e-02,\n",
       "         2.78853234e-02,  -1.25376284e-01,  -4.51841056e-02,\n",
       "         2.44418718e-02,  -9.92230922e-02,   7.17640370e-02,\n",
       "        -5.39226495e-02,   9.22740158e-03,   1.02272414e-01,\n",
       "        -8.26397240e-02,   1.59677104e-04])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat(matfile)\n",
    "A = mat['network']\n",
    "labels_matrix = mat['group']\n",
    "labels_count = labels_matrix.shape[1]\n",
    "mlb = MultiLabelBinarizer(range(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MultiLabelBinarizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fd4e0751c0ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MultiLabelBinarizer' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-13 21:07:11,853 : INFO : collecting all words and their counts\n",
      "2018-03-13 21:07:11,855 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-13 21:07:11,857 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-03-13 21:07:11,858 : INFO : Loading a fresh vocabulary\n",
      "2018-03-13 21:07:11,860 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-03-13 21:07:11,863 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-03-13 21:07:11,865 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-03-13 21:07:11,868 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-03-13 21:07:11,870 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-03-13 21:07:11,872 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-03-13 21:07:11,874 : INFO : resetting layer weights\n",
      "2018-03-13 21:07:11,876 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-03-13 21:07:11,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,883 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,892 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,900 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 183 effective words/s\n",
      "2018-03-13 21:07:11,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,907 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,916 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,917 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 25 effective words/s\n",
      "2018-03-13 21:07:11,918 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 5\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00497301], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8\n",
      "Words processed: 17000K     Vocab size: 4399K  \n",
      "Vocab size (unigrams + bigrams): 2419827\n",
      "Words in train file: 17005206\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('/hdd2/graph_embedding/tmp/text8', '/hdd2/graph_embedding/tmp/text8-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8-phrases\n",
      "Vocab size: 98331\n",
      "Words in train file: 15857306\n",
      "Alpha: 0.000002  Progress: 100.04%  Words/thread/sec: 467.25k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('/hdd2/graph_embedding/tmp/text8-phrases', '/hdd2/graph_embedding/tmp/text8.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 7] Argument list too long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-bd9adf4b8805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word2vec.word2vec(data,\n\u001b[1;32m      2\u001b[0m                   \u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.1.bin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                   size=128, verbose=True)\n\u001b[0m",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/word2vec/scripts_interface.pyc\u001b[0m in \u001b[0;36mword2vec\u001b[0;34m(train, output, size, window, sample, hs, negative, threads, iter_, min_count, alpha, debug, binary, cbow, save_vocab, read_vocab, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/word2vec/scripts_interface.pyc\u001b[0m in \u001b[0;36mrun_cmd\u001b[0;34m(command, verbose)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     proc = subprocess.Popen(command, stdout=subprocess.PIPE,\n\u001b[0;32m--> 142\u001b[0;31m                             stderr=subprocess.PIPE)\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 7] Argument list too long"
     ]
    }
   ],
   "source": [
    "word2vec.word2vec(data,\n",
    "                  '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.1.bin', \n",
    "                  size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03300432, -0.07565996, -0.16463131, -0.10145742,  0.00168844,\n",
       "       -0.09007662, -0.03259023, -0.22714335, -0.00282227, -0.05083675,\n",
       "       -0.07032426, -0.15832779, -0.13998848,  0.07574239,  0.01739285,\n",
       "        0.05170207, -0.01704509, -0.04309488,  0.13984987, -0.07098495,\n",
       "       -0.08131668, -0.06108802,  0.03238546,  0.03199002, -0.14047897,\n",
       "        0.1192169 , -0.11983237,  0.17141342, -0.04238614, -0.07682881,\n",
       "       -0.09226497, -0.08641661, -0.05119943,  0.04880232,  0.05202007,\n",
       "        0.11911941,  0.02494194,  0.0460702 , -0.02544389,  0.02227429,\n",
       "       -0.02918422,  0.08963519, -0.01449389,  0.13147458,  0.04316   ,\n",
       "        0.05062373,  0.01030556,  0.14064927,  0.05300813,  0.09054667,\n",
       "        0.16458763,  0.00496632,  0.09853401, -0.09157445,  0.03466847,\n",
       "       -0.192109  , -0.04286476,  0.0337339 ,  0.09414463,  0.02883872,\n",
       "       -0.12935385,  0.05580147,  0.04444493,  0.12305645, -0.06843153,\n",
       "       -0.05650309, -0.06930542, -0.0933516 , -0.08945961,  0.0355445 ,\n",
       "        0.05217047, -0.09187157,  0.0095493 , -0.16591462,  0.02874937,\n",
       "       -0.0271637 , -0.1071742 , -0.07731676,  0.17057821, -0.08997511,\n",
       "       -0.15251309,  0.0335523 ,  0.08237864,  0.05653743,  0.05472478,\n",
       "       -0.16204691,  0.09632239, -0.0988512 , -0.12040844, -0.13067709,\n",
       "       -0.03478862, -0.07479616,  0.07388658,  0.08031294, -0.12074082,\n",
       "       -0.04398282, -0.04163745,  0.09059986,  0.1189674 ,  0.05980834,\n",
       "       -0.03435113, -0.12926504,  0.06568849, -0.04446449, -0.093913  ,\n",
       "       -0.02966127, -0.00469951, -0.15404062,  0.07520716, -0.02498432,\n",
       "        0.12759562, -0.08096215, -0.07254944,  0.04480873, -0.19941515,\n",
       "       -0.0785991 ,  0.00944981, -0.00052774,  0.031948  ,  0.05943104,\n",
       "       -0.03682529,  0.02468612, -0.00828248,  0.08574598, -0.08077673,\n",
       "        0.09135576,  0.03673941,  0.01044743])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
