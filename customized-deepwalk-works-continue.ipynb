{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from multiprocessing import cpu_count\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "import word2vec\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "    embeddings = np.loadtxt(embeddings_file)\n",
    "    \n",
    "    ## for original deepwalk\n",
    "    #model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    \n",
    "    ## for external word2vec lib\n",
    "#     model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "    features_matrix = embeddings\n",
    "\n",
    "    # original code\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "\n",
    "    # use other word2vec lib\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "#     rand_list = range(len(model.vocab)-1)\n",
    "# #     random.shuffle(rand_list)\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in rand_list])\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix, random_state = 1))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'blogcatalog'\n",
    "\n",
    "embedding_size = 128\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 10\n",
    "path_size = 10\n",
    "\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = ['/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0']\n",
    "dataset = genfromtxt(file_list[0], delimiter=' ')\n",
    "\n",
    "def get_num_vacabulary(dataset):\n",
    "    word_count = 0\n",
    "    for d in dataset:\n",
    "        word_count = max(word_count, max(d))\n",
    "    return int(word_count)\n",
    "\n",
    "vocabulary_size = get_num_vacabulary(dataset) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = dataset.flatten()\n",
    "words = [str(int(w)) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614', '5573', '4732', '1159', '448', '5704', '5382', '1008', '5395', '6958']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1614.,   5573.,   4732.,   1159.,    448.,   5704.,   5382.,\n",
       "         1008.,   5395.,   6958.,   1320.,   4139.,   6814.,   5404.,\n",
       "         7988.,   8384.,  10139.,   6248.,   4226.,   2829.,   9090.,\n",
       "         8456.,   6892.,    738.,   3898.,   6958.,   3048.,   6796.,\n",
       "         7636.,   4104.,   7450.,   1225.,   6061.,    457.,   8968.,\n",
       "           35.,   1141.,   4996.,   1453.,   2979.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common words (+UNK)', [('4838', 196735), ('175', 195187), ('4373', 169490), ('8156', 147968), ('1225', 137221)])\n",
      "('Sample data', [1193, 2874, 225, 457, 19, 34, 5662, 37, 3046, 29])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = []\n",
    "#     count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "#         else:\n",
    "#             index = 0  # dictionary['UNK']\n",
    "        data.append(index)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1193, 2874, 225, 457, 19, 34, 5662, 37, 3046, 29, 1978, 802, 5882, 1526, 32, 883, 168, 1850, 788, 194, 1969]\n",
      "[  1614.   5573.   4732.   1159.    448.   5704.   5382.   1008.   5395.\n",
      "   6958.   1320.   4139.   6814.   5404.   7988.   8384.  10139.   6248.\n",
      "   4226.   2829.   9090.]\n"
     ]
    }
   ],
   "source": [
    "print(data[:21])\n",
    "print(dataset[0][:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1476"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['9809']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 20 and skip_window = 10:\n"
     ]
    }
   ],
   "source": [
    "path_index = 0\n",
    "batch_path_size = 1\n",
    "batch_size = batch_path_size * 400\n",
    "def generate_batch(batch_path_size, num_skips, skip_window):\n",
    "    global path_index\n",
    "    assert batch_size % num_skips == 0\n",
    "#     print(num_skips, skip_window)\n",
    "    assert num_skips <= 2 * skip_window\n",
    "#     batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "#     labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    batch = []\n",
    "    labels = []\n",
    "#     span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=batch_path_size)\n",
    "    for i in range(batch_path_size):\n",
    "        len_path = len(dataset[path_index])\n",
    "        for l in range(skip_window, len_path - skip_window): # [ skip_window target skip_window ]\n",
    "            for m in range(l - skip_window, l + skip_window + 1):\n",
    "                if m < 0 or m >= len_path or m == l: \n",
    "                    continue\n",
    "                batch.append(dictionary[str(int(dataset[path_index][l]))])\n",
    "                labels.append(dictionary[str(int(dataset[path_index][m]))])\n",
    "        \n",
    "        path_index = (path_index + 1) % len(dataset)\n",
    "    return (np.asarray(batch, dtype = np.int32), \n",
    "            np.asarray(labels, dtype = np.int32).reshape([len(labels), 1]))\n",
    "\n",
    "# print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for batch_path_size, num_skips, skip_window in [(2, 20, 10)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_path_size=batch_path_size, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "#     print('    batch node id:', [reverse_dictionary[bi] for bi in batch])\n",
    "#     print('    node position:')\n",
    "#     print(batch.tolist())\n",
    "#     print(labels)\n",
    "#     print('    labels:', [reverse_dictionary[li] for li in labels.reshape(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4838'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # inject trained embedding\n",
    "# model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "# features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "# # make the order consistant to the current one\n",
    "# reordered_list = np.asarray([int(reverse_dictionary[idx]) for idx in range(len(model.vocab)-1)])\n",
    "# reordered_features_matrix = features_matrix[reordered_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproduce Gensim weights initialization\n",
    "def seeded_vector(seed_string, vector_size):\n",
    "    \"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\n",
    "    # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "    once = np.random.RandomState(hash(seed_string) & 0xffffffff)\n",
    "    return (once.rand(vector_size) - 0.5) / vector_size\n",
    "\n",
    "features_list = []\n",
    "for idx in range(vocabulary_size):\n",
    "    str_node = reverse_dictionary[idx]\n",
    "    features_list.append(seeded_vector(str_node + str(1), 128))\n",
    "features_matrix = np.asarray(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_path_size = 2\n",
    "batch_size = batch_path_size * 400\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 10 # How many words to consider left and right.\n",
    "num_skips = 20 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "\n",
    "    # Variables.\n",
    "#     embeddings = tf.Variable(\n",
    "#         tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embeddings = tf.Variable(features_matrix, dtype=tf.float32)\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "#     learning_rate = tf.train.exponential_decay(\n",
    "#         learning_rate = 0.25,                # Base learning rate.\n",
    "#         global_step = global_step,  # Current index into the dataset.\n",
    "#         decay_steps = 10000,          # Decay step.\n",
    "#         decay_rate = 0.96,                # Decay rate.\n",
    "#         staircase=True)\n",
    "\n",
    "    learning_rate = tf.train.polynomial_decay(\n",
    "        learning_rate = 0.25,                # Base learning rate.\n",
    "        global_step = global_step,  # Current index into the dataset.\n",
    "        decay_steps = 10000,          # Decay step.\n",
    "        end_learning_rate = 0.025,\n",
    "        power = 1)\n",
    "    \n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.0338\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    batch_data, batch_labels = generate_batch(\n",
    "        batch_path_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l, res_embed = session.run([optimizer, loss, embed], feed_dict=feed_dict)\n",
    "    print('loss: %.4f' %(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.109962\n",
      "Average loss at step 2000: 5.028637\n",
      "Average loss at step 4000: 4.744091\n",
      "Average loss at step 6000: 4.579077\n",
      "Average loss at step 8000: 4.487539\n",
      "Average loss at step 10000: 4.416965\n",
      "Average loss at step 12000: 4.370949\n",
      "Average loss at step 14000: 4.336626\n",
      "Average loss at step 16000: 4.305372\n",
      "Average loss at step 18000: 4.276690\n",
      "Average loss at step 20000: 4.256679\n",
      "Average loss at step 22000: 4.241553\n",
      "Average loss at step 24000: 4.229317\n",
      "Average loss at step 26000: 4.220893\n",
      "Average loss at step 28000: 4.207267\n",
      "Average loss at step 30000: 4.195736\n",
      "Average loss at step 32000: 4.188950\n",
      "Average loss at step 34000: 4.183138\n",
      "Average loss at step 36000: 4.176661\n",
      "Average loss at step 38000: 4.170078\n",
      "Average loss at step 40000: 4.165262\n",
      "Average loss at step 42000: 4.159073\n",
      "Average loss at step 44000: 4.156700\n",
      "Average loss at step 46000: 4.151855\n",
      "Average loss at step 48000: 4.147589\n",
      "Average loss at step 50000: 4.145404\n",
      "Average loss at step 52000: 4.143178\n",
      "Average loss at step 54000: 4.136470\n",
      "Average loss at step 56000: 4.136927\n",
      "Average loss at step 58000: 4.129803\n",
      "Average loss at step 60000: 4.130845\n",
      "Average loss at step 62000: 4.129246\n",
      "Average loss at step 64000: 4.127043\n",
      "Average loss at step 66000: 4.124818\n",
      "Average loss at step 68000: 4.124264\n",
      "Average loss at step 70000: 4.120675\n",
      "Average loss at step 72000: 4.119937\n",
      "Average loss at step 74000: 4.118239\n",
      "Average loss at step 76000: 4.115878\n",
      "Average loss at step 78000: 4.117249\n",
      "Average loss at step 80000: 4.113847\n",
      "Average loss at step 82000: 4.112964\n",
      "Average loss at step 84000: 4.114446\n",
      "Average loss at step 86000: 4.110197\n",
      "Average loss at step 88000: 4.109316\n",
      "Average loss at step 90000: 4.108076\n",
      "Average loss at step 92000: 4.107904\n",
      "Average loss at step 94000: 4.105460\n",
      "Average loss at step 96000: 4.106893\n",
      "Average loss at step 98000: 4.105742\n",
      "Average loss at step 100000: 4.105104\n",
      "Average loss at step 102000: 4.105279\n",
      "Average loss at step 104000: 4.102577\n",
      "Average loss at step 106000: 4.101827\n",
      "Average loss at step 108000: 4.101463\n",
      "Average loss at step 110000: 4.099994\n",
      "Average loss at step 112000: 4.099750\n",
      "Average loss at step 114000: 4.100039\n",
      "Average loss at step 116000: 4.100691\n",
      "Average loss at step 118000: 4.096819\n",
      "Average loss at step 120000: 4.098074\n",
      "Average loss at step 122000: 4.096296\n",
      "Average loss at step 124000: 4.096971\n",
      "Average loss at step 126000: 4.095192\n",
      "Average loss at step 128000: 4.094974\n",
      "Average loss at step 130000: 4.095752\n",
      "Average loss at step 132000: 4.095762\n",
      "Average loss at step 134000: 4.094827\n",
      "Average loss at step 136000: 4.094584\n",
      "Average loss at step 138000: 4.092517\n",
      "Average loss at step 140000: 4.094396\n",
      "Average loss at step 142000: 4.093047\n",
      "Average loss at step 144000: 4.092995\n",
      "Average loss at step 146000: 4.093046\n",
      "Average loss at step 148000: 4.092744\n",
      "Average loss at step 150000: 4.090691\n",
      "Average loss at step 152000: 4.090422\n",
      "Average loss at step 154000: 4.090376\n",
      "Average loss at step 156000: 4.089694\n",
      "Average loss at step 158000: 4.090931\n",
      "Average loss at step 160000: 4.088970\n",
      "Average loss at step 162000: 4.090761\n",
      "Average loss at step 164000: 4.090591\n",
      "Average loss at step 166000: 4.088004\n",
      "Average loss at step 168000: 4.087439\n",
      "Average loss at step 170000: 4.089066\n",
      "Average loss at step 172000: 4.087593\n",
      "Average loss at step 174000: 4.087053\n",
      "Average loss at step 176000: 4.088584\n",
      "Average loss at step 178000: 4.089221\n",
      "Average loss at step 180000: 4.086989\n",
      "Average loss at step 182000: 4.086656\n",
      "Average loss at step 184000: 4.088544\n",
      "Average loss at step 186000: 4.084731\n",
      "Average loss at step 188000: 4.086715\n",
      "Average loss at step 190000: 4.084876\n",
      "Average loss at step 192000: 4.086866\n",
      "Average loss at step 194000: 4.083886\n",
      "Average loss at step 196000: 4.088003\n",
      "Average loss at step 198000: 4.085372\n",
      "Average loss at step 200000: 4.084073\n",
      "Average loss at step 202000: 4.086436\n",
      "Average loss at step 204000: 4.083340\n",
      "Average loss at step 206000: 4.085147\n",
      "Average loss at step 208000: 4.083564\n",
      "Average loss at step 210000: 4.084420\n",
      "Average loss at step 212000: 4.083686\n",
      "Average loss at step 214000: 4.083728\n",
      "Average loss at step 216000: 4.082747\n",
      "Average loss at step 218000: 4.083287\n",
      "Average loss at step 220000: 4.084679\n",
      "Average loss at step 222000: 4.085277\n",
      "Average loss at step 224000: 4.081993\n",
      "Average loss at step 226000: 4.083545\n",
      "Average loss at step 228000: 4.083853\n",
      "Average loss at step 230000: 4.080964\n",
      "Average loss at step 232000: 4.082898\n",
      "Average loss at step 234000: 4.081146\n",
      "Average loss at step 236000: 4.082557\n",
      "Average loss at step 238000: 4.080684\n",
      "Average loss at step 240000: 4.082571\n",
      "Average loss at step 242000: 4.084061\n",
      "Average loss at step 244000: 4.081722\n",
      "Average loss at step 246000: 4.081630\n",
      "Average loss at step 248000: 4.080664\n",
      "Average loss at step 250000: 4.081492\n",
      "Average loss at step 252000: 4.082330\n",
      "Average loss at step 254000: 4.080300\n",
      "Average loss at step 256000: 4.080418\n",
      "Average loss at step 258000: 4.081151\n",
      "Average loss at step 260000: 4.081716\n",
      "Average loss at step 262000: 4.078724\n",
      "Average loss at step 264000: 4.078821\n",
      "Average loss at step 266000: 4.078378\n",
      "Average loss at step 268000: 4.081736\n",
      "Average loss at step 270000: 4.080744\n",
      "Average loss at step 272000: 4.080286\n",
      "Average loss at step 274000: 4.082587\n",
      "Average loss at step 276000: 4.080670\n",
      "Average loss at step 278000: 4.080744\n",
      "Average loss at step 280000: 4.078733\n",
      "Average loss at step 282000: 4.078253\n",
      "Average loss at step 284000: 4.078959\n",
      "Average loss at step 286000: 4.077164\n",
      "Average loss at step 288000: 4.080631\n",
      "Average loss at step 290000: 4.077798\n",
      "Average loss at step 292000: 4.079117\n",
      "Average loss at step 294000: 4.078901\n",
      "Average loss at step 296000: 4.079584\n",
      "Average loss at step 298000: 4.078437\n",
      "Average loss at step 300000: 4.077609\n",
      "Average loss at step 302000: 4.078210\n",
      "Average loss at step 304000: 4.081754\n",
      "Average loss at step 306000: 4.078394\n",
      "Average loss at step 308000: 4.078443\n",
      "Average loss at step 310000: 4.078911\n",
      "Average loss at step 312000: 4.078519\n",
      "Average loss at step 314000: 4.076763\n",
      "Average loss at step 316000: 4.080140\n",
      "Average loss at step 318000: 4.077849\n",
      "Average loss at step 320000: 4.077538\n",
      "Average loss at step 322000: 4.079400\n",
      "Average loss at step 324000: 4.077531\n",
      "Average loss at step 326000: 4.077323\n",
      "Average loss at step 328000: 4.079244\n",
      "Average loss at step 330000: 4.075482\n",
      "Average loss at step 332000: 4.077496\n",
      "Average loss at step 334000: 4.078768\n",
      "Average loss at step 336000: 4.076465\n",
      "Average loss at step 338000: 4.076780\n",
      "Average loss at step 340000: 4.077343\n",
      "Average loss at step 342000: 4.080322\n",
      "Average loss at step 344000: 4.077166\n",
      "Average loss at step 346000: 4.076537\n",
      "Average loss at step 348000: 4.077428\n",
      "Average loss at step 350000: 4.077570\n",
      "Average loss at step 352000: 4.075444\n",
      "Average loss at step 354000: 4.075973\n",
      "Average loss at step 356000: 4.076607\n",
      "Average loss at step 358000: 4.076751\n",
      "Average loss at step 360000: 4.076307\n",
      "Average loss at step 362000: 4.076370\n",
      "Average loss at step 364000: 4.075824\n",
      "Average loss at step 366000: 4.075262\n",
      "Average loss at step 368000: 4.077231\n",
      "Average loss at step 370000: 4.072351\n",
      "Average loss at step 372000: 4.075387\n",
      "Average loss at step 374000: 4.074570\n",
      "Average loss at step 376000: 4.075253\n",
      "Average loss at step 378000: 4.076901\n",
      "Average loss at step 380000: 4.073389\n",
      "Average loss at step 382000: 4.073288\n",
      "Average loss at step 384000: 4.077396\n",
      "Average loss at step 386000: 4.074888\n",
      "Average loss at step 388000: 4.074195\n",
      "Average loss at step 390000: 4.073105\n",
      "Average loss at step 392000: 4.076017\n",
      "Average loss at step 394000: 4.074957\n",
      "Average loss at step 396000: 4.074461\n",
      "Average loss at step 398000: 4.076359\n",
      "Average loss at step 400000: 4.072406\n",
      "Average loss at step 402000: 4.073542\n",
      "Average loss at step 404000: 4.075050\n",
      "Average loss at step 406000: 4.073648\n",
      "Average loss at step 408000: 4.074417\n",
      "Average loss at step 410000: 4.074977\n",
      "Average loss at step 412000: 4.073384\n",
      "Average loss at step 414000: 4.075223\n",
      "Average loss at step 416000: 4.073705\n",
      "Average loss at step 418000: 4.076130\n",
      "Average loss at step 420000: 4.075693\n",
      "Average loss at step 422000: 4.076832\n",
      "Average loss at step 424000: 4.073058\n",
      "Average loss at step 426000: 4.073248\n",
      "Average loss at step 428000: 4.072339\n",
      "Average loss at step 430000: 4.073945\n",
      "Average loss at step 432000: 4.075247\n",
      "Average loss at step 434000: 4.074977\n",
      "Average loss at step 436000: 4.074022\n",
      "Average loss at step 438000: 4.072997\n",
      "Average loss at step 440000: 4.072462\n",
      "Average loss at step 442000: 4.070846\n",
      "Average loss at step 444000: 4.071242\n",
      "Average loss at step 446000: 4.072991\n",
      "Average loss at step 448000: 4.071429\n",
      "Average loss at step 450000: 4.071352\n",
      "Average loss at step 452000: 4.072256\n",
      "Average loss at step 454000: 4.070843\n",
      "Average loss at step 456000: 4.074433\n",
      "Average loss at step 458000: 4.072193\n",
      "Average loss at step 460000: 4.072614\n",
      "Average loss at step 462000: 4.073456\n",
      "Average loss at step 464000: 4.072625\n",
      "Average loss at step 466000: 4.070958\n",
      "Average loss at step 468000: 4.072510\n",
      "Average loss at step 470000: 4.071141\n",
      "Average loss at step 472000: 4.071091\n",
      "Average loss at step 474000: 4.071969\n",
      "Average loss at step 476000: 4.070182\n",
      "Average loss at step 478000: 4.072543\n",
      "Average loss at step 480000: 4.072647\n",
      "Average loss at step 482000: 4.073908\n",
      "Average loss at step 484000: 4.069617\n",
      "Average loss at step 486000: 4.073395\n",
      "Average loss at step 488000: 4.071008\n",
      "Average loss at step 490000: 4.073443\n",
      "Average loss at step 492000: 4.070374\n",
      "Average loss at step 494000: 4.072254\n",
      "Average loss at step 496000: 4.072280\n",
      "Average loss at step 498000: 4.070691\n",
      "Average loss at step 500000: 4.073429\n",
      "Average loss at step 502000: 4.069172\n",
      "Average loss at step 504000: 4.071274\n",
      "Average loss at step 506000: 4.071408\n",
      "Average loss at step 508000: 4.071987\n",
      "Average loss at step 510000: 4.071111\n",
      "Average loss at step 512000: 4.072702\n",
      "Average loss at step 514000: 4.071032\n",
      "Average loss at step 516000: 4.072809\n",
      "Average loss at step 518000: 4.070583\n",
      "Average loss at step 520000: 4.070799\n",
      "Average loss at step 522000: 4.070865\n",
      "Average loss at step 524000: 4.072265\n",
      "Average loss at step 526000: 4.071617\n",
      "Average loss at step 528000: 4.071184\n",
      "Average loss at step 530000: 4.072520\n",
      "Average loss at step 532000: 4.068756\n",
      "Average loss at step 534000: 4.071126\n",
      "Average loss at step 536000: 4.070977\n",
      "Average loss at step 538000: 4.071324\n",
      "Average loss at step 540000: 4.071767\n",
      "Average loss at step 542000: 4.072566\n",
      "Average loss at step 544000: 4.072754\n",
      "Average loss at step 546000: 4.069029\n",
      "Average loss at step 548000: 4.072456\n",
      "Average loss at step 550000: 4.070154\n",
      "Average loss at step 552000: 4.070820\n",
      "Average loss at step 554000: 4.068794\n",
      "Average loss at step 556000: 4.074317\n",
      "Average loss at step 558000: 4.069119\n",
      "Average loss at step 560000: 4.071922\n",
      "Average loss at step 562000: 4.069941\n",
      "Average loss at step 564000: 4.071458\n",
      "Average loss at step 566000: 4.070719\n",
      "Average loss at step 568000: 4.069884\n",
      "Average loss at step 570000: 4.070775\n",
      "Average loss at step 572000: 4.069209\n",
      "Average loss at step 574000: 4.070830\n",
      "Average loss at step 576000: 4.072272\n",
      "Average loss at step 578000: 4.068206\n",
      "Average loss at step 580000: 4.069160\n",
      "Average loss at step 582000: 4.072025\n",
      "Average loss at step 584000: 4.069567\n",
      "Average loss at step 586000: 4.069150\n",
      "Average loss at step 588000: 4.070686\n",
      "Average loss at step 590000: 4.071623\n",
      "Average loss at step 592000: 4.068576\n",
      "Average loss at step 594000: 4.072981\n",
      "Average loss at step 596000: 4.072430\n",
      "Average loss at step 598000: 4.069550\n",
      "Average loss at step 600000: 4.069974\n",
      "Average loss at step 602000: 4.066810\n",
      "Average loss at step 604000: 4.072838\n",
      "Average loss at step 606000: 4.070552\n",
      "Average loss at step 608000: 4.069782\n",
      "Average loss at step 610000: 4.070515\n",
      "Average loss at step 612000: 4.069169\n",
      "Average loss at step 614000: 4.068960\n",
      "Average loss at step 616000: 4.070677\n",
      "Average loss at step 618000: 4.071761\n",
      "Average loss at step 620000: 4.068829\n",
      "Average loss at step 622000: 4.069198\n",
      "Average loss at step 624000: 4.070968\n",
      "Average loss at step 626000: 4.070433\n",
      "Average loss at step 628000: 4.070354\n",
      "Average loss at step 630000: 4.068862\n",
      "Average loss at step 632000: 4.070143\n",
      "Average loss at step 634000: 4.070716\n",
      "Average loss at step 636000: 4.071401\n",
      "Average loss at step 638000: 4.070663\n",
      "Average loss at step 640000: 4.069386\n",
      "Average loss at step 642000: 4.070816\n",
      "Average loss at step 644000: 4.066842\n",
      "Average loss at step 646000: 4.069055\n",
      "Average loss at step 648000: 4.069075\n",
      "Average loss at step 650000: 4.067879\n",
      "Average loss at step 652000: 4.068728\n",
      "Average loss at step 654000: 4.072035\n",
      "Average loss at step 656000: 4.071055\n",
      "Average loss at step 658000: 4.069639\n",
      "Average loss at step 660000: 4.066774\n",
      "Average loss at step 662000: 4.067867\n",
      "Average loss at step 664000: 4.068253\n",
      "Average loss at step 666000: 4.068049\n",
      "Average loss at step 668000: 4.069745\n",
      "Average loss at step 670000: 4.067586\n",
      "Average loss at step 672000: 4.067116\n",
      "Average loss at step 674000: 4.068035\n",
      "Average loss at step 676000: 4.067884\n",
      "Average loss at step 678000: 4.068785\n",
      "Average loss at step 680000: 4.070756\n",
      "Average loss at step 682000: 4.068508\n",
      "Average loss at step 684000: 4.068909\n",
      "Average loss at step 686000: 4.069472\n",
      "Average loss at step 688000: 4.066991\n",
      "Average loss at step 690000: 4.068533\n",
      "Average loss at step 692000: 4.067553\n",
      "Average loss at step 694000: 4.065833\n",
      "Average loss at step 696000: 4.068554\n",
      "Average loss at step 698000: 4.068217\n",
      "Average loss at step 700000: 4.069241\n",
      "Average loss at step 702000: 4.066326\n",
      "Average loss at step 704000: 4.067588\n",
      "Average loss at step 706000: 4.068157\n",
      "Average loss at step 708000: 4.068605\n",
      "Average loss at step 710000: 4.069207\n",
      "Average loss at step 712000: 4.069040\n",
      "Average loss at step 714000: 4.068670\n",
      "Average loss at step 716000: 4.069524\n",
      "Average loss at step 718000: 4.067312\n",
      "Average loss at step 720000: 4.066584\n",
      "Average loss at step 722000: 4.070641\n",
      "Average loss at step 724000: 4.066255\n",
      "Average loss at step 726000: 4.067327\n",
      "Average loss at step 728000: 4.070131\n",
      "Average loss at step 730000: 4.068723\n",
      "Average loss at step 732000: 4.068737\n",
      "Average loss at step 734000: 4.069665\n",
      "Average loss at step 736000: 4.066691\n",
      "Average loss at step 738000: 4.069024\n",
      "Average loss at step 740000: 4.068738\n",
      "Average loss at step 742000: 4.066666\n",
      "Average loss at step 744000: 4.067985\n",
      "Average loss at step 746000: 4.070990\n",
      "Average loss at step 748000: 4.068057\n",
      "Average loss at step 750000: 4.067595\n",
      "Average loss at step 752000: 4.067992\n",
      "Average loss at step 754000: 4.069293\n",
      "Average loss at step 756000: 4.067942\n",
      "Average loss at step 758000: 4.066048\n",
      "Average loss at step 760000: 4.068028\n",
      "Average loss at step 762000: 4.069897\n",
      "Average loss at step 764000: 4.066814\n",
      "Average loss at step 766000: 4.064593\n",
      "Average loss at step 768000: 4.068781\n",
      "Average loss at step 770000: 4.067859\n",
      "Average loss at step 772000: 4.068516\n",
      "Average loss at step 774000: 4.067874\n",
      "Average loss at step 776000: 4.069168\n",
      "Average loss at step 778000: 4.066024\n",
      "Average loss at step 780000: 4.066123\n",
      "Average loss at step 782000: 4.065260\n",
      "Average loss at step 784000: 4.066138\n",
      "Average loss at step 786000: 4.067496\n",
      "Average loss at step 788000: 4.068614\n",
      "Average loss at step 790000: 4.064711\n",
      "Average loss at step 792000: 4.065029\n",
      "Average loss at step 794000: 4.063680\n",
      "Average loss at step 796000: 4.068522\n",
      "Average loss at step 798000: 4.069067\n",
      "Average loss at step 800000: 4.066067\n",
      "Average loss at step 802000: 4.066514\n",
      "Average loss at step 804000: 4.067806\n",
      "Average loss at step 806000: 4.067202\n",
      "Average loss at step 808000: 4.064500\n",
      "Average loss at step 810000: 4.066413\n",
      "Average loss at step 812000: 4.064682\n",
      "Average loss at step 814000: 4.066015\n",
      "Average loss at step 816000: 4.064466\n",
      "Average loss at step 818000: 4.068512\n",
      "Average loss at step 820000: 4.065560\n",
      "Average loss at step 822000: 4.068042\n",
      "Average loss at step 824000: 4.067254\n",
      "Average loss at step 826000: 4.066899\n",
      "Average loss at step 828000: 4.065381\n",
      "Average loss at step 830000: 4.068648\n",
      "Average loss at step 832000: 4.068515\n",
      "Average loss at step 834000: 4.066092\n",
      "Average loss at step 836000: 4.066137\n",
      "Average loss at step 838000: 4.066601\n",
      "Average loss at step 840000: 4.065238\n",
      "Average loss at step 842000: 4.066498\n",
      "Average loss at step 844000: 4.066736\n",
      "Average loss at step 846000: 4.065969\n",
      "Average loss at step 848000: 4.065566\n",
      "Average loss at step 850000: 4.065695\n",
      "Average loss at step 852000: 4.065491\n",
      "Average loss at step 854000: 4.065683\n",
      "Average loss at step 856000: 4.063000\n",
      "Average loss at step 858000: 4.065839\n",
      "Average loss at step 860000: 4.064615\n",
      "Average loss at step 862000: 4.062692\n",
      "Average loss at step 864000: 4.065543\n",
      "Average loss at step 866000: 4.063983\n",
      "Average loss at step 868000: 4.066431\n",
      "Average loss at step 870000: 4.065977\n",
      "Average loss at step 872000: 4.068024\n",
      "Average loss at step 874000: 4.063937\n",
      "Average loss at step 876000: 4.066620\n",
      "Average loss at step 878000: 4.065485\n",
      "Average loss at step 880000: 4.066304\n",
      "Average loss at step 882000: 4.064877\n",
      "Average loss at step 884000: 4.064191\n",
      "Average loss at step 886000: 4.063062\n",
      "Average loss at step 888000: 4.063355\n",
      "Average loss at step 890000: 4.063370\n",
      "Average loss at step 892000: 4.066015\n",
      "Average loss at step 894000: 4.064101\n",
      "Average loss at step 896000: 4.063409\n",
      "Average loss at step 898000: 4.064680\n",
      "Average loss at step 900000: 4.066160\n",
      "Average loss at step 902000: 4.065493\n",
      "Average loss at step 904000: 4.065046\n",
      "Average loss at step 906000: 4.064777\n",
      "Average loss at step 908000: 4.066281\n",
      "Average loss at step 910000: 4.063110\n",
      "Average loss at step 912000: 4.066827\n",
      "Average loss at step 914000: 4.066222\n",
      "Average loss at step 916000: 4.063491\n",
      "Average loss at step 918000: 4.063733\n",
      "Average loss at step 920000: 4.064170\n",
      "Average loss at step 922000: 4.066407\n",
      "Average loss at step 924000: 4.064174\n",
      "Average loss at step 926000: 4.065217\n",
      "Average loss at step 928000: 4.066806\n",
      "Average loss at step 930000: 4.061483\n",
      "Average loss at step 932000: 4.064292\n",
      "Average loss at step 934000: 4.064527\n",
      "Average loss at step 936000: 4.065852\n",
      "Average loss at step 938000: 4.064997\n",
      "Average loss at step 940000: 4.063426\n",
      "Average loss at step 942000: 4.065927\n",
      "Average loss at step 944000: 4.064854\n",
      "Average loss at step 946000: 4.065318\n",
      "Average loss at step 948000: 4.063951\n",
      "Average loss at step 950000: 4.063801\n",
      "Average loss at step 952000: 4.065897\n",
      "Average loss at step 954000: 4.064425\n",
      "Average loss at step 956000: 4.067209\n",
      "Average loss at step 958000: 4.062769\n",
      "Average loss at step 960000: 4.063993\n",
      "Average loss at step 962000: 4.063448\n",
      "Average loss at step 964000: 4.063497\n",
      "Average loss at step 966000: 4.065966\n",
      "Average loss at step 968000: 4.063828\n",
      "Average loss at step 970000: 4.061931\n",
      "Average loss at step 972000: 4.065016\n",
      "Average loss at step 974000: 4.064748\n",
      "Average loss at step 976000: 4.063849\n",
      "Average loss at step 978000: 4.062184\n",
      "Average loss at step 980000: 4.063979\n",
      "Average loss at step 982000: 4.063462\n",
      "Average loss at step 984000: 4.062968\n",
      "Average loss at step 986000: 4.063483\n",
      "Average loss at step 988000: 4.063814\n",
      "Average loss at step 990000: 4.064065\n",
      "Average loss at step 992000: 4.063475\n",
      "Average loss at step 994000: 4.062540\n",
      "Average loss at step 996000: 4.061723\n",
      "Average loss at step 998000: 4.064760\n",
      "Average loss at step 1000000: 4.063618\n",
      "Average loss at step 1002000: 4.066073\n",
      "Average loss at step 1004000: 4.062543\n",
      "Average loss at step 1006000: 4.064771\n",
      "Average loss at step 1008000: 4.066597\n",
      "Average loss at step 1010000: 4.062317\n",
      "Average loss at step 1012000: 4.066510\n",
      "Average loss at step 1014000: 4.062592\n",
      "Average loss at step 1016000: 4.065557\n",
      "Average loss at step 1018000: 4.065046\n",
      "Average loss at step 1020000: 4.062669\n",
      "Average loss at step 1022000: 4.067155\n",
      "Average loss at step 1024000: 4.063595\n",
      "Average loss at step 1026000: 4.063569\n",
      "Average loss at step 1028000: 4.063920\n",
      "Average loss at step 1030000: 4.062896\n",
      "Average loss at step 1032000: 4.062280\n",
      "Average loss at step 1034000: 4.062591\n",
      "Average loss at step 1036000: 4.062644\n",
      "Average loss at step 1038000: 4.063334\n",
      "Average loss at step 1040000: 4.062983\n",
      "Average loss at step 1042000: 4.063119\n",
      "Average loss at step 1044000: 4.064250\n",
      "Average loss at step 1046000: 4.064850\n",
      "Average loss at step 1048000: 4.063982\n",
      "Average loss at step 1050000: 4.063057\n",
      "Average loss at step 1052000: 4.064697\n",
      "Average loss at step 1054000: 4.062403\n",
      "Average loss at step 1056000: 4.062178\n",
      "Average loss at step 1058000: 4.063485\n",
      "Average loss at step 1060000: 4.063327\n",
      "Average loss at step 1062000: 4.062404\n",
      "Average loss at step 1064000: 4.061297\n",
      "Average loss at step 1066000: 4.062613\n",
      "Average loss at step 1068000: 4.063322\n",
      "Average loss at step 1070000: 4.064095\n",
      "Average loss at step 1072000: 4.060511\n",
      "Average loss at step 1074000: 4.062517\n",
      "Average loss at step 1076000: 4.062071\n",
      "Average loss at step 1078000: 4.064196\n",
      "Average loss at step 1080000: 4.063882\n",
      "Average loss at step 1082000: 4.060672\n",
      "Average loss at step 1084000: 4.063203\n",
      "Average loss at step 1086000: 4.061904\n",
      "Average loss at step 1088000: 4.059143\n",
      "Average loss at step 1090000: 4.061899\n",
      "Average loss at step 1092000: 4.061134\n",
      "Average loss at step 1094000: 4.061643\n",
      "Average loss at step 1096000: 4.063217\n",
      "Average loss at step 1098000: 4.062905\n",
      "Average loss at step 1100000: 4.065792\n",
      "Average loss at step 1102000: 4.062307\n",
      "Average loss at step 1104000: 4.062550\n",
      "Average loss at step 1106000: 4.060711\n",
      "Average loss at step 1108000: 4.060299\n",
      "Average loss at step 1110000: 4.063439\n",
      "Average loss at step 1112000: 4.063820\n",
      "Average loss at step 1114000: 4.060674\n",
      "Average loss at step 1116000: 4.059193\n",
      "Average loss at step 1118000: 4.063532\n",
      "Average loss at step 1120000: 4.061745\n",
      "Average loss at step 1122000: 4.061099\n",
      "Average loss at step 1124000: 4.064379\n",
      "Average loss at step 1126000: 4.065436\n",
      "Average loss at step 1128000: 4.061896\n",
      "Average loss at step 1130000: 4.062170\n",
      "Average loss at step 1132000: 4.061120\n",
      "Average loss at step 1134000: 4.062843\n",
      "Average loss at step 1136000: 4.060506\n",
      "Average loss at step 1138000: 4.061648\n",
      "Average loss at step 1140000: 4.063332\n",
      "Average loss at step 1142000: 4.063272\n",
      "Average loss at step 1144000: 4.062830\n",
      "Average loss at step 1146000: 4.061313\n",
      "Average loss at step 1148000: 4.060514\n",
      "Average loss at step 1150000: 4.061280\n",
      "Average loss at step 1152000: 4.062979\n",
      "Average loss at step 1154000: 4.063498\n",
      "Average loss at step 1156000: 4.060598\n",
      "Average loss at step 1158000: 4.063669\n",
      "Average loss at step 1160000: 4.061739\n",
      "Average loss at step 1162000: 4.062786\n",
      "Average loss at step 1164000: 4.062613\n",
      "Average loss at step 1166000: 4.063688\n",
      "Average loss at step 1168000: 4.062922\n",
      "Average loss at step 1170000: 4.062492\n",
      "Average loss at step 1172000: 4.062417\n",
      "Average loss at step 1174000: 4.060733\n",
      "Average loss at step 1176000: 4.062175\n",
      "Average loss at step 1178000: 4.060308\n",
      "Average loss at step 1180000: 4.060399\n",
      "Average loss at step 1182000: 4.063759\n",
      "Average loss at step 1184000: 4.059875\n",
      "Average loss at step 1186000: 4.060903\n",
      "Average loss at step 1188000: 4.062628\n",
      "Average loss at step 1190000: 4.060563\n",
      "Average loss at step 1192000: 4.059200\n",
      "Average loss at step 1194000: 4.061327\n",
      "Average loss at step 1196000: 4.060596\n",
      "Average loss at step 1198000: 4.061932\n",
      "Average loss at step 1200000: 4.060934\n",
      "Average loss at step 1202000: 4.062776\n",
      "Average loss at step 1204000: 4.060527\n",
      "Average loss at step 1206000: 4.057252\n",
      "Average loss at step 1208000: 4.060335\n",
      "Average loss at step 1210000: 4.065171\n",
      "Average loss at step 1212000: 4.061446\n",
      "Average loss at step 1214000: 4.060442\n",
      "Average loss at step 1216000: 4.061178\n",
      "Average loss at step 1218000: 4.061524\n",
      "Average loss at step 1220000: 4.060971\n",
      "Average loss at step 1222000: 4.060121\n",
      "Average loss at step 1224000: 4.059024\n",
      "Average loss at step 1226000: 4.060017\n",
      "Average loss at step 1228000: 4.058585\n",
      "Average loss at step 1230000: 4.060210\n",
      "Average loss at step 1232000: 4.062514\n",
      "Average loss at step 1234000: 4.060324\n",
      "Average loss at step 1236000: 4.061570\n",
      "Average loss at step 1238000: 4.060567\n",
      "Average loss at step 1240000: 4.061303\n",
      "Average loss at step 1242000: 4.061168\n",
      "Average loss at step 1244000: 4.063722\n",
      "Average loss at step 1246000: 4.063409\n",
      "Average loss at step 1248000: 4.057722\n",
      "Average loss at step 1250000: 4.061595\n",
      "Average loss at step 1252000: 4.060368\n",
      "Average loss at step 1254000: 4.059887\n",
      "Average loss at step 1256000: 4.062842\n",
      "Average loss at step 1258000: 4.059820\n",
      "Average loss at step 1260000: 4.059180\n",
      "Average loss at step 1262000: 4.059302\n",
      "Average loss at step 1264000: 4.059288\n",
      "Average loss at step 1266000: 4.060332\n",
      "Average loss at step 1268000: 4.056544\n",
      "Average loss at step 1270000: 4.060521\n",
      "Average loss at step 1272000: 4.059193\n",
      "Average loss at step 1274000: 4.059335\n",
      "Average loss at step 1276000: 4.059397\n",
      "Average loss at step 1278000: 4.060071\n",
      "Average loss at step 1280000: 4.059001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-aa8a161ff705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             batch_path_size=batch_path_size, num_skips=num_skips, skip_window=skip_window)\n\u001b[1;32m     10\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1290\u001b[0m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_name_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_name_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     def _run_fn(session, feed_dict, fetch_list, target_list, options,\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_name_list\u001b[0;34m(tensor_list)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0ms\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m   \"\"\"\n\u001b[0;32m--> 515\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/compat.pyc\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \"\"\"\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "            batch_path_size=batch_path_size, num_skips=num_skips, skip_window=skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            embedding_filename = '/hdd2/graph_embedding/customized/results/exp_decay_lr/blog_embeddings_iter%d.txt' %step\n",
    "            not_normal_embeddings = embeddings.eval()\n",
    "            ordered_embeddings = [not_normal_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "            np.savetxt(embedding_filename, ordered_embeddings)\n",
    "#             sim = similarity.eval()\n",
    "#             for i in range(valid_size):\n",
    "#                 valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#                 top_k = 8 # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "#                 log = 'Nearest to %s:' % valid_word\n",
    "#                 for k in range(top_k):\n",
    "#                     close_word = reverse_dictionary[nearest[k]]\n",
    "#                     log = '%s %s,' % (log, close_word)\n",
    "#                 print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    not_normal_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings_1.txt'\n",
    "ordered_embeddings = [final_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "np.savetxt(embedding_filename, ordered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58.47768402, -14.06970406, -11.84331131,   3.77727461,\n",
       "        27.18597221, -25.0004673 ,  65.72541809, -65.32319641,\n",
       "        46.84730148, -17.94046402,  -9.88634205,   4.96879911,\n",
       "       -39.52791977, -41.69575119,  74.56922913,   7.61631155,\n",
       "        23.03376389,  50.97651291, -33.59208679,  31.2266407 ,\n",
       "        81.93914032,  15.72993279,  -5.28370953,  17.18107605,\n",
       "       -75.29068756,  14.2514925 ,   7.94441748,  -4.89663982,\n",
       "       -51.30142212, -33.08482361, -32.42542267,   1.93974102,\n",
       "        46.80765533,   8.50759125,  10.44501019, -40.15472794,\n",
       "        -5.02595329, -36.50287247, -45.88523102, -79.72190094,\n",
       "       -17.26217461,   8.80545712, -52.35463333, -16.01053047,\n",
       "       -43.30438232, -39.79665756, -16.72794151,  34.34871674,\n",
       "        57.16449738,  -4.06061411, -72.57341003,  -7.9510541 ,\n",
       "        63.76834869,  43.05852509,   2.076653  , -98.20155334,\n",
       "        16.07836723,  46.57059479,  68.2883606 ,  -1.08950758,\n",
       "         5.00839376,   9.26826859,   8.00768375,  18.40788651,\n",
       "        57.93119049,  69.93483734, -30.36079407, -24.08821869,\n",
       "       -98.82420349,  56.28701782,  -6.49470615, -29.10713005,\n",
       "         9.16787815,  29.59499741,  82.98940277,  83.68479919,\n",
       "       -54.79275894,  25.96276855,  22.00436592, -61.77494812,\n",
       "       -33.84917831, -51.7205925 , -47.96949768, -18.86766052,\n",
       "       -10.83108044,   3.68754077,  65.11179352,  28.03749084,\n",
       "       -43.74489975,  14.33013916,  11.00716972,  48.05020142,\n",
       "       -17.2221508 ,  46.67128754,   9.17617607, -66.37744141,\n",
       "        28.96920204, -23.5886116 ,  48.79130173,   3.59684467,\n",
       "       -10.80480194,  -7.6785965 ,  23.84983635, -46.33642578,\n",
       "       -20.64024162,   0.37871927,  22.73223305,  52.55372238,\n",
       "        10.19163036, -11.5577383 ,  -4.63438082,   3.70495296,\n",
       "         7.59680653,  27.99442673,  29.33081055, -51.17831421,\n",
       "        59.33683395,  11.71917534,   9.37813663, -17.67382622,\n",
       "       -19.4407177 , -19.11290359, -16.30101204, -26.50016785,\n",
       "        37.48277664,   3.48921919,  24.05833054,  27.21180725], dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_embeddings[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(embedding_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58.47768402, -14.06970406, -11.84331131,   3.77727461,\n",
       "        27.18597221, -25.0004673 ,  65.72541809, -65.32319641,\n",
       "        46.84730148, -17.94046402,  -9.88634205,   4.96879911,\n",
       "       -39.52791977, -41.69575119,  74.56922913,   7.61631155,\n",
       "        23.03376389,  50.97651291, -33.59208679,  31.2266407 ,\n",
       "        81.93914032,  15.72993279,  -5.28370953,  17.18107605,\n",
       "       -75.29068756,  14.2514925 ,   7.94441748,  -4.89663982,\n",
       "       -51.30142212, -33.08482361, -32.42542267,   1.93974102,\n",
       "        46.80765533,   8.50759125,  10.44501019, -40.15472794,\n",
       "        -5.02595329, -36.50287247, -45.88523102, -79.72190094,\n",
       "       -17.26217461,   8.80545712, -52.35463333, -16.01053047,\n",
       "       -43.30438232, -39.79665756, -16.72794151,  34.34871674,\n",
       "        57.16449738,  -4.06061411, -72.57341003,  -7.9510541 ,\n",
       "        63.76834869,  43.05852509,   2.076653  , -98.20155334,\n",
       "        16.07836723,  46.57059479,  68.2883606 ,  -1.08950758,\n",
       "         5.00839376,   9.26826859,   8.00768375,  18.40788651,\n",
       "        57.93119049,  69.93483734, -30.36079407, -24.08821869,\n",
       "       -98.82420349,  56.28701782,  -6.49470615, -29.10713005,\n",
       "         9.16787815,  29.59499741,  82.98940277,  83.68479919,\n",
       "       -54.79275894,  25.96276855,  22.00436592, -61.77494812,\n",
       "       -33.84917831, -51.7205925 , -47.96949768, -18.86766052,\n",
       "       -10.83108044,   3.68754077,  65.11179352,  28.03749084,\n",
       "       -43.74489975,  14.33013916,  11.00716972,  48.05020142,\n",
       "       -17.2221508 ,  46.67128754,   9.17617607, -66.37744141,\n",
       "        28.96920204, -23.5886116 ,  48.79130173,   3.59684467,\n",
       "       -10.80480194,  -7.6785965 ,  23.84983635, -46.33642578,\n",
       "       -20.64024162,   0.37871927,  22.73223305,  52.55372238,\n",
       "        10.19163036, -11.5577383 ,  -4.63438082,   3.70495296,\n",
       "         7.59680653,  27.99442673,  29.33081055, -51.17831421,\n",
       "        59.33683395,  11.71917534,   9.37813663, -17.67382622,\n",
       "       -19.4407177 , -19.11290359, -16.30101204, -26.50016785,\n",
       "        37.48277664,   3.48921919,  24.05833054,  27.21180725])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 10308, 10309, 10310])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00, ...,\n",
       "         1.03090000e+04,   1.03100000e+04,   1.03110000e+04])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10312"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10311"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocabulary_size)\n",
    "couples, labels = skipgrams(data, vocabulary_size, window_size=10, sampling_table=sampling_table, negative_samples=5)\n",
    "# word_target, word_context = zip(*couples)\n",
    "# word_target = np.array(word_target, dtype=\"int32\")\n",
    "# word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "# print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824960\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in walks_corpus:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = Skipgram(sentences=walks_corpus, vocabulary_counts=vocabulary_size, size=128,\n",
    "#                  window=10, min_count=0, trim_rule=None, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "#     embeddings = np.loadtxt(embeddings_file)\n",
    "    \n",
    "    ## for original deepwalk\n",
    "    #model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    \n",
    "    ## for external word2vec lib\n",
    "    model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "#     features_matrix = embeddings\n",
    "\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "\n",
    "    # use other word2vec lib\n",
    "    features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8496e862a2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_filename_other_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-e951e3ee7ec9>\u001b[0m in \u001b[0;36mscoring\u001b[0;34m(emb_filename, matfile)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m## for external word2vec lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 2. Load labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "embedding_filename_other_lib = '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\n",
    "\n",
    "scoring('', matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "model = KeyedVectors.load_word2vec_format(embedding_filename_original, binary=False)\n",
    "                                          \n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embeddings = np.loadtxt(embedding_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.110529  ,  0.19354101,  0.3969    ,  0.16305301, -0.198276  ,\n",
       "       -0.219273  ,  0.045498  ,  0.138824  , -0.174495  ,  0.20618799,\n",
       "        0.48984301,  0.302856  , -0.46013999,  0.227097  ,  0.071323  ,\n",
       "       -0.43010399,  0.015921  , -0.021136  ,  0.28175899, -0.26225701,\n",
       "        0.102745  , -0.31963301,  0.058964  ,  0.132149  ,  0.176946  ,\n",
       "       -0.155433  ,  0.160973  , -0.282166  ,  0.030017  , -0.165079  ,\n",
       "        0.25168899, -0.52189702,  0.005448  ,  0.18592501, -0.013992  ,\n",
       "       -0.070675  ,  0.033961  ,  0.117675  , -0.073083  ,  0.068748  ,\n",
       "       -0.10755   ,  0.068676  ,  0.17162   , -0.136898  ,  0.17979699,\n",
       "       -0.106551  , -0.212037  ,  0.103523  , -0.242975  , -0.46731299,\n",
       "        0.182107  ,  0.092075  , -0.141946  ,  0.051342  ,  0.31653801,\n",
       "        0.28085399, -0.029812  ,  0.19881   ,  0.31846499, -0.12293   ,\n",
       "        0.22415499, -0.21315201, -0.220193  , -0.15813901,  0.104372  ,\n",
       "       -0.038969  ,  0.020534  , -0.18385699, -0.049377  ,  0.241363  ,\n",
       "        0.15504301,  0.27251899, -0.20202599,  0.074011  ,  0.248133  ,\n",
       "       -0.015073  ,  0.046193  , -0.124894  , -0.06814   , -0.128296  ,\n",
       "        0.020906  , -0.07755   , -0.172672  ,  0.032093  ,  0.156846  ,\n",
       "       -0.019415  ,  0.051345  , -0.13377801, -0.195548  ,  0.128029  ,\n",
       "        0.066909  ,  0.13171799, -0.214425  ,  0.097262  , -0.20645601,\n",
       "       -0.114667  , -0.193297  , -0.218493  , -0.14356001, -0.06486   ,\n",
       "        0.078362  ,  0.043282  , -0.195365  ,  0.035029  , -0.100952  ,\n",
       "        0.028167  , -0.254026  ,  0.080769  ,  0.138097  , -0.118832  ,\n",
       "        0.22276799, -0.099504  ,  0.045412  ,  0.141414  ,  0.123148  ,\n",
       "        0.13963801, -0.004271  ,  0.172041  ,  0.12623601,  0.24216899,\n",
       "       -0.219863  ,  0.27274099,  0.026154  ,  0.10464   , -0.023742  ,\n",
       "       -0.206168  , -0.33293399, -0.034799  ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.47322476e-01,   2.67052501e-02,   1.52672976e-02,\n",
       "        -1.37635857e-01,  -6.47586957e-02,  -2.46349778e-02,\n",
       "        -8.18009302e-02,   6.91710338e-02,  -9.66271833e-02,\n",
       "        -9.89247933e-02,   5.70955575e-02,   1.53366596e-01,\n",
       "         8.30798447e-02,  -1.44452751e-01,   9.11711424e-04,\n",
       "         1.01996966e-01,   4.97803837e-02,  -4.91250493e-02,\n",
       "         1.03872299e-01,  -1.11238003e-01,   3.35200056e-02,\n",
       "         6.37169033e-02,  -3.92291024e-02,  -2.92820130e-02,\n",
       "        -2.85616820e-03,  -3.53677608e-02,  -5.77375665e-02,\n",
       "        -3.55093554e-02,   4.43368331e-02,  -1.63600937e-01,\n",
       "        -5.06901741e-02,   1.09737791e-01,  -6.03695633e-03,\n",
       "        -9.09792027e-04,  -9.44678709e-02,  -5.35581484e-02,\n",
       "         5.67527264e-02,  -2.60556079e-02,   1.89456977e-02,\n",
       "         2.38609854e-02,   4.84210551e-02,   1.34210438e-01,\n",
       "        -1.30895942e-01,   6.51589632e-02,   6.92558140e-02,\n",
       "        -5.88307390e-03,  -1.87402926e-02,   1.52439341e-01,\n",
       "        -7.65966475e-02,  -1.14347152e-02,  -5.93073331e-02,\n",
       "         3.97681519e-02,  -2.26105414e-02,  -1.00840345e-01,\n",
       "        -2.73720361e-02,  -1.35795742e-01,  -7.55931512e-02,\n",
       "         6.89051822e-02,  -9.89682674e-02,   8.19836278e-03,\n",
       "         9.56752300e-02,   1.11364149e-01,  -5.73285446e-02,\n",
       "        -7.95298442e-02,  -6.94709942e-02,   1.97762456e-02,\n",
       "        -7.93395117e-02,   5.48537821e-02,  -7.36792460e-02,\n",
       "         4.13507484e-02,   1.48582190e-01,  -1.37633294e-01,\n",
       "        -1.10649310e-01,   5.96768968e-02,   9.31203440e-02,\n",
       "         1.78780377e-01,  -1.70841515e-01,   9.95152742e-02,\n",
       "         9.34500918e-02,   1.38932645e-01,  -1.15846768e-01,\n",
       "         3.72944437e-02,  -1.21906027e-01,  -1.49959236e-01,\n",
       "        -6.60825595e-02,   8.33349824e-02,   2.63328534e-02,\n",
       "         3.53457741e-02,   1.41411439e-01,  -7.07744882e-02,\n",
       "         1.21387757e-01,   5.68825640e-02,  -1.21080384e-01,\n",
       "         1.42978668e-01,  -1.67644396e-01,   1.68275498e-02,\n",
       "        -9.96617302e-02,   4.47709225e-02,   2.08472624e-01,\n",
       "        -1.08954571e-01,  -8.83026272e-02,  -1.23706110e-01,\n",
       "        -5.51782846e-02,   1.30027205e-01,   5.61708212e-02,\n",
       "         7.12516978e-02,   3.37933712e-02,   8.94594342e-02,\n",
       "         1.06588854e-02,  -2.62339506e-02,   4.57158871e-02,\n",
       "        -2.36465354e-02,   1.83984548e-01,  -5.11433706e-02,\n",
       "         1.30957412e-02,   1.15885787e-01,   1.39660118e-02,\n",
       "         2.78853234e-02,  -1.25376284e-01,  -4.51841056e-02,\n",
       "         2.44418718e-02,  -9.92230922e-02,   7.17640370e-02,\n",
       "        -5.39226495e-02,   9.22740158e-03,   1.02272414e-01,\n",
       "        -8.26397240e-02,   1.59677104e-04])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sio.loadmat(matfile)\n",
    "A = mat['network']\n",
    "labels_matrix = mat['group']\n",
    "labels_count = labels_matrix.shape[1]\n",
    "mlb = MultiLabelBinarizer(range(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MultiLabelBinarizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fd4e0751c0ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MultiLabelBinarizer' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-13 21:07:11,853 : INFO : collecting all words and their counts\n",
      "2018-03-13 21:07:11,855 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-13 21:07:11,857 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-03-13 21:07:11,858 : INFO : Loading a fresh vocabulary\n",
      "2018-03-13 21:07:11,860 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-03-13 21:07:11,863 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-03-13 21:07:11,865 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-03-13 21:07:11,868 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-03-13 21:07:11,870 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-03-13 21:07:11,872 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-03-13 21:07:11,874 : INFO : resetting layer weights\n",
      "2018-03-13 21:07:11,876 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-03-13 21:07:11,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,883 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,892 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,900 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 183 effective words/s\n",
      "2018-03-13 21:07:11,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,907 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,916 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,917 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 25 effective words/s\n",
      "2018-03-13 21:07:11,918 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 5\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00497301], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8\n",
      "Words processed: 17000K     Vocab size: 4399K  \n",
      "Vocab size (unigrams + bigrams): 2419827\n",
      "Words in train file: 17005206\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('/hdd2/graph_embedding/tmp/text8', '/hdd2/graph_embedding/tmp/text8-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8-phrases\n",
      "Vocab size: 98331\n",
      "Words in train file: 15857306\n",
      "Alpha: 0.000002  Progress: 100.04%  Words/thread/sec: 467.25k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('/hdd2/graph_embedding/tmp/text8-phrases', '/hdd2/graph_embedding/tmp/text8.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 7] Argument list too long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-bd9adf4b8805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word2vec.word2vec(data,\n\u001b[1;32m      2\u001b[0m                   \u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.1.bin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                   size=128, verbose=True)\n\u001b[0m",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/word2vec/scripts_interface.pyc\u001b[0m in \u001b[0;36mword2vec\u001b[0;34m(train, output, size, window, sample, hs, negative, threads, iter_, min_count, alpha, debug, binary, cbow, save_vocab, read_vocab, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/word2vec/scripts_interface.pyc\u001b[0m in \u001b[0;36mrun_cmd\u001b[0;34m(command, verbose)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     proc = subprocess.Popen(command, stdout=subprocess.PIPE,\n\u001b[0;32m--> 142\u001b[0;31m                             stderr=subprocess.PIPE)\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 7] Argument list too long"
     ]
    }
   ],
   "source": [
    "word2vec.word2vec(data,\n",
    "                  '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.1.bin', \n",
    "                  size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03300432, -0.07565996, -0.16463131, -0.10145742,  0.00168844,\n",
       "       -0.09007662, -0.03259023, -0.22714335, -0.00282227, -0.05083675,\n",
       "       -0.07032426, -0.15832779, -0.13998848,  0.07574239,  0.01739285,\n",
       "        0.05170207, -0.01704509, -0.04309488,  0.13984987, -0.07098495,\n",
       "       -0.08131668, -0.06108802,  0.03238546,  0.03199002, -0.14047897,\n",
       "        0.1192169 , -0.11983237,  0.17141342, -0.04238614, -0.07682881,\n",
       "       -0.09226497, -0.08641661, -0.05119943,  0.04880232,  0.05202007,\n",
       "        0.11911941,  0.02494194,  0.0460702 , -0.02544389,  0.02227429,\n",
       "       -0.02918422,  0.08963519, -0.01449389,  0.13147458,  0.04316   ,\n",
       "        0.05062373,  0.01030556,  0.14064927,  0.05300813,  0.09054667,\n",
       "        0.16458763,  0.00496632,  0.09853401, -0.09157445,  0.03466847,\n",
       "       -0.192109  , -0.04286476,  0.0337339 ,  0.09414463,  0.02883872,\n",
       "       -0.12935385,  0.05580147,  0.04444493,  0.12305645, -0.06843153,\n",
       "       -0.05650309, -0.06930542, -0.0933516 , -0.08945961,  0.0355445 ,\n",
       "        0.05217047, -0.09187157,  0.0095493 , -0.16591462,  0.02874937,\n",
       "       -0.0271637 , -0.1071742 , -0.07731676,  0.17057821, -0.08997511,\n",
       "       -0.15251309,  0.0335523 ,  0.08237864,  0.05653743,  0.05472478,\n",
       "       -0.16204691,  0.09632239, -0.0988512 , -0.12040844, -0.13067709,\n",
       "       -0.03478862, -0.07479616,  0.07388658,  0.08031294, -0.12074082,\n",
       "       -0.04398282, -0.04163745,  0.09059986,  0.1189674 ,  0.05980834,\n",
       "       -0.03435113, -0.12926504,  0.06568849, -0.04446449, -0.093913  ,\n",
       "       -0.02966127, -0.00469951, -0.15404062,  0.07520716, -0.02498432,\n",
       "        0.12759562, -0.08096215, -0.07254944,  0.04480873, -0.19941515,\n",
       "       -0.0785991 ,  0.00944981, -0.00052774,  0.031948  ,  0.05943104,\n",
       "       -0.03682529,  0.02468612, -0.00828248,  0.08574598, -0.08077673,\n",
       "        0.09135576,  0.03673941,  0.01044743])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
