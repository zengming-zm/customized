{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'citeseer'\n",
    "\n",
    "embedding_size = 50\n",
    "learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 0.01\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    OBJECTS.append(cPickle.load(open(\"/hdd2/graph_embedding/planetoid/data/trans.{}.{}\".format(DATASET, NAMES[i]))))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_iter(iter):\n",
    "    \"\"\"an auxiliary function used for computing the number of iterations given the argument iter.\n",
    "    iter can either be an int or a float.\n",
    "    \"\"\"\n",
    "    if iter >= 1:\n",
    "        return iter\n",
    "    return 1 if random.random() < iter else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38732192  0.12907903  0.46030581]\n",
      " [ 0.05552411  0.5345757   1.12406611]\n",
      " [ 0.40012899  0.12259372 -0.88986874]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters([3,3], [3,4], [4,2])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(str(parameters['l_emd_f_W'].eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_supervised_cost(a_C, a_G):\n",
    "    \"\"\"\n",
    "    Computes the supervised cost\n",
    "    \n",
    "    Arguments:  todo\n",
    "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_supervised -- scalar that you compute using equation 1 above.\n",
    "    \"\"\"\n",
    "    \n",
    "    return J_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_embedding_weight():\n",
    "    \"\"\"\n",
    "    Computes the embedding weights\n",
    "    \n",
    "    Return:\n",
    "    reweight -- vector for each path\n",
    "    \"\"\"\n",
    "    \n",
    "    return reweight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_embedding_cost():\n",
    "    \"\"\"\n",
    "    Computes the embedding cost\n",
    "    \n",
    "    Returns:\n",
    "    J_embedding -- scalar that you compute using equation x above.\n",
    "    \"\"\"\n",
    "    \n",
    "    return J_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, 3703], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, 6], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [None, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [None], name = 'ind')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l_x_hid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f2c5d179dba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_x_hid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_emd_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_sym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_x_hid' is not defined"
     ]
    }
   ],
   "source": [
    "print(l_x_hid.shape)\n",
    "print(l_emd_f.shape)\n",
    "print(ind_sym)\n",
    "print(embeddings)\n",
    "print(softmax_weights)\n",
    "print(x_sym.shape)\n",
    "print(y_sym.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3327), Dimension(50)])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "num_sampled = 64\n",
    "num_ver = max(graph.keys()) + 1\n",
    "vocabulary_size = num_ver\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# with graph.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "\n",
    "\n",
    "\n",
    "# word embedding\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "tf.set_random_seed(1)\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "\n",
    "##\n",
    "\n",
    "l_x_hid = tf.layers.dense(inputs = x_sym, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "if use_feature:\n",
    "    l_emd_z = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    l_f = tf.concat([l_x_hid, l_emd_z], axis = 1)\n",
    "    l_y = tf.layers.dense(inputs = l_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "else:\n",
    "    l_y = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "\n",
    "l_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_y, labels = y_sym))\n",
    "\n",
    "if layer_loss and use_feature:\n",
    "    l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_x_hid, labels = y_sym))\n",
    "    l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_emd_z, labels = y_sym))\n",
    "\n",
    "# if neg_samp == 0:\n",
    "#     pass\n",
    "# else:\n",
    "g_loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, \n",
    "                               inputs = l_emd_f, labels = gy_sym, \n",
    "                               num_sampled = vocabulary_size, \n",
    "                               num_classes = vocabulary_size))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(l_loss)\n",
    "\n",
    "g_optimizer = tf.train.GradientDescentOptimizer(g_learning_rate).minimize(g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_inst():\n",
    "    \"\"\"generator for batches for classification loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    while True:\n",
    "        ind = np.array(np.random.permutation(x.shape[0]), dtype = np.int32)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            j = min(ind.shape[0], i + batch_size)\n",
    "            yield x[ind[i: j]], y[ind[i: j]], ind[i: j]\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label_graph():\n",
    "    \"\"\"generator for batches for label context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    labels, label2inst, not_label = [], dd(list), dd(list)\n",
    "    for i in range(x.shape[0]):\n",
    "        flag = False\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i, j] == 1 and not flag:\n",
    "                labels.append(j)\n",
    "                label2inst[j].append(i)\n",
    "                flag = True\n",
    "            elif y[i, j] == 0:\n",
    "                not_label[j].append(i)\n",
    "\n",
    "    while True:\n",
    "        g, gy = [], []\n",
    "        for _ in range(g_sample_size):\n",
    "            x1 = random.randint(0, x.shape[0] - 1)\n",
    "            label = labels[x1]\n",
    "            if len(label2inst) == 1: continue\n",
    "            x2 = random.choice(label2inst[label])\n",
    "            g.append([x1, x2])\n",
    "            gy.append(1.0)\n",
    "#             for _ in range(neg_samp):\n",
    "#                 g.append([x1, random.choice(not_label[label])])\n",
    "#                 gy.append( - 1.0)\n",
    "        yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_graph():\n",
    "    \"\"\"generator for batches for graph context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_ver = max(graph.keys()) + 1\n",
    "\n",
    "    while True:\n",
    "        ind = np.random.permutation(num_ver)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            g, gy = [], []\n",
    "            j = min(ind.shape[0], i + g_batch_size)\n",
    "            for k in ind[i: j]:\n",
    "                if len(graph[k]) == 0: continue\n",
    "                path = [k]\n",
    "                for _ in range(path_size):\n",
    "                    path.append(random.choice(graph[path[-1]]))\n",
    "                for l in range(len(path)):\n",
    "                    for m in range(l - window_size, l + window_size + 1):\n",
    "                        if m < 0 or m >= len(path): continue\n",
    "                        g.append([path[l], path[m]])\n",
    "                        gy.append(1.0)\n",
    "#                         for _ in range(neg_samp):\n",
    "#                             # if the random number euqals to path[m], the it creates noise!\n",
    "#                             g.append([path[l], random.randint(0, num_ver - 1)])\n",
    "#                             gy.append(- 1.0)\n",
    "            yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def init_train(self, init_iter_label, init_iter_graph):\n",
    "#     \"\"\"pre-training of graph embeddings.\n",
    "#     init_iter_label (int): # iterations for optimizing label context loss.\n",
    "#     init_iter_graph (int): # iterations for optimizing graph context loss.\n",
    "#     \"\"\"\n",
    "#     for i in range(init_iter_label):\n",
    "#         gx, gy = next(label_generator)\n",
    "#         loss = g_fn(gx, gy)\n",
    "#         print 'iter label', i, loss\n",
    "\n",
    "#     for i in range(init_iter_graph):\n",
    "#         gx, gy = next(graph_generator)\n",
    "#         loss = g_fn(gx, gy)\n",
    "#         print 'iter graph', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 1\n",
    "iter_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_iter_label\n",
      "iter label 0 8.11048\n",
      "iter label 1 8.10993\n",
      "iter label 2 8.10979\n",
      "iter label 3 8.11004\n",
      "iter label 4 8.10954\n",
      "iter label 5 8.10979\n",
      "iter label 6 8.10982\n",
      "iter label 7 8.10936\n",
      "iter label 8 8.11002\n",
      "iter label 9 8.10949\n",
      "iter label 10 8.10906\n",
      "iter label 11 8.10903\n",
      "iter label 12 8.10915\n",
      "iter label 13 8.1092\n",
      "iter label 14 8.10932\n",
      "iter label 15 8.10876\n",
      "iter label 16 8.10875\n",
      "iter label 17 8.10892\n",
      "iter label 18 8.10908\n",
      "iter label 19 8.10927\n",
      "iter label 20 8.10829\n",
      "iter label 21 8.10822\n",
      "iter label 22 8.10879\n",
      "iter label 23 8.10806\n",
      "iter label 24 8.10837\n",
      "iter label 25 8.10788\n",
      "iter label 26 8.10801\n",
      "iter label 27 8.10775\n",
      "iter label 28 8.10755\n",
      "iter label 29 8.10763\n",
      "iter label 30 8.10843\n",
      "iter label 31 8.10792\n",
      "iter label 32 8.10797\n",
      "iter label 33 8.10723\n",
      "iter label 34 8.10723\n",
      "iter label 35 8.10681\n",
      "iter label 36 8.10754\n",
      "iter label 37 8.10726\n",
      "iter label 38 8.10682\n",
      "iter label 39 8.10692\n",
      "iter label 40 8.10681\n",
      "iter label 41 8.10724\n",
      "iter label 42 8.10648\n",
      "iter label 43 8.10662\n",
      "iter label 44 8.10655\n",
      "iter label 45 8.10616\n",
      "iter label 46 8.10719\n",
      "iter label 47 8.10679\n",
      "iter label 48 8.1061\n",
      "iter label 49 8.10625\n",
      "iter label 50 8.10607\n",
      "iter label 51 8.1059\n",
      "iter label 52 8.10568\n",
      "iter label 53 8.10567\n",
      "iter label 54 8.10598\n",
      "iter label 55 8.10584\n",
      "iter label 56 8.1056\n",
      "iter label 57 8.10551\n",
      "iter label 58 8.10562\n",
      "iter label 59 8.10552\n",
      "iter label 60 8.10528\n",
      "iter label 61 8.1052\n",
      "iter label 62 8.105\n",
      "iter label 63 8.10477\n",
      "iter label 64 8.10553\n",
      "iter label 65 8.10499\n",
      "iter label 66 8.10464\n",
      "iter label 67 8.10482\n",
      "iter label 68 8.10523\n",
      "iter label 69 8.10475\n",
      "iter label 70 8.10409\n",
      "iter label 71 8.10487\n",
      "iter label 72 8.104\n",
      "iter label 73 8.1041\n",
      "iter label 74 8.10399\n",
      "iter label 75 8.10418\n",
      "iter label 76 8.10364\n",
      "iter label 77 8.10381\n",
      "iter label 78 8.10369\n",
      "iter label 79 8.10333\n",
      "iter label 80 8.10399\n",
      "iter label 81 8.10353\n",
      "iter label 82 8.10389\n",
      "iter label 83 8.10353\n",
      "iter label 84 8.10316\n",
      "iter label 85 8.10309\n",
      "iter label 86 8.10335\n",
      "iter label 87 8.10306\n",
      "iter label 88 8.10286\n",
      "iter label 89 8.10256\n",
      "iter label 90 8.10295\n",
      "iter label 91 8.1028\n",
      "iter label 92 8.10322\n",
      "iter label 93 8.1024\n",
      "iter label 94 8.10303\n",
      "iter label 95 8.10218\n",
      "iter label 96 8.10259\n",
      "iter label 97 8.10234\n",
      "iter label 98 8.1023\n",
      "iter label 99 8.10238\n",
      "iter label 100 8.1019\n",
      "iter label 101 8.10232\n",
      "iter label 102 8.10239\n",
      "iter label 103 8.10211\n",
      "iter label 104 8.10217\n",
      "iter label 105 8.10133\n",
      "iter label 106 8.10165\n",
      "iter label 107 8.10151\n",
      "iter label 108 8.1018\n",
      "iter label 109 8.10119\n",
      "iter label 110 8.10126\n",
      "iter label 111 8.10088\n",
      "iter label 112 8.10093\n",
      "iter label 113 8.10178\n",
      "iter label 114 8.10077\n",
      "iter label 115 8.1004\n",
      "iter label 116 8.10055\n",
      "iter label 117 8.10055\n",
      "iter label 118 8.1006\n",
      "iter label 119 8.1007\n",
      "iter label 120 8.1011\n",
      "iter label 121 8.10108\n",
      "iter label 122 8.10142\n",
      "iter label 123 8.10015\n",
      "iter label 124 8.1002\n",
      "iter label 125 8.09982\n",
      "iter label 126 8.09984\n",
      "iter label 127 8.09981\n",
      "iter label 128 8.09961\n",
      "iter label 129 8.09973\n",
      "iter label 130 8.09942\n",
      "iter label 131 8.09997\n",
      "iter label 132 8.09941\n",
      "iter label 133 8.09945\n",
      "iter label 134 8.09903\n",
      "iter label 135 8.09913\n",
      "iter label 136 8.09985\n",
      "iter label 137 8.0993\n",
      "iter label 138 8.09951\n",
      "iter label 139 8.0996\n",
      "iter label 140 8.09853\n",
      "iter label 141 8.09855\n",
      "iter label 142 8.0984\n",
      "iter label 143 8.09875\n",
      "iter label 144 8.09871\n",
      "iter label 145 8.09817\n",
      "iter label 146 8.09948\n",
      "iter label 147 8.09805\n",
      "iter label 148 8.09778\n",
      "iter label 149 8.09787\n",
      "iter label 150 8.09803\n",
      "iter label 151 8.09801\n",
      "iter label 152 8.09775\n",
      "iter label 153 8.09783\n"
     ]
    }
   ],
   "source": [
    "init_iter_label = 2000\n",
    "init_iter_graph = 70\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()\n",
    "\n",
    "# init_train\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('init_iter_label')\n",
    "    for i in range(init_iter_label):\n",
    "        gx, gy = next(label_generator)\n",
    "#         print(gx)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, l, res_l_emd_f = session.run([g_optimizer, g_loss, softmax_weights], feed_dict=feed_dict)\n",
    "        print 'iter label', i, l\n",
    "#         print(res_l_emd_f)\n",
    "#         sys.exit(0)\n",
    "\n",
    "#     sys.exit(0)\n",
    "    print('init_iter_graph')\n",
    "    for i in range(init_iter_graph):\n",
    "        gx, gy = next(graph_generator)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "        print 'iter graph', i, l\n",
    "        \n",
    "#     print('init_iter_label')\n",
    "#     for i in range(init_iter_label):\n",
    "#         gx, gy = next(label_generator)\n",
    "#         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#         _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#         print 'iter label', i, l\n",
    "\n",
    "    iter_cnt = 0\n",
    "    while True:\n",
    "        for _ in range(max_iter):\n",
    "            for _ in range(comp_iter(iter_graph)):\n",
    "                gx, gy = next(graph_generator)\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "                _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "\n",
    "            for _ in range(comp_iter(iter_inst)):\n",
    "                xs, ys, indexs = next(inst_generator)\n",
    "#                 gx, gy = next(label_generator)\n",
    "                xs = xs.toarray()\n",
    "                feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)}\n",
    "                _, l = session.run([optimizer, l_loss], feed_dict=feed_dict)\n",
    "    #           print 'iter inst', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_label)):\n",
    "                gx, gy = next(label_generator)\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "                _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        predict_y = tf.arg_max(l_y, 1)\n",
    "        correct_prediction = tf.equal(predict_y, tf.arg_max(y_sym, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    #     print(accuracy)\n",
    "#         print('number of iteration: %d', iter_cnt)\n",
    "        iter_cnt += 1\n",
    "        train_accuracy = accuracy.eval({x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)})\n",
    "#         print('Train Accuracy: %.4f', train_accuracy)\n",
    "        txs = tx.toarray()\n",
    "        t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)\n",
    "        test_accuracy = accuracy.eval({x_sym: txs, y_sym: ty, g_sym: t_index, gy_sym: t_index.reshape(t_index.shape[0], 1)})\n",
    "        print('iter: %d, Tst Acc: %.4f, Trn Acc: %.4f' %(iter_cnt, test_accuracy, train_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter label 4 6.26195\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    gx, gy = next(label_generator)\n",
    "    tf.global_variables_initializer().run()\n",
    "    feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "    _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "    print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs, ys, indexs = next(inst_generator)\n",
    "xs = xs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(120, 3703)\n",
      "(120, 6)\n"
     ]
    }
   ],
   "source": [
    "print(type(xs))\n",
    "print(type(ys))\n",
    "print(type(indexs))\n",
    "print(xs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_l_emd_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
