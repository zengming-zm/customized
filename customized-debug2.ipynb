{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "np.set_printoptions(precision=10)\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'pubmed'\n",
    "\n",
    "embedding_size = 50\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 10\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 3\n",
    "path_size = 10\n",
    "\n",
    "g_batch_size = 200\n",
    "g_learning_rate = 1e-2\n",
    "g_sample_size = 100\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load the dataset\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    OBJECTS.append(cPickle.load(open(\"/hdd2/graph_embedding/planetoid/data/trans.{}.{}\".format(DATASET, NAMES[i]))))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_iter(iter):\n",
    "    \"\"\"an auxiliary function used for computing the number of iterations given the argument iter.\n",
    "    iter can either be an int or a float.\n",
    "    \"\"\"\n",
    "    if iter >= 1:\n",
    "        return iter\n",
    "    return 1 if random.random() < iter else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample a collections of paths from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(l_emd_f_W_size, l_x_hid_W_size, l_y_W_size):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]??\n",
    "                        W2 : [2, 2, 8, 16]?\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    l_emd_f_W = tf.Variable(tf.truncated_normal(l_emd_f_W_size,\n",
    "                            stddev=1.0 / math.sqrt(l_emd_f_W_size[1])))\n",
    "    l_emd_f_b = tf.Variable(tf.zeros([l_emd_f_W_size[0]]))\n",
    "    \n",
    "    l_x_hid_W = tf.get_variable('l_x_hid_W', shape = l_x_hid_W_size,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_x_hid_b = tf.get_variable('l_x_hid_b', shape = [l_x_hid_W_size[0], 1],\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_W = tf.get_variable('l_y_W', shape = l_y_W_size,\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    l_y_b = tf.get_variable('l_y_b', shape = [l_y_W_size[0], 1],\n",
    "                           initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed = 1))\n",
    "    \n",
    "    parameters = {'l_emd_f_W': l_emd_f_W,\n",
    "                  'l_emd_f_b': l_emd_f_b,\n",
    "                  'l_x_hid_W': l_x_hid_W,\n",
    "                  'l_x_hid_b': l_x_hid_b,\n",
    "                  'l_y_W': l_y_W,\n",
    "                  'l_y_b': l_y_b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3873219192  0.129079029   0.46030581  ]\n",
      " [ 0.0555241145  0.5345757008  1.1240661144]\n",
      " [ 0.4001289904  0.1225937158 -0.8898686767]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters([3,3], [3,4], [4,2])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(str(parameters['l_emd_f_W'].eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    x_sym = tf.placeholder(tf.float32, shape = [None, x.shape[1]], name = 'x')\n",
    "    y_sym = tf.placeholder(tf.int32, shape = [None, y.shape[1]], name = 'y')\n",
    "    g_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'g')\n",
    "    gy_sym = tf.placeholder(tf.int32, shape = [None, 1], name = 'gy')\n",
    "    ind_sym = tf.placeholder(tf.int32, shape = [None], name = 'ind')\n",
    "    \n",
    "    path_sym = tf.placeholder(tf.int32, shape = [200, path_size+1], name = 'path')\n",
    "    path_id_sym = tf.placeholder(tf.int32, shape = [None, ], name = 'path_id')\n",
    "    w_path2pair_sym = tf.placeholder(tf.float32, shape = [13000, 200], name = 'w_path2pair_sym')\n",
    "    \n",
    "    \n",
    "    return x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_sym, y_sym, g_sym, gy_sym, ind_sym = create_placeholders()\n",
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# softmax_weights = tf.Variab`le(\n",
    "#     tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "#                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "# softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "# l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_sampled = 64\n",
    "num_ver = max(graph.keys()) + 1\n",
    "vocabulary_size = num_ver\n",
    "n_hidden = 32\n",
    "n_steps = path_size+1 #path length\n",
    "n_input = 50 # dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weight2, _bias2):\n",
    "# #     _X = tf.transpose(_X, [1, 0, 2]) \n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     print('after: tf.reshape(_X, [-1, n_input]) ')\n",
    "#     print(_X.shape)\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    keep_prob = tf.constant(.8)\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple=True)\n",
    "    seqlen = np.ones(batch_size) * (path_size+1)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "#     print(rnn_outputs[:,-1,:].shape)\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "#     print(outputs)\n",
    "    \n",
    "#     lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "#     lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "#     lstm_cells = lstm_cell_1\n",
    "    # Get LSTM cell output\n",
    "#     outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    \n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = rnn_outputs[:,-1,:]\n",
    "#     scale_output = tf.layers.dense(inputs=lstm_last_output, units=1, activation=tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(lstm_last_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, rnn_outputs[:,-1,:], lstm_last_output\n",
    "#     return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dnn_paths(_X, _weight2, _bias2):\n",
    "# #     _X = tf.transpose(_X, [1, 0, 2]) \n",
    "#     _X = tf.reshape(_X, [-1, n_input]) \n",
    "#     print('after: tf.reshape(_X, [-1, n_input]) ')\n",
    "#     print(_X.shape)\n",
    "#     _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "#     _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    path_dnn_output = tf.layers.dense(inputs = _X, units = 1, \n",
    "                    activation = tf.nn.softmax, \n",
    "                    kernel_initializer=glorot_uniform_initializer())\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_dnn_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, rnn_outputs[:,-1,:], lstm_last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average_Paths(_X, _weight2, _bias2):\n",
    "    path_avg_output = tf.reduce_mean(_X, axis=1)\n",
    "    \n",
    "    scale_output = tf.nn.softmax(tf.matmul(_weight2, tf.transpose(path_avg_output)) + _bias2)\n",
    "    \n",
    "#     scale_output = tf.nn.softmax(scale_output)\n",
    "\n",
    "#     print('softmax_output.shape:')\n",
    "#     print(softmax_output.shape)\n",
    "    # Linear activation\n",
    "    return scale_output, path_avg_output[-1], path_avg_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build():\n",
    "\"\"\"\n",
    "Builds the model.\n",
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random_normal_initializer(seed = 1)\n",
    "\n",
    "cgraph = tf.Graph()\n",
    "\n",
    "with cgraph.as_default(), tf.device('/gpu:0'):\n",
    "\n",
    "    x_sym, y_sym, g_sym, gy_sym, ind_sym, path_sym, path_id_sym, w_path2pair_sym = create_placeholders()\n",
    "\n",
    "    # word embedding\n",
    "    tf.random_normal_initializer(seed = 1)\n",
    "    tf.set_random_seed(1)\n",
    "    embeddings = tf.Variable(tf.constant(shape=[vocabulary_size, embedding_size], value=0.5))\n",
    "#         tf.random_normal([vocabulary_size, embedding_size], mean = 0.0, stddev = 0.01))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.constant(shape=[vocabulary_size, embedding_size], value=0.5))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    l_emd_f = tf.nn.embedding_lookup(embeddings, g_sym)\n",
    "    ##\n",
    "    \n",
    "    path_weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size], mean=1.0))\n",
    "    }\n",
    "    path_biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([vocabulary_size]))\n",
    "    }\n",
    "    \n",
    "    s_weight_avg = tf.Variable(tf.random_normal([1, embedding_size]))\n",
    "    s_biase_avg = tf.Variable(tf.random_normal([1]))\n",
    "    \n",
    "#     print('path_sym.shape:')\n",
    "#     print(path_sym.shape)\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, path_sym)\n",
    "#     print('rnn_inputs.shape:')\n",
    "#     print(rnn_inputs.shape)\n",
    "    \n",
    "    if (use_reweight):\n",
    "        reweight, cg_outputs, cg_last_output = Average_Paths(\n",
    "            rnn_inputs, s_weight_avg, s_biase_avg)    \n",
    "        reweight = tf.reshape(reweight, [-1, 1])\n",
    "        reweight_id = tf.matmul(w_path2pair_sym, reweight)\n",
    "    else:\n",
    "        reweight = tf.ones(shape=[w_path2pair_sym.shape[0], 1])\n",
    "        reweight_id = tf.ones(shape=[path_sym.shape[0], 1])\n",
    "    \n",
    "\n",
    "    l_x_hid = tf.layers.dense(inputs = x_sym, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    if use_feature:\n",
    "        l_emd_z = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "        l_f = tf.concat([l_x_hid, l_emd_z], axis = 1)\n",
    "        l_y = tf.layers.dense(inputs = l_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "    else:\n",
    "        l_y = tf.layers.dense(inputs = l_emd_f, units = y.shape[1], activation = tf.nn.softmax, kernel_initializer=glorot_uniform_initializer())\n",
    "\n",
    "    l_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_y, labels = y_sym))\n",
    "\n",
    "    if layer_loss and use_feature:\n",
    "        l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_x_hid, labels = y_sym))\n",
    "        l_loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l_emd_z, labels = y_sym))\n",
    "        \n",
    "    # l_rnn = lstm(emb_f, units = xx)\n",
    "    # l_w = tf.layer(l_rnn, units = , activation = softmax)\n",
    "    # g_loss = weighted...\n",
    "\n",
    "    # if neg_samp == 0:\n",
    "    #     pass\n",
    "    # else:\n",
    "#     gw2v_loss = word2vec(l_emd_f, gy_sym, softmax_wecg_last_outputights, softmax_biases)\n",
    "    \n",
    "#     g_loss = tf.reduce_mean(reweight_id *\n",
    "#         tf.nn.sampled_softmax_loss(weights=softmax_weights, biases = softmax_biases,\n",
    "#                                    inputs = l_emd_f, labels = gy_sym, \n",
    "#                                    num_sampled = vocabulary_size, \n",
    "#                                    num_classes = vocabulary_size))\n",
    "#     yhat = tf.nn.softmax(tf.matmul(l_emd_f, tf.transpose(softmax_weights)) + softmax_biases)\n",
    "#     train_one_hot = tf.one_hot(gy_sym, vocabulary_size)\n",
    "#     train_one_hot = tf.reshape(train_one_hot, [-1, vocabulary_size])\n",
    "#     g_loss = tf.reduce_mean(reweight_id * tf.nn.softmax_cross_entropy_with_logits(logits=yhat, labels=train_one_hot)) \n",
    "                            \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(l_loss)\n",
    "    \n",
    "#     g_optimizer = tf.train.GradientDescentOptimizer(g_learning_rate).minimize(g_loss)\n",
    "    \n",
    "#     gl_loss = tf.reduce_mean(\n",
    "#         tf.nn.sampled_softmax_loss(weights=softmax_weights, biases = softmax_biases,\n",
    "#                                    inputs = l_emd_f, labels = gy_sym,\n",
    "#                                    num_sampled = vocabulary_size, \n",
    "#                                    num_classes = vocabulary_size))\n",
    "    \n",
    "    yhat_l = tf.nn.softmax(tf.matmul(l_emd_f, tf.transpose(softmax_weights)) + softmax_biases)\n",
    "    train_one_hot_l = tf.one_hot(gy_sym, vocabulary_size)\n",
    "    gl_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yhat_l, labels=train_one_hot_l))\n",
    "    \n",
    "    gl_optimizer = tf.train.GradientDescentOptimizer(gl_learning_rate).minimize(gl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_inst():\n",
    "    \"\"\"generator for batches for classification loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    while True:\n",
    "        ind = np.array(np.random.permutation(x.shape[0]), dtype = np.int32)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            j = min(ind.shape[0], i + batch_size)\n",
    "            yield x[ind[i: j]], y[ind[i: j]], ind[i: j]\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label_graph():\n",
    "    \"\"\"generator for batches for label context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    labels, label2inst, not_label = [], dd(list), dd(list)\n",
    "    for i in range(x.shape[0]):\n",
    "        flag = False\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i, j] == 1 and not flag:\n",
    "                labels.append(j)\n",
    "                label2inst[j].append(i)\n",
    "                flag = True\n",
    "            elif y[i, j] == 0:\n",
    "                not_label[j].append(i)\n",
    "\n",
    "    while True:\n",
    "        g, gy = [], []\n",
    "        for _ in range(g_sample_size):\n",
    "            x1 = random.randint(0, x.shape[0] - 1)\n",
    "            label = labels[x1]\n",
    "            if len(label2inst) == 1: continue\n",
    "            x2 = random.choice(label2inst[label])\n",
    "            g.append([x1, x2])\n",
    "            gy.append(1.0)\n",
    "#             for _ in range(neg_samp):\n",
    "#                 g.append([x1, random.choice(not_label[label])])\n",
    "#                 gy.append( - 1.0)\n",
    "        yield np.array(g, dtype = np.int32), np.array(gy, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_graph():\n",
    "    \"\"\"generator for batches for graph context loss.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_ver = max(graph.keys()) + 1\n",
    "\n",
    "    while True:\n",
    "        ind = np.random.permutation(num_ver)\n",
    "        i = 0\n",
    "        while i < ind.shape[0]:\n",
    "            g, gy = [], []\n",
    "            list_path = []\n",
    "            list_path_id = []\n",
    "            count_path = 0\n",
    "            row_w_path2pair = 0\n",
    "            j = min(ind.shape[0], i + g_batch_size)\n",
    "            w_path2pair = np.zeros((13000, g_batch_size))\n",
    "            for k in ind[i: j]:\n",
    "                if len(graph[k]) == 0: continue\n",
    "                path = [k]\n",
    "                for _ in range(path_size):\n",
    "                    path.append(random.choice(graph[path[-1]]))\n",
    "                list_path.append(path)\n",
    "                for l in range(len(path)):\n",
    "                    for m in range(l - window_size, l + window_size + 1):\n",
    "                        if m < 0 or m >= len(path): continue\n",
    "                        g.append([path[l], path[m]])\n",
    "                        gy.append(1.0)\n",
    "                        list_path_id.append(count_path)\n",
    "                        w_path2pair[row_w_path2pair, count_path] = 1\n",
    "                        row_w_path2pair += 1\n",
    "                count_path += 1 \n",
    "#                         for _ in range(neg_samp):\n",
    "#                             # if the random number euqals to path[m], the it creates noise!\n",
    "#                             g.append([path[l], random.randint(0, num_ver - 1)])\n",
    "#                             gy.append(- 1.0)\n",
    "            yield (np.array(g, dtype = np.int32), \n",
    "                   np.array(gy, dtype = np.float32), \n",
    "                   np.array(list_path, np.float32), \n",
    "                   np.array(list_path_id, np.float32),\n",
    "                   w_path2pair)\n",
    "            i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 0\n",
    "iter_graph = 0\n",
    "iter_inst = 2\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_iter_label\n",
      "[[ 8 27]\n",
      " [45 16]\n",
      " [29 25]\n",
      " [39 26]\n",
      " [ 5  3]\n",
      " [50 48]\n",
      " [45  0]\n",
      " [26 24]\n",
      " [13 32]\n",
      " [54  2]\n",
      " [ 1 30]\n",
      " [56 47]\n",
      " [12 14]\n",
      " [ 1  9]\n",
      " [26 17]\n",
      " [13 10]\n",
      " [13 17]\n",
      " [17  3]\n",
      " [50 51]\n",
      " [38 35]\n",
      " [59 57]\n",
      " [ 7 18]\n",
      " [43 37]\n",
      " [56 48]\n",
      " [49 53]\n",
      " [18 31]\n",
      " [52 56]\n",
      " [30 31]\n",
      " [ 2 38]\n",
      " [47 48]\n",
      " [10 19]\n",
      " [42 53]\n",
      " [22 14]\n",
      " [30 41]\n",
      " [31 21]\n",
      " [29  0]\n",
      " [ 2 54]\n",
      " [58 51]\n",
      " [23  8]\n",
      " [30 46]\n",
      " [46 30]\n",
      " [51 38]\n",
      " [30 46]\n",
      " [34 49]\n",
      " [16 30]\n",
      " [57  2]\n",
      " [47 56]\n",
      " [53 54]\n",
      " [48 50]\n",
      " [33 25]\n",
      " [ 3 28]\n",
      " [34 35]\n",
      " [30 29]\n",
      " [21 18]\n",
      " [32 22]\n",
      " [36 29]\n",
      " [ 1  9]\n",
      " [10 20]\n",
      " [51 55]\n",
      " [47 56]\n",
      " [15 56]\n",
      " [40 15]\n",
      " [ 1  0]\n",
      " [45  9]\n",
      " [ 6 22]\n",
      " [20  5]\n",
      " [ 9 30]\n",
      " [10 11]\n",
      " [42 49]\n",
      " [19 17]\n",
      " [ 1 21]\n",
      " [25  7]\n",
      " [ 6 28]\n",
      " [30  9]\n",
      " [36 43]\n",
      " [ 1  0]\n",
      " [ 8 24]\n",
      " [ 9 37]\n",
      " [40 50]\n",
      " [13 39]\n",
      " [47 50]\n",
      " [13 22]\n",
      " [23 20]\n",
      " [19 22]\n",
      " [ 3 11]\n",
      " [58 57]\n",
      " [18 44]\n",
      " [18 45]\n",
      " [44 25]\n",
      " [15  2]\n",
      " [52  2]\n",
      " [49 59]\n",
      " [34 35]\n",
      " [52 59]\n",
      " [42 50]\n",
      " [22 12]\n",
      " [12 23]\n",
      " [25  7]\n",
      " [ 6 23]\n",
      " [17 17]]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "use_reweight = False\n",
    "init_iter_label = 2\n",
    "init_iter_graph = 2\n",
    "iter_inst = 1\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()\n",
    "\n",
    "# init_train\n",
    "max_acc = -1\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(graph = cgraph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('init_iter_label')\n",
    "    for i in range(5):\n",
    "        gx, gy = next(label_generator)\n",
    "        print gx\n",
    "        sys.exit(0)\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "        _, res_gl_loss, res_softmax_weights, res_yhat_l, res_l_emd_f, res_train_one_hot_l = session.run(\n",
    "            [gl_optimizer, gl_loss, softmax_weights, yhat_l, l_emd_f, train_one_hot_l], feed_dict=feed_dict)\n",
    "        print('res_softmax_weights[0]:')\n",
    "        print(res_softmax_weights[0])\n",
    "        print('res_l_emd_f[0]:')\n",
    "        print(res_l_emd_f[0])\n",
    "        print ('res_yhat_l[0]:')\n",
    "        print(res_yhat_l[0])\n",
    "        print('res_train_one_hot_l[0]:')\n",
    "        print(res_train_one_hot_l[0])\n",
    "        print 'iter label, %d, %.20f' %(i, res_gl_loss)\n",
    "    sys.exit(0)\n",
    "#         print(res_l_emd_f)\n",
    "#         sys.exit(0)\n",
    "\n",
    "#     sys.exit(0)\n",
    "    print('init_iter_graph')\n",
    "    for i in range(init_iter_graph):\n",
    "        gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#         print(list_path[0])\n",
    "        feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                   path_sym: list_path, path_id_sym: list_path_id,\n",
    "                   w_path2pair_sym: w_path2pair}\n",
    "        if (use_reweight):\n",
    "            _, l, res_reweight_id, res_reweight, res_rnn_inputs, res_outputs, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, cg_outputs, cg_last_output, s_weight_avg, s_biase_avg], feed_dict=feed_dict)\n",
    "        else:\n",
    "            _, l, res_reweight_id, res_reweight, res_lstm_last_output, res_s_weight, res_s_biase = session.run(\n",
    "                [g_optimizer, g_loss, reweight_id, reweight, rnn_inputs, s_weight_avg, s_biase_avg], feed_dict=feed_dict)\n",
    "\n",
    "#         print(res_rnn_inputs[0])\n",
    "#         print(res_reweight)\n",
    "#         print(res_lstm_last_output[0])\n",
    "        print 'iter graph', i, l\n",
    "#         print('reweight[0]:', res_reweight[0])\n",
    "#         sys.exit(0)\n",
    "    \n",
    "    \n",
    "#     sys.exit(0)\n",
    "    \n",
    "#     print('init_iter_label')\n",
    "#     for i in range(init_iter_label):\n",
    "#         gx, gy = next(label_generator)\n",
    "#         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "#                    path_sym: list_path, path_id_sym: list_path_id,\n",
    "#                    w_path2pair_sym: w_path2pair}\n",
    "# #         feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#         _, l = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#         print 'iter label', i, l\n",
    "\n",
    "#### Start\n",
    "    iter_cnt = 0\n",
    "    while True:\n",
    "        for _ in range(max_iter):\n",
    "            for _ in range(comp_iter(iter_graph)):\n",
    "                gx, gy, list_path, list_path_id, w_path2pair = next(graph_generator)\n",
    "#                 print(list_path[0])\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1), \n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "                _, l, res_reweight_id, res_reweight = session.run([g_optimizer, g_loss, reweight_id, reweight], feed_dict=feed_dict)\n",
    "                print 'iter graph', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_inst)):\n",
    "#                 print('loop for iter_inst')\n",
    "                xs, ys, indexs = next(inst_generator)\n",
    "#                 gx, gy = next(label_generator)\n",
    "                xs = xs.toarray()\n",
    "                feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)}\n",
    "                _, l = session.run([optimizer, l_loss], feed_dict=feed_dict)\n",
    "    #           print 'iter inst', i, l\n",
    "\n",
    "            for _ in range(comp_iter(iter_label)):\n",
    "                print('loop for iter_label')\n",
    "                gx, gy = next(label_generator)\n",
    "                feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1),\n",
    "                           path_sym: list_path, path_id_sym: list_path_id,\n",
    "                           w_path2pair_sym: w_path2pair}\n",
    "#                 feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "                _, l = session.run([gl_optimizer, gl_loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        predict_y = tf.argmax(l_y, 1)\n",
    "        correct_prediction = tf.equal(predict_y, tf.argmax(y_sym, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    #     print(accuracy)\n",
    "#         print('number of iteration: %d', iter_cnt)\n",
    "        iter_cnt += 1\n",
    "        train_accuracy = accuracy.eval({x_sym: xs, y_sym: ys, g_sym: indexs, gy_sym: indexs.reshape(indexs.shape[0], 1)})\n",
    "#         print('Train Accuracy: %.4f', train_accuracy)\n",
    "        txs = tx.toarray()\n",
    "        t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)\n",
    "        test_accuracy = accuracy.eval({x_sym: txs, y_sym: ty, g_sym: t_index, gy_sym: t_index.reshape(t_index.shape[0], 1)})\n",
    "        if (test_accuracy > max_acc):\n",
    "            max_acc = test_accuracy\n",
    "        print('iter: %d, Tst Acc: %.4f, Trn Acc: %.4f, max test acc: %.4f' %(iter_cnt, test_accuracy, train_accuracy, max_acc))\n",
    "        \n",
    "#         if (iter_cnt == 5):\n",
    "#             sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(res_yhat_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :0.5000000000\n"
     ]
    }
   ],
   "source": [
    "tmp = 0.50000004\n",
    "\n",
    "print 'output :%.10f' %res_softmax_weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00496026,  0.00500135,  0.00504243,  0.0049207 ,  0.00508371,\n",
       "         0.00487118,  0.00503232,  0.00513423,  0.00512543,  0.00511278,\n",
       "         0.00495276,  0.00496932,  0.00509642,  0.00476472,  0.00508202,\n",
       "         0.00513261,  0.00514402,  0.00488825,  0.00513993,  0.00504225,\n",
       "         0.0049523 ,  0.00508264,  0.00510294,  0.0050783 ,  0.00510226,\n",
       "         0.00496483,  0.00512717,  0.00503125,  0.00491823,  0.00499666,\n",
       "         0.00503218,  0.00506654,  0.00481618,  0.00498946,  0.00489985,\n",
       "         0.00475723,  0.00493511,  0.00506777,  0.00507806,  0.0049991 ,\n",
       "         0.00499596,  0.00515256,  0.00484974,  0.0048238 ,  0.00505941,\n",
       "         0.00483398,  0.00483752,  0.00492471,  0.00502468,  0.00476777,\n",
       "         0.0050379 ,  0.00510353,  0.00502782,  0.00506198,  0.00492944,\n",
       "         0.00501779,  0.00505026,  0.00508298,  0.00483345,  0.00514694,\n",
       "         0.00506833,  0.00501812,  0.00487534,  0.00484263,  0.00506357,\n",
       "         0.00490818,  0.00507065,  0.00500399,  0.00495066,  0.00499   ,\n",
       "         0.00487355,  0.00497376,  0.00498765,  0.00495542,  0.0049043 ,\n",
       "         0.0050043 ,  0.00518636,  0.00499181,  0.0051011 ,  0.00515039,\n",
       "         0.00508644,  0.00495242,  0.00508357,  0.00487556,  0.00507548,\n",
       "         0.00498322,  0.00503005,  0.00568577,  0.00489993,  0.00480872,\n",
       "         0.00503724,  0.0050563 ,  0.0048571 ,  0.00502431,  0.0048797 ,\n",
       "         0.00501142,  0.00500673,  0.00496143,  0.00503046,  0.00510918,\n",
       "         0.00500047,  0.00493136,  0.0050557 ,  0.00499628,  0.0052007 ,\n",
       "         0.00492173,  0.00488759,  0.00474988,  0.00511011,  0.00496439,\n",
       "         0.00489418,  0.0050486 ,  0.00481584,  0.00498723,  0.00491892,\n",
       "         0.00495984,  0.00485087,  0.00514758,  0.00497422,  0.00501689,\n",
       "         0.0048694 ,  0.00520414,  0.00491453,  0.00506592,  0.00500151,\n",
       "         0.00512879,  0.0050356 ,  0.0051442 ,  0.00527726,  0.00503876,\n",
       "         0.00498829,  0.00472622,  0.00505534,  0.00485775,  0.00502062,\n",
       "         0.00509845,  0.00496754,  0.00513213,  0.00498726,  0.00487203,\n",
       "         0.00500988,  0.00522336,  0.00492245,  0.00509936,  0.00493318,\n",
       "         0.00506344,  0.00474202,  0.00499739,  0.00493809,  0.00499221,\n",
       "         0.00509367,  0.00490118,  0.0048842 ,  0.00516469,  0.00521361,\n",
       "         0.00498449,  0.00506454,  0.00482399,  0.0048924 ,  0.00496765,\n",
       "         0.00494862,  0.00504681,  0.0051941 ,  0.00496056,  0.00501945,\n",
       "         0.00498026,  0.00491342,  0.005009  ,  0.00502826,  0.00501721,\n",
       "         0.00481298,  0.00485981,  0.00495513,  0.00504775,  0.00514643,\n",
       "         0.00527066,  0.00487078,  0.00480212,  0.00497824,  0.00501305,\n",
       "         0.00504017,  0.00499521,  0.00516052,  0.00504666,  0.00497709,\n",
       "         0.00505974,  0.00473788,  0.00505384,  0.00501693,  0.00513083,\n",
       "         0.00515988,  0.00520042,  0.00503958,  0.00487334,  0.0049438 ,\n",
       "         0.00499534,  0.00499547,  0.00490065,  0.00468878,  0.00494727]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax((np.matmul(res_lstm_last_output, res_s_weight) + res_s_biase).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_lstm_last_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-041efa712b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_lstm_last_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res_lstm_last_output' is not defined"
     ]
    }
   ],
   "source": [
    "res_lstm_last_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 500)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_index = np.arange(x.shape[0], x.shape[0] + tx.shape[0], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_l_emd_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-keyword arg after keyword arg (<ipython-input-77-780a68635923>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-780a68635923>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    my_variable = tf.Variable(name=\"my_variable\", [[1,2,3],[4,5,6]], dtype = tf.float32)\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-keyword arg after keyword arg\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "my_variable = tf.Variable(name=\"my_variable\", [[1,2,3],[4,5,6]], dtype = tf.float32)\n",
    "sess.run(my_variable)\n",
    "print(sess.run(my_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Compatiable Dataset\n",
    "1. extract the feature vector, label vector and graph matrix\n",
    "   - mat_x, mat_y: sparse \n",
    "   - mat_graph: list\n",
    "2. split the dataset into training and test\n",
    "3. restore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from sklearn.utils import shuffle as skshuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = spio.loadmat('/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat', squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__globals__': [],\n",
       " '__header__': 'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Thu Jun  4 13:49:17 2009',\n",
       " '__version__': '1.0',\n",
       " 'group': <10312x39 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 14476 stored elements in Compressed Sparse Column format>,\n",
       " 'network': <10312x10312 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 667966 stored elements in Compressed Sparse Column format>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_sparse_graph_to_adj(g):\n",
    "    dict_g = dict()\n",
    "    for i in xrange(g.shape[0]):\n",
    "        one_ind = find(mat_graph[0])[1]\n",
    "        dict_g[i] = list(one_ind)\n",
    "    return dict_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_graph = convert_sparse_graph_to_adj(mat['network'])\n",
    "mat_y = np.array(mat['group'].todense(), dtype=np.int32)\n",
    "mat_x = mat['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_shuffles = 1\n",
    "shuffles = []\n",
    "for x in range(num_shuffles):\n",
    "    shuffles.append(skshuffle(mat_x, mat_y))\n",
    "\n",
    "training_percents = [0.1]\n",
    "\n",
    "for train_percent in training_percents:\n",
    "    for shuf in shuffles:\n",
    "        \n",
    "        X, y = shuf\n",
    "        \n",
    "        training_size = int(train_percent * X.shape[0])\n",
    "        \n",
    "        X_train = X[:training_size, :]\n",
    "        y_train_ = y[:training_size]\n",
    "        \n",
    "        X_test = X[training_size:, :]\n",
    "        y_test_ = y[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mat_y[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATASET = 'pubmed'\n",
    "NAMES = ['x', 'y', 'tx', 'ty', 'graph']\n",
    "OBJECTS = []\n",
    "for i in range(len(NAMES)):\n",
    "    OBJECTS.append(cPickle.load(open(\"/hdd2/graph_embedding/planetoid/data/trans.{}.{}\".format(DATASET, NAMES[i]))))\n",
    "x, y, tx, ty, graph = tuple(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.x\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.y\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.tx\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.ty\n",
      "/hdd2/graph_embedding/dataset/blogcatalog/trans.blogcatalog.graph\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'blogcatalog'\n",
    "OBJECTS = []\n",
    "OBJECTS.append(X_train)\n",
    "OBJECTS.append(y_train_)\n",
    "OBJECTS.append(X_test)\n",
    "OBJECTS.append(y_test_)\n",
    "OBJECTS.append(graph)\n",
    "for i in range(len(NAMES)):\n",
    "    f = \"/hdd2/graph_embedding/dataset/blogcatalog/trans.{}.{}\".format(DATASET, NAMES[i])\n",
    "    print(f)\n",
    "    cPickle.dump(OBJECTS[i], open(f, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 2.,  1.]]), array([0, 2, 1]), array([1, 3, 2])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x = np.array([[1., 0.], [2., 1.], [0., 0.]])\n",
    "tmp_y = np.array([0, 1, 2])\n",
    "tmp_z = np.array([1,2,3])\n",
    "skshuffle(tmp_x, tmp_y, tmp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-8182ff768e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
