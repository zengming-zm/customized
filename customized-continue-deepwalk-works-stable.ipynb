{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from multiprocessing import cpu_count\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from six import iteritems\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer, RandomUniform, RandomNormal\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "\n",
    "import word2vec\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "DATASET = 'blogcatalog'\n",
    "\n",
    "embedding_size = 128\n",
    "learning_rate = 0.1\n",
    "gl_learning_rate = 0.1\n",
    "batch_size = 200\n",
    "neg_samp = 0\n",
    "model_file = 'trans.model'\n",
    "\n",
    "window_size = 10\n",
    "path_size = 10\n",
    "\n",
    "\n",
    "use_feature = True\n",
    "update_emb = True\n",
    "layer_loss =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = ['/hdd2/graph_embedding/customized/blogcatalog.embeddings.walks.0']\n",
    "dataset = genfromtxt(file_list[0], delimiter=' ')\n",
    "\n",
    "def get_num_vacabulary(dataset):\n",
    "    word_count = 0\n",
    "    for d in dataset:\n",
    "        word_count = max(word_count, max(d))\n",
    "    return int(word_count)\n",
    "\n",
    "vocabulary_size = get_num_vacabulary(dataset) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = dataset.flatten()\n",
    "words = [str(int(w)) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614', '5573', '4732', '1159', '448', '5704', '5382', '1008', '5395', '6958']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1614.,   5573.,   4732.,   1159.,    448.,   5704.,   5382.,\n",
       "         1008.,   5395.,   6958.,   1320.,   4139.,   6814.,   5404.,\n",
       "         7988.,   8384.,  10139.,   6248.,   4226.,   2829.,   9090.,\n",
       "         8456.,   6892.,    738.,   3898.,   6958.,   3048.,   6796.,\n",
       "         7636.,   4104.,   7450.,   1225.,   6061.,    457.,   8968.,\n",
       "           35.,   1141.,   4996.,   1453.,   2979.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common words (+UNK)', [('4838', 196735), ('175', 195187), ('4373', 169490), ('8156', 147968), ('1225', 137221)])\n",
      "('Sample data', [1193, 2874, 225, 457, 19, 34, 5662, 37, 3046, 29])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = []\n",
    "#     count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "#         else:\n",
    "#             index = 0  # dictionary['UNK']\n",
    "        data.append(index)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1193, 2874, 225, 457, 19, 34, 5662, 37, 3046, 29, 1978, 802, 5882, 1526, 32, 883, 168, 1850, 788, 194, 1969]\n",
      "[  1614.   5573.   4732.   1159.    448.   5704.   5382.   1008.   5395.\n",
      "   6958.   1320.   4139.   6814.   5404.   7988.   8384.  10139.   6248.\n",
      "   4226.   2829.   9090.]\n"
     ]
    }
   ],
   "source": [
    "print(data[:21])\n",
    "print(dataset[0][:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['1008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 20 and skip_window = 10:\n",
      "    node position:\n",
      "[1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978\n",
      " 1978 1978 1978 1978 1978  802  802  802  802  802  802  802  802  802  802\n",
      "  802  802  802  802  802  802  802  802  802  802 5882 5882 5882 5882 5882\n",
      " 5882 5882 5882 5882 5882 5882 5882 5882 5882 5882 5882 5882 5882 5882 5882\n",
      " 1526 1526 1526 1526 1526 1526 1526 1526 1526 1526 1526 1526 1526 1526 1526\n",
      " 1526 1526 1526 1526 1526   32   32   32   32   32   32   32   32   32   32\n",
      "   32   32   32   32   32   32   32   32   32   32  883  883  883  883  883\n",
      "  883  883  883  883  883  883  883  883  883  883  883  883  883  883  883\n",
      "  168  168  168  168  168  168  168  168  168  168  168  168  168  168  168\n",
      "  168  168  168  168  168 1850 1850 1850 1850 1850 1850 1850 1850 1850 1850\n",
      " 1850 1850 1850 1850 1850 1850 1850 1850 1850 1850  788  788  788  788  788\n",
      "  788  788  788  788  788  788  788  788  788  788  788  788  788  788  788\n",
      "  194  194  194  194  194  194  194  194  194  194  194  194  194  194  194\n",
      "  194  194  194  194  194 1969 1969 1969 1969 1969 1969 1969 1969 1969 1969\n",
      " 1969 1969 1969 1969 1969 1969 1969 1969 1969 1969  246  246  246  246  246\n",
      "  246  246  246  246  246  246  246  246  246  246  246  246  246  246  246\n",
      " 3787 3787 3787 3787 3787 3787 3787 3787 3787 3787 3787 3787 3787 3787 3787\n",
      " 3787 3787 3787 3787 3787   17   17   17   17   17   17   17   17   17   17\n",
      "   17   17   17   17   17   17   17   17   17   17  781  781  781  781  781\n",
      "  781  781  781  781  781  781  781  781  781  781  781  781  781  781  781\n",
      "   29   29   29   29   29   29   29   29   29   29   29   29   29   29   29\n",
      "   29   29   29   29   29 1213 1213 1213 1213 1213 1213 1213 1213 1213 1213\n",
      " 1213 1213 1213 1213 1213 1213 1213 1213 1213 1213 3905 3905 3905 3905 3905\n",
      " 3905 3905 3905 3905 3905 3905 3905 3905 3905 3905 3905 3905 3905 3905 3905\n",
      "  636  636  636  636  636  636  636  636  636  636  636  636  636  636  636\n",
      "  636  636  636  636  636  593  593  593  593  593  593  593  593  593  593\n",
      "  593  593  593  593  593  593  593  593  593  593 1105 1105 1105 1105 1105\n",
      " 1105 1105 1105 1105 1105 1105 1105 1105 1105 1105 1105 1105 1105 1105 1105]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "#     print(buffer)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "# print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(20, 10)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=420, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "#     print('    batch node id:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    node position:')\n",
    "    print(batch)\n",
    "#     print(labels)\n",
    "#     print('    labels:', [reverse_dictionary[li] for li in labels.reshape(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[1193, 2874, 225, 457, 19, 34, 5662, 37, 3046, 29, 1978, 802, 5882, 1526, 32, 883, 168, 1850, 788, 194, 1969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4838'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inject trained embedding\n",
    "model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "features_matrix = np.asarray([model[str(node)] for node in range(len(model.vocab)-1)])\n",
    "# make the order consistant to the current one\n",
    "reordered_list = np.asarray([int(reverse_dictionary[idx]) for idx in range(len(model.vocab)-1)])\n",
    "reordered_features_matrix = features_matrix[reordered_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproduce Gensim weights initialization\n",
    "def seeded_vector(seed_string, vector_size):\n",
    "    \"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\n",
    "    # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "    once = np.random.RandomState(hash(seed_string) & 0xffffffff)\n",
    "    return (once.rand(vector_size) - 0.5) / vector_size\n",
    "\n",
    "features_list = []\n",
    "for idx in range(len(model.vocab)-1):\n",
    "    str_node = reverse_dictionary[idx]\n",
    "    features_list.append(seeded_vector(str_node + str(1), 128))\n",
    "features_matrix = np.asarray(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 10 # How many words to consider left and right.\n",
    "num_skips = 20 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Variables.\n",
    "#     embeddings = tf.Variable(\n",
    "#         tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embeddings = tf.Variable(features_matrix, dtype=tf.float32)\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.25).minimize(loss)\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.2862\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l, res_embed = session.run([optimizer, loss, embed], feed_dict=feed_dict)\n",
    "    print('loss: %.4f' %(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.887600\n",
      "Average loss at step 2000: 4.763097\n",
      "Average loss at step 4000: 4.414267\n",
      "Average loss at step 6000: 4.301806\n",
      "Average loss at step 8000: 4.242945\n",
      "Average loss at step 10000: 4.210729\n",
      "Average loss at step 12000: 4.186070\n",
      "Average loss at step 14000: 4.167826\n",
      "Average loss at step 16000: 4.154741\n",
      "Average loss at step 18000: 4.147257\n",
      "Average loss at step 20000: 4.140110\n",
      "Average loss at step 22000: 4.134115\n",
      "Average loss at step 24000: 4.128324\n",
      "Average loss at step 26000: 4.124496\n",
      "Average loss at step 28000: 4.120764\n",
      "Average loss at step 30000: 4.117326\n",
      "Average loss at step 32000: 4.114615\n",
      "Average loss at step 34000: 4.112008\n",
      "Average loss at step 36000: 4.109431\n",
      "Average loss at step 38000: 4.105626\n",
      "Average loss at step 40000: 4.104640\n",
      "Average loss at step 42000: 4.104435\n",
      "Average loss at step 44000: 4.103202\n",
      "Average loss at step 46000: 4.099879\n",
      "Average loss at step 48000: 4.098715\n",
      "Average loss at step 50000: 4.099005\n",
      "Average loss at step 52000: 4.097219\n",
      "Average loss at step 54000: 4.096598\n",
      "Average loss at step 56000: 4.095877\n",
      "Average loss at step 58000: 4.095760\n",
      "Average loss at step 60000: 4.093854\n",
      "Average loss at step 62000: 4.093604\n",
      "Average loss at step 64000: 4.092517\n",
      "Average loss at step 66000: 4.091513\n",
      "Average loss at step 68000: 4.092448\n",
      "Average loss at step 70000: 4.091114\n",
      "Average loss at step 72000: 4.089298\n",
      "Average loss at step 74000: 4.090794\n",
      "Average loss at step 76000: 4.089272\n",
      "Average loss at step 78000: 4.090877\n",
      "Average loss at step 80000: 4.090186\n",
      "Average loss at step 82000: 4.088848\n",
      "Average loss at step 84000: 4.088860\n",
      "Average loss at step 86000: 4.087469\n",
      "Average loss at step 88000: 4.088179\n",
      "Average loss at step 90000: 4.086729\n",
      "Average loss at step 92000: 4.088254\n",
      "Average loss at step 94000: 4.085934\n",
      "Average loss at step 96000: 4.084625\n",
      "Average loss at step 98000: 4.085054\n",
      "Average loss at step 100000: 4.083939\n",
      "Average loss at step 102000: 4.084696\n",
      "Average loss at step 104000: 4.087001\n",
      "Average loss at step 106000: 4.085225\n",
      "Average loss at step 108000: 4.083407\n",
      "Average loss at step 110000: 4.084001\n",
      "Average loss at step 112000: 4.081570\n",
      "Average loss at step 114000: 4.084160\n",
      "Average loss at step 116000: 4.081591\n",
      "Average loss at step 118000: 4.082645\n",
      "Average loss at step 120000: 4.080750\n",
      "Average loss at step 122000: 4.083444\n",
      "Average loss at step 124000: 4.082879\n",
      "Average loss at step 126000: 4.082467\n",
      "Average loss at step 128000: 4.081600\n",
      "Average loss at step 130000: 4.082737\n",
      "Average loss at step 132000: 4.081038\n",
      "Average loss at step 134000: 4.080651\n",
      "Average loss at step 136000: 4.081792\n",
      "Average loss at step 138000: 4.081914\n",
      "Average loss at step 140000: 4.080068\n",
      "Average loss at step 142000: 4.079256\n",
      "Average loss at step 144000: 4.081905\n",
      "Average loss at step 146000: 4.080783\n",
      "Average loss at step 148000: 4.080042\n",
      "Average loss at step 150000: 4.081627\n",
      "Average loss at step 152000: 4.080622\n",
      "Average loss at step 154000: 4.079881\n",
      "Average loss at step 156000: 4.080084\n",
      "Average loss at step 158000: 4.078028\n",
      "Average loss at step 160000: 4.076942\n",
      "Average loss at step 162000: 4.081099\n",
      "Average loss at step 164000: 4.078711\n",
      "Average loss at step 166000: 4.077974\n",
      "Average loss at step 168000: 4.078655\n",
      "Average loss at step 170000: 4.076471\n",
      "Average loss at step 172000: 4.077743\n",
      "Average loss at step 174000: 4.079027\n",
      "Average loss at step 176000: 4.077118\n",
      "Average loss at step 178000: 4.078387\n",
      "Average loss at step 180000: 4.077521\n",
      "Average loss at step 182000: 4.077083\n",
      "Average loss at step 184000: 4.076524\n",
      "Average loss at step 186000: 4.076515\n",
      "Average loss at step 188000: 4.075211\n",
      "Average loss at step 190000: 4.075089\n",
      "Average loss at step 192000: 4.077681\n",
      "Average loss at step 194000: 4.077631\n",
      "Average loss at step 196000: 4.075534\n",
      "Average loss at step 198000: 4.074936\n",
      "Average loss at step 200000: 4.073497\n",
      "Average loss at step 202000: 4.074867\n",
      "Average loss at step 204000: 4.077895\n",
      "Average loss at step 206000: 4.076011\n",
      "Average loss at step 208000: 4.075105\n",
      "Average loss at step 210000: 4.074636\n",
      "Average loss at step 212000: 4.075994\n",
      "Average loss at step 214000: 4.075119\n",
      "Average loss at step 216000: 4.076487\n",
      "Average loss at step 218000: 4.072235\n",
      "Average loss at step 220000: 4.075835\n",
      "Average loss at step 222000: 4.075480\n",
      "Average loss at step 224000: 4.076202\n",
      "Average loss at step 226000: 4.073913\n",
      "Average loss at step 228000: 4.075663\n",
      "Average loss at step 230000: 4.074935\n",
      "Average loss at step 232000: 4.076663\n",
      "Average loss at step 234000: 4.076018\n",
      "Average loss at step 236000: 4.073742\n",
      "Average loss at step 238000: 4.073933\n",
      "Average loss at step 240000: 4.071304\n",
      "Average loss at step 242000: 4.073823\n",
      "Average loss at step 244000: 4.075150\n",
      "Average loss at step 246000: 4.072077\n",
      "Average loss at step 248000: 4.075718\n",
      "Average loss at step 250000: 4.071534\n",
      "Average loss at step 252000: 4.074101\n",
      "Average loss at step 254000: 4.072696\n",
      "Average loss at step 256000: 4.074805\n",
      "Average loss at step 258000: 4.071814\n",
      "Average loss at step 260000: 4.076378\n",
      "Average loss at step 262000: 4.074035\n",
      "Average loss at step 264000: 4.071833\n",
      "Average loss at step 266000: 4.074777\n",
      "Average loss at step 268000: 4.071472\n",
      "Average loss at step 270000: 4.073493\n",
      "Average loss at step 272000: 4.070748\n",
      "Average loss at step 274000: 4.071678\n",
      "Average loss at step 276000: 4.071783\n",
      "Average loss at step 278000: 4.071935\n",
      "Average loss at step 280000: 4.070862\n",
      "Average loss at step 282000: 4.073410\n",
      "Average loss at step 284000: 4.071368\n",
      "Average loss at step 286000: 4.073374\n",
      "Average loss at step 288000: 4.071941\n",
      "Average loss at step 290000: 4.071247\n",
      "Average loss at step 292000: 4.070807\n",
      "Average loss at step 294000: 4.070784\n",
      "Average loss at step 296000: 4.072882\n",
      "Average loss at step 298000: 4.068802\n",
      "Average loss at step 300000: 4.070177\n",
      "Average loss at step 302000: 4.070718\n",
      "Average loss at step 304000: 4.070809\n",
      "Average loss at step 306000: 4.069045\n",
      "Average loss at step 308000: 4.070834\n",
      "Average loss at step 310000: 4.071587\n",
      "Average loss at step 312000: 4.070584\n",
      "Average loss at step 314000: 4.070032\n",
      "Average loss at step 316000: 4.070295\n",
      "Average loss at step 318000: 4.071221\n",
      "Average loss at step 320000: 4.069886\n",
      "Average loss at step 322000: 4.071850\n",
      "Average loss at step 324000: 4.067656\n",
      "Average loss at step 326000: 4.067188\n",
      "Average loss at step 328000: 4.070192\n",
      "Average loss at step 330000: 4.071305\n",
      "Average loss at step 332000: 4.069327\n",
      "Average loss at step 334000: 4.069150\n",
      "Average loss at step 336000: 4.070226\n",
      "Average loss at step 338000: 4.069133\n",
      "Average loss at step 340000: 4.070866\n",
      "Average loss at step 342000: 4.070018\n",
      "Average loss at step 344000: 4.068217\n",
      "Average loss at step 346000: 4.067968\n",
      "Average loss at step 348000: 4.070425\n",
      "Average loss at step 350000: 4.069750\n",
      "Average loss at step 352000: 4.070097\n",
      "Average loss at step 354000: 4.068980\n",
      "Average loss at step 356000: 4.069586\n",
      "Average loss at step 358000: 4.068298\n",
      "Average loss at step 360000: 4.069462\n",
      "Average loss at step 362000: 4.068375\n",
      "Average loss at step 364000: 4.069152\n",
      "Average loss at step 366000: 4.070494\n",
      "Average loss at step 368000: 4.069837\n",
      "Average loss at step 370000: 4.068453\n",
      "Average loss at step 372000: 4.069983\n",
      "Average loss at step 374000: 4.069295\n",
      "Average loss at step 376000: 4.068756\n",
      "Average loss at step 378000: 4.066103\n",
      "Average loss at step 380000: 4.068722\n",
      "Average loss at step 382000: 4.066602\n",
      "Average loss at step 384000: 4.066959\n",
      "Average loss at step 386000: 4.069269\n",
      "Average loss at step 388000: 4.065574\n",
      "Average loss at step 390000: 4.066710\n",
      "Average loss at step 392000: 4.067877\n",
      "Average loss at step 394000: 4.068210\n",
      "Average loss at step 396000: 4.067526\n",
      "Average loss at step 398000: 4.067964\n",
      "Average loss at step 400000: 4.069286\n",
      "Average loss at step 402000: 4.069024\n",
      "Average loss at step 404000: 4.065954\n",
      "Average loss at step 406000: 4.065622\n",
      "Average loss at step 408000: 4.066287\n",
      "Average loss at step 410000: 4.067321\n",
      "Average loss at step 412000: 4.067310\n",
      "Average loss at step 414000: 4.065710\n",
      "Average loss at step 416000: 4.067021\n",
      "Average loss at step 418000: 4.068339\n",
      "Average loss at step 420000: 4.067135\n",
      "Average loss at step 422000: 4.066885\n",
      "Average loss at step 424000: 4.069528\n",
      "Average loss at step 426000: 4.063974\n",
      "Average loss at step 428000: 4.067322\n",
      "Average loss at step 430000: 4.067326\n",
      "Average loss at step 432000: 4.065028\n",
      "Average loss at step 434000: 4.067956\n",
      "Average loss at step 436000: 4.064195\n",
      "Average loss at step 438000: 4.064618\n",
      "Average loss at step 440000: 4.066397\n",
      "Average loss at step 442000: 4.066077\n",
      "Average loss at step 444000: 4.064713\n",
      "Average loss at step 446000: 4.067709\n",
      "Average loss at step 448000: 4.066868\n",
      "Average loss at step 450000: 4.065804\n",
      "Average loss at step 452000: 4.064636\n",
      "Average loss at step 454000: 4.062480\n",
      "Average loss at step 456000: 4.066139\n",
      "Average loss at step 458000: 4.064316\n",
      "Average loss at step 460000: 4.066375\n",
      "Average loss at step 462000: 4.065440\n",
      "Average loss at step 464000: 4.065330\n",
      "Average loss at step 466000: 4.064867\n",
      "Average loss at step 468000: 4.066324\n",
      "Average loss at step 470000: 4.065828\n",
      "Average loss at step 472000: 4.063960\n",
      "Average loss at step 474000: 4.063444\n",
      "Average loss at step 476000: 4.064657\n",
      "Average loss at step 478000: 4.066539\n",
      "Average loss at step 480000: 4.063814\n",
      "Average loss at step 482000: 4.062397\n",
      "Average loss at step 484000: 4.064035\n",
      "Average loss at step 486000: 4.062870\n",
      "Average loss at step 488000: 4.066847\n",
      "Average loss at step 490000: 4.061796\n",
      "Average loss at step 492000: 4.063662\n",
      "Average loss at step 494000: 4.066781\n",
      "Average loss at step 496000: 4.065329\n",
      "Average loss at step 498000: 4.064387\n",
      "Average loss at step 500000: 4.064396\n",
      "Average loss at step 502000: 4.061978\n",
      "Average loss at step 504000: 4.063591\n",
      "Average loss at step 506000: 4.064821\n",
      "Average loss at step 508000: 4.064485\n",
      "Average loss at step 510000: 4.061809\n",
      "Average loss at step 512000: 4.065260\n",
      "Average loss at step 514000: 4.063927\n",
      "Average loss at step 516000: 4.064208\n",
      "Average loss at step 518000: 4.062439\n",
      "Average loss at step 520000: 4.064603\n",
      "Average loss at step 522000: 4.061532\n",
      "Average loss at step 524000: 4.064441\n",
      "Average loss at step 526000: 4.063190\n",
      "Average loss at step 528000: 4.061994\n",
      "Average loss at step 530000: 4.064315\n",
      "Average loss at step 532000: 4.062272\n",
      "Average loss at step 534000: 4.063932\n",
      "Average loss at step 536000: 4.061719\n",
      "Average loss at step 538000: 4.062329\n",
      "Average loss at step 540000: 4.062743\n",
      "Average loss at step 542000: 4.063217\n",
      "Average loss at step 544000: 4.061115\n",
      "Average loss at step 546000: 4.062924\n",
      "Average loss at step 548000: 4.060926\n",
      "Average loss at step 550000: 4.063191\n",
      "Average loss at step 552000: 4.060734\n",
      "Average loss at step 554000: 4.061393\n",
      "Average loss at step 556000: 4.062705\n",
      "Average loss at step 558000: 4.061685\n",
      "Average loss at step 560000: 4.060950\n",
      "Average loss at step 562000: 4.063216\n",
      "Average loss at step 564000: 4.062250\n",
      "Average loss at step 566000: 4.059795\n",
      "Average loss at step 568000: 4.061975\n",
      "Average loss at step 570000: 4.061976\n",
      "Average loss at step 572000: 4.059568\n",
      "Average loss at step 574000: 4.061762\n",
      "Average loss at step 576000: 4.061406\n",
      "Average loss at step 578000: 4.061941\n",
      "Average loss at step 580000: 4.061681\n",
      "Average loss at step 582000: 4.061057\n",
      "Average loss at step 584000: 4.064013\n",
      "Average loss at step 586000: 4.060063\n",
      "Average loss at step 588000: 4.062069\n",
      "Average loss at step 590000: 4.062113\n",
      "Average loss at step 592000: 4.062813\n",
      "Average loss at step 594000: 4.060556\n",
      "Average loss at step 596000: 4.061311\n",
      "Average loss at step 598000: 4.057846\n",
      "Average loss at step 600000: 4.058992\n",
      "Average loss at step 602000: 4.061226\n",
      "Average loss at step 604000: 4.060063\n",
      "Average loss at step 606000: 4.060382\n",
      "Average loss at step 608000: 4.061791\n",
      "Average loss at step 610000: 4.058530\n",
      "Average loss at step 612000: 4.060576\n",
      "Average loss at step 614000: 4.059489\n",
      "Average loss at step 616000: 4.061364\n",
      "Average loss at step 618000: 4.060150\n",
      "Average loss at step 620000: 4.059730\n",
      "Average loss at step 622000: 4.062108\n",
      "Average loss at step 624000: 4.061658\n",
      "Average loss at step 626000: 4.058624\n",
      "Average loss at step 628000: 4.060655\n",
      "Average loss at step 630000: 4.059359\n",
      "Average loss at step 632000: 4.060041\n",
      "Average loss at step 634000: 4.060753\n",
      "Average loss at step 636000: 4.061025\n",
      "Average loss at step 638000: 4.060729\n",
      "Average loss at step 640000: 4.060966\n",
      "Average loss at step 642000: 4.058764\n",
      "Average loss at step 644000: 4.060430\n",
      "Average loss at step 646000: 4.058556\n",
      "Average loss at step 648000: 4.061240\n",
      "Average loss at step 650000: 4.058518\n",
      "Average loss at step 652000: 4.058769\n",
      "Average loss at step 654000: 4.059771\n",
      "Average loss at step 656000: 4.059810\n",
      "Average loss at step 658000: 4.059070\n",
      "Average loss at step 660000: 4.060187\n",
      "Average loss at step 662000: 4.061048\n",
      "Average loss at step 664000: 4.057401\n",
      "Average loss at step 666000: 4.058296\n",
      "Average loss at step 668000: 4.060336\n",
      "Average loss at step 670000: 4.058361\n",
      "Average loss at step 672000: 4.060883\n",
      "Average loss at step 674000: 4.061359\n",
      "Average loss at step 676000: 4.058992\n",
      "Average loss at step 678000: 4.059571\n",
      "Average loss at step 680000: 4.059256\n",
      "Average loss at step 682000: 4.058043\n",
      "Average loss at step 684000: 4.059093\n",
      "Average loss at step 686000: 4.057860\n",
      "Average loss at step 688000: 4.058094\n",
      "Average loss at step 690000: 4.061535\n",
      "Average loss at step 692000: 4.057784\n",
      "Average loss at step 694000: 4.059987\n",
      "Average loss at step 696000: 4.060050\n",
      "Average loss at step 698000: 4.057822\n",
      "Average loss at step 700000: 4.058537\n",
      "Average loss at step 702000: 4.058116\n",
      "Average loss at step 704000: 4.058368\n",
      "Average loss at step 706000: 4.057780\n",
      "Average loss at step 708000: 4.059259\n",
      "Average loss at step 710000: 4.058748\n",
      "Average loss at step 712000: 4.058472\n",
      "Average loss at step 714000: 4.057203\n",
      "Average loss at step 716000: 4.057492\n",
      "Average loss at step 718000: 4.057944\n",
      "Average loss at step 720000: 4.058783\n",
      "Average loss at step 722000: 4.056988\n",
      "Average loss at step 724000: 4.055216\n",
      "Average loss at step 726000: 4.056647\n",
      "Average loss at step 728000: 4.057775\n",
      "Average loss at step 730000: 4.059943\n",
      "Average loss at step 732000: 4.056410\n",
      "Average loss at step 734000: 4.058287\n",
      "Average loss at step 736000: 4.059231\n",
      "Average loss at step 738000: 4.057164\n",
      "Average loss at step 740000: 4.056301\n",
      "Average loss at step 742000: 4.057619\n",
      "Average loss at step 744000: 4.056462\n",
      "Average loss at step 746000: 4.058549\n",
      "Average loss at step 748000: 4.058048\n",
      "Average loss at step 750000: 4.056969\n",
      "Average loss at step 752000: 4.058064\n",
      "Average loss at step 754000: 4.055552\n",
      "Average loss at step 756000: 4.057033\n",
      "Average loss at step 758000: 4.056542\n",
      "Average loss at step 760000: 4.056932\n",
      "Average loss at step 762000: 4.057700\n",
      "Average loss at step 764000: 4.056246\n",
      "Average loss at step 766000: 4.057930\n",
      "Average loss at step 768000: 4.055846\n",
      "Average loss at step 770000: 4.058863\n",
      "Average loss at step 772000: 4.057239\n",
      "Average loss at step 774000: 4.055837\n",
      "Average loss at step 776000: 4.055760\n",
      "Average loss at step 778000: 4.057267\n",
      "Average loss at step 780000: 4.056102\n",
      "Average loss at step 782000: 4.058020\n",
      "Average loss at step 784000: 4.056526\n",
      "Average loss at step 786000: 4.057914\n",
      "Average loss at step 788000: 4.057932\n",
      "Average loss at step 790000: 4.057328\n",
      "Average loss at step 792000: 4.056732\n",
      "Average loss at step 794000: 4.057094\n",
      "Average loss at step 796000: 4.056268\n",
      "Average loss at step 798000: 4.056549\n",
      "Average loss at step 800000: 4.057098\n",
      "Average loss at step 802000: 4.057821\n",
      "Average loss at step 804000: 4.055672\n",
      "Average loss at step 806000: 4.057767\n",
      "Average loss at step 808000: 4.055215\n",
      "Average loss at step 810000: 4.055186\n",
      "Average loss at step 812000: 4.059243\n",
      "Average loss at step 814000: 4.056719\n",
      "Average loss at step 816000: 4.056019\n",
      "Average loss at step 818000: 4.058032\n",
      "Average loss at step 820000: 4.055006\n",
      "Average loss at step 822000: 4.054738\n",
      "Average loss at step 824000: 4.056477\n",
      "Average loss at step 826000: 4.057038\n",
      "Average loss at step 828000: 4.056058\n",
      "Average loss at step 830000: 4.056651\n",
      "Average loss at step 832000: 4.055630\n",
      "Average loss at step 834000: 4.058079\n",
      "Average loss at step 836000: 4.054782\n",
      "Average loss at step 838000: 4.054747\n",
      "Average loss at step 840000: 4.055378\n",
      "Average loss at step 842000: 4.055298\n",
      "Average loss at step 844000: 4.056153\n",
      "Average loss at step 846000: 4.054124\n",
      "Average loss at step 848000: 4.054363\n",
      "Average loss at step 850000: 4.056288\n",
      "Average loss at step 852000: 4.056389\n",
      "Average loss at step 854000: 4.053390\n",
      "Average loss at step 856000: 4.055585\n",
      "Average loss at step 858000: 4.056535\n",
      "Average loss at step 860000: 4.056085\n",
      "Average loss at step 862000: 4.053956\n",
      "Average loss at step 864000: 4.055814\n",
      "Average loss at step 866000: 4.054607\n",
      "Average loss at step 868000: 4.054993\n",
      "Average loss at step 870000: 4.052942\n",
      "Average loss at step 872000: 4.053903\n",
      "Average loss at step 874000: 4.055379\n",
      "Average loss at step 876000: 4.054234\n",
      "Average loss at step 878000: 4.052951\n",
      "Average loss at step 880000: 4.053132\n",
      "Average loss at step 882000: 4.054035\n",
      "Average loss at step 884000: 4.056626\n",
      "Average loss at step 886000: 4.055153\n",
      "Average loss at step 888000: 4.054262\n",
      "Average loss at step 890000: 4.052892\n",
      "Average loss at step 892000: 4.052712\n",
      "Average loss at step 894000: 4.054772\n",
      "Average loss at step 896000: 4.055932\n",
      "Average loss at step 898000: 4.054708\n",
      "Average loss at step 900000: 4.057160\n",
      "Average loss at step 902000: 4.056463\n",
      "Average loss at step 904000: 4.053736\n",
      "Average loss at step 906000: 4.054950\n",
      "Average loss at step 908000: 4.055172\n",
      "Average loss at step 910000: 4.054942\n",
      "Average loss at step 912000: 4.055838\n",
      "Average loss at step 914000: 4.052670\n",
      "Average loss at step 916000: 4.053545\n",
      "Average loss at step 918000: 4.053275\n",
      "Average loss at step 920000: 4.056707\n",
      "Average loss at step 922000: 4.053383\n",
      "Average loss at step 924000: 4.052247\n",
      "Average loss at step 926000: 4.053041\n",
      "Average loss at step 928000: 4.054101\n",
      "Average loss at step 930000: 4.053162\n",
      "Average loss at step 932000: 4.055943\n",
      "Average loss at step 934000: 4.053585\n",
      "Average loss at step 936000: 4.051265\n",
      "Average loss at step 938000: 4.052250\n",
      "Average loss at step 940000: 4.054994\n",
      "Average loss at step 942000: 4.053660\n",
      "Average loss at step 944000: 4.055168\n",
      "Average loss at step 946000: 4.053223\n",
      "Average loss at step 948000: 4.055969\n",
      "Average loss at step 950000: 4.053432\n",
      "Average loss at step 952000: 4.051927\n",
      "Average loss at step 954000: 4.052599\n",
      "Average loss at step 956000: 4.054528\n",
      "Average loss at step 958000: 4.053574\n",
      "Average loss at step 960000: 4.054779\n",
      "Average loss at step 962000: 4.055479\n",
      "Average loss at step 964000: 4.054037\n",
      "Average loss at step 966000: 4.052258\n",
      "Average loss at step 968000: 4.054222\n",
      "Average loss at step 970000: 4.054692\n",
      "Average loss at step 972000: 4.053180\n",
      "Average loss at step 974000: 4.051788\n",
      "Average loss at step 976000: 4.052483\n",
      "Average loss at step 978000: 4.050368\n",
      "Average loss at step 980000: 4.055357\n",
      "Average loss at step 982000: 4.052214\n",
      "Average loss at step 984000: 4.054254\n",
      "Average loss at step 986000: 4.054503\n",
      "Average loss at step 988000: 4.053621\n",
      "Average loss at step 990000: 4.050747\n",
      "Average loss at step 992000: 4.052995\n",
      "Average loss at step 994000: 4.054182\n",
      "Average loss at step 996000: 4.053374\n",
      "Average loss at step 998000: 4.052566\n",
      "Average loss at step 1000000: 4.053767\n",
      "Average loss at step 1002000: 4.053307\n",
      "Average loss at step 1004000: 4.052416\n",
      "Average loss at step 1006000: 4.054828\n",
      "Average loss at step 1008000: 4.052359\n",
      "Average loss at step 1010000: 4.052665\n",
      "Average loss at step 1012000: 4.052595\n",
      "Average loss at step 1014000: 4.052846\n",
      "Average loss at step 1016000: 4.054065\n",
      "Average loss at step 1018000: 4.050494\n",
      "Average loss at step 1020000: 4.051194\n",
      "Average loss at step 1022000: 4.051246\n",
      "Average loss at step 1024000: 4.052807\n",
      "Average loss at step 1026000: 4.053666\n",
      "Average loss at step 1028000: 4.051846\n",
      "Average loss at step 1030000: 4.053749\n",
      "Average loss at step 1032000: 4.053525\n",
      "Average loss at step 1034000: 4.054622\n",
      "Average loss at step 1036000: 4.052945\n",
      "Average loss at step 1038000: 4.051173\n",
      "Average loss at step 1040000: 4.051321\n",
      "Average loss at step 1042000: 4.054010\n",
      "Average loss at step 1044000: 4.053210\n",
      "Average loss at step 1046000: 4.051141\n",
      "Average loss at step 1048000: 4.052430\n",
      "Average loss at step 1050000: 4.052130\n",
      "Average loss at step 1052000: 4.053893\n",
      "Average loss at step 1054000: 4.052056\n",
      "Average loss at step 1056000: 4.052643\n",
      "Average loss at step 1058000: 4.050221\n",
      "Average loss at step 1060000: 4.051154\n",
      "Average loss at step 1062000: 4.054030\n",
      "Average loss at step 1064000: 4.051515\n",
      "Average loss at step 1066000: 4.052339\n",
      "Average loss at step 1068000: 4.048868\n",
      "Average loss at step 1070000: 4.053091\n",
      "Average loss at step 1072000: 4.052724\n",
      "Average loss at step 1074000: 4.053717\n",
      "Average loss at step 1076000: 4.049374\n",
      "Average loss at step 1078000: 4.052771\n",
      "Average loss at step 1080000: 4.053146\n",
      "Average loss at step 1082000: 4.051312\n",
      "Average loss at step 1084000: 4.054390\n",
      "Average loss at step 1086000: 4.051514\n",
      "Average loss at step 1088000: 4.050039\n",
      "Average loss at step 1090000: 4.051423\n",
      "Average loss at step 1092000: 4.051794\n",
      "Average loss at step 1094000: 4.049052\n",
      "Average loss at step 1096000: 4.052689\n",
      "Average loss at step 1098000: 4.052559\n",
      "Average loss at step 1100000: 4.052304\n",
      "Average loss at step 1102000: 4.052994\n",
      "Average loss at step 1104000: 4.053060\n",
      "Average loss at step 1106000: 4.052513\n",
      "Average loss at step 1108000: 4.054149\n",
      "Average loss at step 1110000: 4.051749\n",
      "Average loss at step 1112000: 4.051411\n",
      "Average loss at step 1114000: 4.052414\n",
      "Average loss at step 1116000: 4.049721\n",
      "Average loss at step 1118000: 4.051548\n",
      "Average loss at step 1120000: 4.054003\n",
      "Average loss at step 1122000: 4.050785\n",
      "Average loss at step 1124000: 4.052966\n",
      "Average loss at step 1126000: 4.051270\n",
      "Average loss at step 1128000: 4.051057\n",
      "Average loss at step 1130000: 4.050788\n",
      "Average loss at step 1132000: 4.048973\n",
      "Average loss at step 1134000: 4.051189\n",
      "Average loss at step 1136000: 4.049985\n",
      "Average loss at step 1138000: 4.052040\n",
      "Average loss at step 1140000: 4.050210\n",
      "Average loss at step 1142000: 4.051420\n",
      "Average loss at step 1144000: 4.050239\n",
      "Average loss at step 1146000: 4.050890\n",
      "Average loss at step 1148000: 4.052171\n",
      "Average loss at step 1150000: 4.049001\n",
      "Average loss at step 1152000: 4.050293\n",
      "Average loss at step 1154000: 4.050995\n",
      "Average loss at step 1156000: 4.050350\n",
      "Average loss at step 1158000: 4.049176\n",
      "Average loss at step 1160000: 4.050619\n",
      "Average loss at step 1162000: 4.050550\n",
      "Average loss at step 1164000: 4.048219\n",
      "Average loss at step 1166000: 4.052714\n",
      "Average loss at step 1168000: 4.052014\n",
      "Average loss at step 1170000: 4.051727\n",
      "Average loss at step 1172000: 4.051295\n",
      "Average loss at step 1174000: 4.052414\n",
      "Average loss at step 1176000: 4.049090\n",
      "Average loss at step 1178000: 4.050503\n",
      "Average loss at step 1180000: 4.050814\n",
      "Average loss at step 1182000: 4.051847\n",
      "Average loss at step 1184000: 4.050906\n",
      "Average loss at step 1186000: 4.049801\n",
      "Average loss at step 1188000: 4.050729\n",
      "Average loss at step 1190000: 4.050211\n",
      "Average loss at step 1192000: 4.049105\n",
      "Average loss at step 1194000: 4.048090\n",
      "Average loss at step 1196000: 4.050099\n",
      "Average loss at step 1198000: 4.048338\n",
      "Average loss at step 1200000: 4.049902\n",
      "Average loss at step 1202000: 4.048535\n",
      "Average loss at step 1204000: 4.050427\n",
      "Average loss at step 1206000: 4.048639\n",
      "Average loss at step 1208000: 4.049620\n",
      "Average loss at step 1210000: 4.048893\n",
      "Average loss at step 1212000: 4.051302\n",
      "Average loss at step 1214000: 4.049979\n",
      "Average loss at step 1216000: 4.050830\n",
      "Average loss at step 1218000: 4.049433\n",
      "Average loss at step 1220000: 4.050844\n",
      "Average loss at step 1222000: 4.048988\n",
      "Average loss at step 1224000: 4.049387\n",
      "Average loss at step 1226000: 4.048950\n",
      "Average loss at step 1228000: 4.050020\n",
      "Average loss at step 1230000: 4.050019\n",
      "Average loss at step 1232000: 4.048093\n",
      "Average loss at step 1234000: 4.050332\n",
      "Average loss at step 1236000: 4.052981\n",
      "Average loss at step 1238000: 4.049536\n",
      "Average loss at step 1240000: 4.050709\n",
      "Average loss at step 1242000: 4.051193\n",
      "Average loss at step 1244000: 4.047040\n",
      "Average loss at step 1246000: 4.048844\n",
      "Average loss at step 1248000: 4.050769\n",
      "Average loss at step 1250000: 4.048935\n",
      "Average loss at step 1252000: 4.049299\n",
      "Average loss at step 1254000: 4.050111\n",
      "Average loss at step 1256000: 4.049704\n",
      "Average loss at step 1258000: 4.048739\n",
      "Average loss at step 1260000: 4.049259\n",
      "Average loss at step 1262000: 4.048918\n",
      "Average loss at step 1264000: 4.049266\n",
      "Average loss at step 1266000: 4.048299\n",
      "Average loss at step 1268000: 4.049268\n",
      "Average loss at step 1270000: 4.047915\n",
      "Average loss at step 1272000: 4.046674\n",
      "Average loss at step 1274000: 4.050525\n",
      "Average loss at step 1276000: 4.048397\n",
      "Average loss at step 1278000: 4.051745\n",
      "Average loss at step 1280000: 4.050258\n",
      "Average loss at step 1282000: 4.050482\n",
      "Average loss at step 1284000: 4.048903\n",
      "Average loss at step 1286000: 4.049299\n",
      "Average loss at step 1288000: 4.047625\n",
      "Average loss at step 1290000: 4.048776\n",
      "Average loss at step 1292000: 4.049806\n",
      "Average loss at step 1294000: 4.048890\n",
      "Average loss at step 1296000: 4.049242\n",
      "Average loss at step 1298000: 4.050081\n",
      "Average loss at step 1300000: 4.047396\n",
      "Average loss at step 1302000: 4.048709\n",
      "Average loss at step 1304000: 4.049626\n",
      "Average loss at step 1306000: 4.048477\n",
      "Average loss at step 1308000: 4.051159\n",
      "Average loss at step 1310000: 4.050807\n",
      "Average loss at step 1312000: 4.048958\n",
      "Average loss at step 1314000: 4.049035\n",
      "Average loss at step 1316000: 4.049487\n",
      "Average loss at step 1318000: 4.047486\n",
      "Average loss at step 1320000: 4.048690\n",
      "Average loss at step 1322000: 4.049834\n",
      "Average loss at step 1324000: 4.049436\n",
      "Average loss at step 1326000: 4.050457\n",
      "Average loss at step 1328000: 4.049722\n",
      "Average loss at step 1330000: 4.048034\n",
      "Average loss at step 1332000: 4.048290\n",
      "Average loss at step 1334000: 4.050654\n",
      "Average loss at step 1336000: 4.047303\n",
      "Average loss at step 1338000: 4.050661\n",
      "Average loss at step 1340000: 4.048896\n",
      "Average loss at step 1342000: 4.050037\n",
      "Average loss at step 1344000: 4.048995\n",
      "Average loss at step 1346000: 4.048713\n",
      "Average loss at step 1348000: 4.050072\n",
      "Average loss at step 1350000: 4.048398\n",
      "Average loss at step 1352000: 4.050462\n",
      "Average loss at step 1354000: 4.048315\n",
      "Average loss at step 1356000: 4.048371\n",
      "Average loss at step 1358000: 4.048403\n",
      "Average loss at step 1360000: 4.049646\n",
      "Average loss at step 1362000: 4.049498\n",
      "Average loss at step 1364000: 4.049439\n",
      "Average loss at step 1366000: 4.048001\n",
      "Average loss at step 1368000: 4.048863\n",
      "Average loss at step 1370000: 4.048993\n",
      "Average loss at step 1372000: 4.045484\n",
      "Average loss at step 1374000: 4.047499\n",
      "Average loss at step 1376000: 4.047999\n",
      "Average loss at step 1378000: 4.048356\n",
      "Average loss at step 1380000: 4.050866\n",
      "Average loss at step 1382000: 4.049041\n",
      "Average loss at step 1384000: 4.045340\n",
      "Average loss at step 1386000: 4.048844\n",
      "Average loss at step 1388000: 4.047354\n",
      "Average loss at step 1390000: 4.048908\n",
      "Average loss at step 1392000: 4.050784\n",
      "Average loss at step 1394000: 4.047674\n",
      "Average loss at step 1396000: 4.047736\n",
      "Average loss at step 1398000: 4.047757\n",
      "Average loss at step 1400000: 4.049088\n",
      "Average loss at step 1402000: 4.047519\n",
      "Average loss at step 1404000: 4.046115\n",
      "Average loss at step 1406000: 4.048241\n",
      "Average loss at step 1408000: 4.048901\n",
      "Average loss at step 1410000: 4.047965\n",
      "Average loss at step 1412000: 4.049442\n",
      "Average loss at step 1414000: 4.048366\n",
      "Average loss at step 1416000: 4.045093\n",
      "Average loss at step 1418000: 4.048251\n",
      "Average loss at step 1420000: 4.047177\n",
      "Average loss at step 1422000: 4.048777\n",
      "Average loss at step 1424000: 4.047972\n",
      "Average loss at step 1426000: 4.048482\n",
      "Average loss at step 1428000: 4.045118\n",
      "Average loss at step 1430000: 4.048857\n",
      "Average loss at step 1432000: 4.047481\n",
      "Average loss at step 1434000: 4.046362\n",
      "Average loss at step 1436000: 4.046439\n",
      "Average loss at step 1438000: 4.046505\n",
      "Average loss at step 1440000: 4.047508\n",
      "Average loss at step 1442000: 4.049304\n",
      "Average loss at step 1444000: 4.047315\n",
      "Average loss at step 1446000: 4.048705\n",
      "Average loss at step 1448000: 4.048769\n",
      "Average loss at step 1450000: 4.050022\n",
      "Average loss at step 1452000: 4.049137\n",
      "Average loss at step 1454000: 4.048914\n",
      "Average loss at step 1456000: 4.048376\n",
      "Average loss at step 1458000: 4.046547\n",
      "Average loss at step 1460000: 4.047709\n",
      "Average loss at step 1462000: 4.046954\n",
      "Average loss at step 1464000: 4.046524\n",
      "Average loss at step 1466000: 4.048153\n",
      "Average loss at step 1468000: 4.045775\n",
      "Average loss at step 1470000: 4.046581\n",
      "Average loss at step 1472000: 4.045885\n",
      "Average loss at step 1474000: 4.046146\n",
      "Average loss at step 1476000: 4.046169\n",
      "Average loss at step 1478000: 4.047469\n",
      "Average loss at step 1480000: 4.049010\n",
      "Average loss at step 1482000: 4.047077\n",
      "Average loss at step 1484000: 4.046719\n",
      "Average loss at step 1486000: 4.046956\n",
      "Average loss at step 1488000: 4.046603\n",
      "Average loss at step 1490000: 4.048488\n",
      "Average loss at step 1492000: 4.048795\n",
      "Average loss at step 1494000: 4.048041\n",
      "Average loss at step 1496000: 4.047567\n",
      "Average loss at step 1498000: 4.047510\n",
      "Average loss at step 1500000: 4.045398\n",
      "Average loss at step 1502000: 4.048449\n",
      "Average loss at step 1504000: 4.047894\n",
      "Average loss at step 1506000: 4.045932\n",
      "Average loss at step 1508000: 4.048701\n",
      "Average loss at step 1510000: 4.047306\n",
      "Average loss at step 1512000: 4.045799\n",
      "Average loss at step 1514000: 4.048757\n",
      "Average loss at step 1516000: 4.045985\n",
      "Average loss at step 1518000: 4.045636\n",
      "Average loss at step 1520000: 4.047889\n",
      "Average loss at step 1522000: 4.045652\n",
      "Average loss at step 1524000: 4.046187\n",
      "Average loss at step 1526000: 4.048536\n",
      "Average loss at step 1528000: 4.046957\n",
      "Average loss at step 1530000: 4.047279\n",
      "Average loss at step 1532000: 4.047622\n",
      "Average loss at step 1534000: 4.047360\n",
      "Average loss at step 1536000: 4.047239\n",
      "Average loss at step 1538000: 4.046981\n",
      "Average loss at step 1540000: 4.046897\n",
      "Average loss at step 1542000: 4.047180\n",
      "Average loss at step 1544000: 4.044912\n",
      "Average loss at step 1546000: 4.047680\n",
      "Average loss at step 1548000: 4.047433\n",
      "Average loss at step 1550000: 4.044521\n",
      "Average loss at step 1552000: 4.047810\n",
      "Average loss at step 1554000: 4.047775\n",
      "Average loss at step 1556000: 4.047964\n",
      "Average loss at step 1558000: 4.047256\n",
      "Average loss at step 1560000: 4.046565\n",
      "Average loss at step 1562000: 4.046610\n",
      "Average loss at step 1564000: 4.045920\n",
      "Average loss at step 1566000: 4.048736\n",
      "Average loss at step 1568000: 4.048658\n",
      "Average loss at step 1570000: 4.046364\n",
      "Average loss at step 1572000: 4.047625\n",
      "Average loss at step 1574000: 4.045584\n",
      "Average loss at step 1576000: 4.047221\n",
      "Average loss at step 1578000: 4.045735\n",
      "Average loss at step 1580000: 4.047219\n",
      "Average loss at step 1582000: 4.047056\n",
      "Average loss at step 1584000: 4.045547\n",
      "Average loss at step 1586000: 4.046116\n",
      "Average loss at step 1588000: 4.047464\n",
      "Average loss at step 1590000: 4.045851\n",
      "Average loss at step 1592000: 4.044112\n",
      "Average loss at step 1594000: 4.044181\n",
      "Average loss at step 1596000: 4.047425\n",
      "Average loss at step 1598000: 4.048082\n",
      "Average loss at step 1600000: 4.049504\n",
      "Average loss at step 1602000: 4.044872\n",
      "Average loss at step 1604000: 4.047557\n",
      "Average loss at step 1606000: 4.046279\n",
      "Average loss at step 1608000: 4.046140\n",
      "Average loss at step 1610000: 4.047630\n",
      "Average loss at step 1612000: 4.048460\n",
      "Average loss at step 1614000: 4.044684\n",
      "Average loss at step 1616000: 4.046683\n",
      "Average loss at step 1618000: 4.045891\n",
      "Average loss at step 1620000: 4.047805\n",
      "Average loss at step 1622000: 4.045250\n",
      "Average loss at step 1624000: 4.048911\n",
      "Average loss at step 1626000: 4.045311\n",
      "Average loss at step 1628000: 4.044172\n",
      "Average loss at step 1630000: 4.047260\n",
      "Average loss at step 1632000: 4.045600\n",
      "Average loss at step 1634000: 4.047569\n",
      "Average loss at step 1636000: 4.046392\n",
      "Average loss at step 1638000: 4.046664\n",
      "Average loss at step 1640000: 4.046116\n",
      "Average loss at step 1642000: 4.045940\n",
      "Average loss at step 1644000: 4.043307\n",
      "Average loss at step 1646000: 4.044195\n",
      "Average loss at step 1648000: 4.044731\n",
      "Average loss at step 1650000: 4.046559\n",
      "Average loss at step 1652000: 4.045949\n",
      "Average loss at step 1654000: 4.047928\n",
      "Average loss at step 1656000: 4.046242\n",
      "Average loss at step 1658000: 4.046786\n",
      "Average loss at step 1660000: 4.045524\n",
      "Average loss at step 1662000: 4.044923\n",
      "Average loss at step 1664000: 4.045693\n",
      "Average loss at step 1666000: 4.046347\n",
      "Average loss at step 1668000: 4.046127\n",
      "Average loss at step 1670000: 4.047573\n",
      "Average loss at step 1672000: 4.046135\n",
      "Average loss at step 1674000: 4.045035\n",
      "Average loss at step 1676000: 4.045046\n",
      "Average loss at step 1678000: 4.046336\n",
      "Average loss at step 1680000: 4.046532\n",
      "Average loss at step 1682000: 4.046791\n",
      "Average loss at step 1684000: 4.044906\n",
      "Average loss at step 1686000: 4.045676\n",
      "Average loss at step 1688000: 4.045041\n",
      "Average loss at step 1690000: 4.046036\n",
      "Average loss at step 1692000: 4.046475\n",
      "Average loss at step 1694000: 4.045832\n",
      "Average loss at step 1696000: 4.045161\n",
      "Average loss at step 1698000: 4.044294\n",
      "Average loss at step 1700000: 4.044015\n",
      "Average loss at step 1702000: 4.046641\n",
      "Average loss at step 1704000: 4.046334\n",
      "Average loss at step 1706000: 4.043810\n",
      "Average loss at step 1708000: 4.043861\n",
      "Average loss at step 1710000: 4.044109\n",
      "Average loss at step 1712000: 4.046674\n",
      "Average loss at step 1714000: 4.047784\n",
      "Average loss at step 1716000: 4.046329\n",
      "Average loss at step 1718000: 4.047467\n",
      "Average loss at step 1720000: 4.045753\n",
      "Average loss at step 1722000: 4.046883\n",
      "Average loss at step 1724000: 4.046216\n",
      "Average loss at step 1726000: 4.046534\n",
      "Average loss at step 1728000: 4.045337\n",
      "Average loss at step 1730000: 4.045768\n",
      "Average loss at step 1732000: 4.045853\n",
      "Average loss at step 1734000: 4.045377\n",
      "Average loss at step 1736000: 4.046097\n",
      "Average loss at step 1738000: 4.045621\n",
      "Average loss at step 1740000: 4.047205\n",
      "Average loss at step 1742000: 4.042324\n",
      "Average loss at step 1744000: 4.043153\n",
      "Average loss at step 1746000: 4.044197\n",
      "Average loss at step 1748000: 4.044481\n",
      "Average loss at step 1750000: 4.045194\n",
      "Average loss at step 1752000: 4.047507\n",
      "Average loss at step 1754000: 4.043096\n",
      "Average loss at step 1756000: 4.044188\n",
      "Average loss at step 1758000: 4.045072\n",
      "Average loss at step 1760000: 4.044111\n",
      "Average loss at step 1762000: 4.045927\n",
      "Average loss at step 1764000: 4.045948\n",
      "Average loss at step 1766000: 4.045289\n",
      "Average loss at step 1768000: 4.044168\n",
      "Average loss at step 1770000: 4.044910\n",
      "Average loss at step 1772000: 4.045297\n",
      "Average loss at step 1774000: 4.047520\n",
      "Average loss at step 1776000: 4.042615\n",
      "Average loss at step 1778000: 4.044962\n",
      "Average loss at step 1780000: 4.045584\n",
      "Average loss at step 1782000: 4.047094\n",
      "Average loss at step 1784000: 4.045214\n",
      "Average loss at step 1786000: 4.045699\n",
      "Average loss at step 1788000: 4.046150\n",
      "Average loss at step 1790000: 4.041610\n",
      "Average loss at step 1792000: 4.045778\n",
      "Average loss at step 1794000: 4.044630\n",
      "Average loss at step 1796000: 4.042740\n",
      "Average loss at step 1798000: 4.046446\n",
      "Average loss at step 1800000: 4.042167\n",
      "Average loss at step 1802000: 4.044911\n",
      "Average loss at step 1804000: 4.043797\n",
      "Average loss at step 1806000: 4.045696\n",
      "Average loss at step 1808000: 4.043782\n",
      "Average loss at step 1810000: 4.045711\n",
      "Average loss at step 1812000: 4.045289\n",
      "Average loss at step 1814000: 4.043584\n",
      "Average loss at step 1816000: 4.042536\n",
      "Average loss at step 1818000: 4.045435\n",
      "Average loss at step 1820000: 4.047371\n",
      "Average loss at step 1822000: 4.042763\n",
      "Average loss at step 1824000: 4.045170\n",
      "Average loss at step 1826000: 4.044993\n",
      "Average loss at step 1828000: 4.046910\n",
      "Average loss at step 1830000: 4.043320\n",
      "Average loss at step 1832000: 4.042896\n",
      "Average loss at step 1834000: 4.045231\n",
      "Average loss at step 1836000: 4.042523\n",
      "Average loss at step 1838000: 4.044163\n",
      "Average loss at step 1840000: 4.045641\n",
      "Average loss at step 1842000: 4.045905\n",
      "Average loss at step 1844000: 4.045884\n",
      "Average loss at step 1846000: 4.043950\n",
      "Average loss at step 1848000: 4.045822\n",
      "Average loss at step 1850000: 4.043725\n",
      "Average loss at step 1852000: 4.047790\n",
      "Average loss at step 1854000: 4.042779\n",
      "Average loss at step 1856000: 4.044922\n",
      "Average loss at step 1858000: 4.043229\n",
      "Average loss at step 1860000: 4.045103\n",
      "Average loss at step 1862000: 4.045138\n",
      "Average loss at step 1864000: 4.044049\n",
      "Average loss at step 1866000: 4.045854\n",
      "Average loss at step 1868000: 4.044989\n",
      "Average loss at step 1870000: 4.044759\n",
      "Average loss at step 1872000: 4.042344\n",
      "Average loss at step 1874000: 4.046661\n",
      "Average loss at step 1876000: 4.044214\n",
      "Average loss at step 1878000: 4.044561\n",
      "Average loss at step 1880000: 4.046009\n",
      "Average loss at step 1882000: 4.045427\n",
      "Average loss at step 1884000: 4.045782\n",
      "Average loss at step 1886000: 4.043953\n",
      "Average loss at step 1888000: 4.046524\n",
      "Average loss at step 1890000: 4.043476\n",
      "Average loss at step 1892000: 4.045622\n",
      "Average loss at step 1894000: 4.042211\n",
      "Average loss at step 1896000: 4.044874\n",
      "Average loss at step 1898000: 4.045854\n",
      "Average loss at step 1900000: 4.044586\n",
      "Average loss at step 1902000: 4.044647\n",
      "Average loss at step 1904000: 4.045176\n",
      "Average loss at step 1906000: 4.044608\n",
      "Average loss at step 1908000: 4.044401\n",
      "Average loss at step 1910000: 4.045224\n",
      "Average loss at step 1912000: 4.042383\n",
      "Average loss at step 1914000: 4.042190\n",
      "Average loss at step 1916000: 4.045063\n",
      "Average loss at step 1918000: 4.044086\n",
      "Average loss at step 1920000: 4.043711\n",
      "Average loss at step 1922000: 4.043824\n",
      "Average loss at step 1924000: 4.043292\n",
      "Average loss at step 1926000: 4.043860\n",
      "Average loss at step 1928000: 4.042242\n",
      "Average loss at step 1930000: 4.045030\n",
      "Average loss at step 1932000: 4.044447\n",
      "Average loss at step 1934000: 4.041841\n",
      "Average loss at step 1936000: 4.044862\n",
      "Average loss at step 1938000: 4.046559\n",
      "Average loss at step 1940000: 4.044014\n",
      "Average loss at step 1942000: 4.042925\n",
      "Average loss at step 1944000: 4.042941\n",
      "Average loss at step 1946000: 4.041347\n",
      "Average loss at step 1948000: 4.044301\n",
      "Average loss at step 1950000: 4.043594\n",
      "Average loss at step 1952000: 4.045372\n",
      "Average loss at step 1954000: 4.044253\n",
      "Average loss at step 1956000: 4.044859\n",
      "Average loss at step 1958000: 4.042560\n",
      "Average loss at step 1960000: 4.044161\n",
      "Average loss at step 1962000: 4.043360\n",
      "Average loss at step 1964000: 4.043786\n",
      "Average loss at step 1966000: 4.044126\n",
      "Average loss at step 1968000: 4.042809\n",
      "Average loss at step 1970000: 4.043872\n",
      "Average loss at step 1972000: 4.042238\n",
      "Average loss at step 1974000: 4.045196\n",
      "Average loss at step 1976000: 4.041720\n",
      "Average loss at step 1978000: 4.044220\n",
      "Average loss at step 1980000: 4.041828\n",
      "Average loss at step 1982000: 4.041769\n",
      "Average loss at step 1984000: 4.044008\n",
      "Average loss at step 1986000: 4.043055\n",
      "Average loss at step 1988000: 4.044497\n",
      "Average loss at step 1990000: 4.044687\n",
      "Average loss at step 1992000: 4.044182\n",
      "Average loss at step 1994000: 4.044112\n",
      "Average loss at step 1996000: 4.047456\n",
      "Average loss at step 1998000: 4.043191\n",
      "Average loss at step 2000000: 4.045441\n",
      "Average loss at step 2002000: 4.044678\n",
      "Average loss at step 2004000: 4.042159\n",
      "Average loss at step 2006000: 4.043442\n",
      "Average loss at step 2008000: 4.042378\n",
      "Average loss at step 2010000: 4.045225\n",
      "Average loss at step 2012000: 4.043028\n",
      "Average loss at step 2014000: 4.043657\n",
      "Average loss at step 2016000: 4.043966\n",
      "Average loss at step 2018000: 4.044095\n",
      "Average loss at step 2020000: 4.041167\n",
      "Average loss at step 2022000: 4.042822\n",
      "Average loss at step 2024000: 4.043596\n",
      "Average loss at step 2026000: 4.043472\n",
      "Average loss at step 2028000: 4.040576\n",
      "Average loss at step 2030000: 4.043650\n",
      "Average loss at step 2032000: 4.042775\n",
      "Average loss at step 2034000: 4.043932\n",
      "Average loss at step 2036000: 4.046306\n",
      "Average loss at step 2038000: 4.044309\n",
      "Average loss at step 2040000: 4.042276\n",
      "Average loss at step 2042000: 4.044052\n",
      "Average loss at step 2044000: 4.042880\n",
      "Average loss at step 2046000: 4.043939\n",
      "Average loss at step 2048000: 4.041760\n",
      "Average loss at step 2050000: 4.043982\n",
      "Average loss at step 2052000: 4.042849\n",
      "Average loss at step 2054000: 4.045900\n",
      "Average loss at step 2056000: 4.043008\n",
      "Average loss at step 2058000: 4.044624\n",
      "Average loss at step 2060000: 4.046849\n",
      "Average loss at step 2062000: 4.041983\n",
      "Average loss at step 2064000: 4.042565\n",
      "Average loss at step 2066000: 4.044526\n",
      "Average loss at step 2068000: 4.041849\n",
      "Average loss at step 2070000: 4.042511\n",
      "Average loss at step 2072000: 4.044237\n",
      "Average loss at step 2074000: 4.045423\n",
      "Average loss at step 2076000: 4.043052\n",
      "Average loss at step 2078000: 4.043900\n",
      "Average loss at step 2080000: 4.041792\n",
      "Average loss at step 2082000: 4.044127\n",
      "Average loss at step 2084000: 4.044437\n",
      "Average loss at step 2086000: 4.042744\n",
      "Average loss at step 2088000: 4.042745\n",
      "Average loss at step 2090000: 4.041196\n",
      "Average loss at step 2092000: 4.044728\n",
      "Average loss at step 2094000: 4.041309\n",
      "Average loss at step 2096000: 4.040618\n",
      "Average loss at step 2098000: 4.044674\n",
      "Average loss at step 2100000: 4.044804\n",
      "Average loss at step 2102000: 4.043078\n",
      "Average loss at step 2104000: 4.043294\n",
      "Average loss at step 2106000: 4.043195\n",
      "Average loss at step 2108000: 4.042549\n",
      "Average loss at step 2110000: 4.042186\n",
      "Average loss at step 2112000: 4.044141\n",
      "Average loss at step 2114000: 4.044814\n",
      "Average loss at step 2116000: 4.042935\n",
      "Average loss at step 2118000: 4.041011\n",
      "Average loss at step 2120000: 4.042548\n",
      "Average loss at step 2122000: 4.042880\n",
      "Average loss at step 2124000: 4.043879\n",
      "Average loss at step 2126000: 4.044239\n",
      "Average loss at step 2128000: 4.042068\n",
      "Average loss at step 2130000: 4.042630\n",
      "Average loss at step 2132000: 4.042058\n",
      "Average loss at step 2134000: 4.045899\n",
      "Average loss at step 2136000: 4.043835\n",
      "Average loss at step 2138000: 4.044509\n",
      "Average loss at step 2140000: 4.043200\n",
      "Average loss at step 2142000: 4.043442\n",
      "Average loss at step 2144000: 4.043423\n",
      "Average loss at step 2146000: 4.042908\n",
      "Average loss at step 2148000: 4.042353\n",
      "Average loss at step 2150000: 4.042764\n",
      "Average loss at step 2152000: 4.043847\n",
      "Average loss at step 2154000: 4.041894\n",
      "Average loss at step 2156000: 4.044810\n",
      "Average loss at step 2158000: 4.043575\n",
      "Average loss at step 2160000: 4.043171\n",
      "Average loss at step 2162000: 4.041087\n",
      "Average loss at step 2164000: 4.043552\n",
      "Average loss at step 2166000: 4.041902\n",
      "Average loss at step 2168000: 4.041826\n",
      "Average loss at step 2170000: 4.045912\n",
      "Average loss at step 2172000: 4.041670\n",
      "Average loss at step 2174000: 4.042891\n",
      "Average loss at step 2176000: 4.042499\n",
      "Average loss at step 2178000: 4.042319\n",
      "Average loss at step 2180000: 4.043043\n",
      "Average loss at step 2182000: 4.042915\n",
      "Average loss at step 2184000: 4.044155\n",
      "Average loss at step 2186000: 4.042401\n",
      "Average loss at step 2188000: 4.043651\n",
      "Average loss at step 2190000: 4.042427\n",
      "Average loss at step 2192000: 4.041769\n",
      "Average loss at step 2194000: 4.043691\n",
      "Average loss at step 2196000: 4.041937\n",
      "Average loss at step 2198000: 4.041954\n",
      "Average loss at step 2200000: 4.044360\n",
      "Average loss at step 2202000: 4.042530\n",
      "Average loss at step 2204000: 4.042029\n",
      "Average loss at step 2206000: 4.042790\n",
      "Average loss at step 2208000: 4.043359\n",
      "Average loss at step 2210000: 4.040334\n",
      "Average loss at step 2212000: 4.042014\n",
      "Average loss at step 2214000: 4.041220\n",
      "Average loss at step 2216000: 4.041804\n",
      "Average loss at step 2218000: 4.042380\n",
      "Average loss at step 2220000: 4.043599\n",
      "Average loss at step 2222000: 4.043938\n",
      "Average loss at step 2224000: 4.043161\n",
      "Average loss at step 2226000: 4.041408\n",
      "Average loss at step 2228000: 4.044070\n",
      "Average loss at step 2230000: 4.043473\n",
      "Average loss at step 2232000: 4.041606\n",
      "Average loss at step 2234000: 4.041703\n",
      "Average loss at step 2236000: 4.040473\n",
      "Average loss at step 2238000: 4.043191\n",
      "Average loss at step 2240000: 4.042857\n",
      "Average loss at step 2242000: 4.041316\n",
      "Average loss at step 2244000: 4.040120\n",
      "Average loss at step 2246000: 4.042880\n",
      "Average loss at step 2248000: 4.040642\n",
      "Average loss at step 2250000: 4.043347\n",
      "Average loss at step 2252000: 4.039775\n",
      "Average loss at step 2254000: 4.040519\n",
      "Average loss at step 2256000: 4.041912\n",
      "Average loss at step 2258000: 4.040099\n",
      "Average loss at step 2260000: 4.042499\n",
      "Average loss at step 2262000: 4.043005\n",
      "Average loss at step 2264000: 4.043849\n",
      "Average loss at step 2266000: 4.042943\n",
      "Average loss at step 2268000: 4.042772\n",
      "Average loss at step 2270000: 4.042328\n",
      "Average loss at step 2272000: 4.042596\n",
      "Average loss at step 2274000: 4.044773\n",
      "Average loss at step 2276000: 4.041902\n",
      "Average loss at step 2278000: 4.040993\n",
      "Average loss at step 2280000: 4.042259\n",
      "Average loss at step 2282000: 4.040580\n",
      "Average loss at step 2284000: 4.043514\n",
      "Average loss at step 2286000: 4.042378\n",
      "Average loss at step 2288000: 4.040017\n",
      "Average loss at step 2290000: 4.040415\n",
      "Average loss at step 2292000: 4.041331\n",
      "Average loss at step 2294000: 4.042640\n",
      "Average loss at step 2296000: 4.041822\n",
      "Average loss at step 2298000: 4.041709\n",
      "Average loss at step 2300000: 4.039212\n",
      "Average loss at step 2302000: 4.041928\n",
      "Average loss at step 2304000: 4.039747\n",
      "Average loss at step 2306000: 4.042891\n",
      "Average loss at step 2308000: 4.043522\n",
      "Average loss at step 2310000: 4.041331\n",
      "Average loss at step 2312000: 4.042654\n",
      "Average loss at step 2314000: 4.040269\n",
      "Average loss at step 2316000: 4.040350\n",
      "Average loss at step 2318000: 4.042249\n",
      "Average loss at step 2320000: 4.041517\n",
      "Average loss at step 2322000: 4.039338\n",
      "Average loss at step 2324000: 4.041835\n",
      "Average loss at step 2326000: 4.042687\n",
      "Average loss at step 2328000: 4.043547\n",
      "Average loss at step 2330000: 4.041640\n",
      "Average loss at step 2332000: 4.043325\n",
      "Average loss at step 2334000: 4.040861\n",
      "Average loss at step 2336000: 4.039993\n",
      "Average loss at step 2338000: 4.041947\n",
      "Average loss at step 2340000: 4.039684\n",
      "Average loss at step 2342000: 4.040084\n",
      "Average loss at step 2344000: 4.041868\n",
      "Average loss at step 2346000: 4.042784\n",
      "Average loss at step 2348000: 4.041407\n",
      "Average loss at step 2350000: 4.042151\n",
      "Average loss at step 2352000: 4.042085\n",
      "Average loss at step 2354000: 4.040381\n",
      "Average loss at step 2356000: 4.040901\n",
      "Average loss at step 2358000: 4.042723\n",
      "Average loss at step 2360000: 4.039388\n",
      "Average loss at step 2362000: 4.040235\n",
      "Average loss at step 2364000: 4.041143\n",
      "Average loss at step 2366000: 4.042736\n",
      "Average loss at step 2368000: 4.041244\n",
      "Average loss at step 2370000: 4.042063\n",
      "Average loss at step 2372000: 4.041271\n",
      "Average loss at step 2374000: 4.042475\n",
      "Average loss at step 2376000: 4.042406\n",
      "Average loss at step 2378000: 4.041414\n",
      "Average loss at step 2380000: 4.038719\n",
      "Average loss at step 2382000: 4.041610\n",
      "Average loss at step 2384000: 4.040287\n",
      "Average loss at step 2386000: 4.042941\n",
      "Average loss at step 2388000: 4.041096\n",
      "Average loss at step 2390000: 4.039170\n",
      "Average loss at step 2392000: 4.040316\n",
      "Average loss at step 2394000: 4.042363\n",
      "Average loss at step 2396000: 4.041352\n",
      "Average loss at step 2398000: 4.041693\n",
      "Average loss at step 2400000: 4.038535\n",
      "Average loss at step 2402000: 4.039895\n",
      "Average loss at step 2404000: 4.039612\n",
      "Average loss at step 2406000: 4.042629\n",
      "Average loss at step 2408000: 4.042568\n",
      "Average loss at step 2410000: 4.041437\n",
      "Average loss at step 2412000: 4.040629\n",
      "Average loss at step 2414000: 4.041809\n",
      "Average loss at step 2416000: 4.042643\n",
      "Average loss at step 2418000: 4.041371\n",
      "Average loss at step 2420000: 4.041632\n",
      "Average loss at step 2422000: 4.042435\n",
      "Average loss at step 2424000: 4.041582\n",
      "Average loss at step 2426000: 4.041151\n",
      "Average loss at step 2428000: 4.041846\n",
      "Average loss at step 2430000: 4.045502\n",
      "Average loss at step 2432000: 4.040295\n",
      "Average loss at step 2434000: 4.040972\n",
      "Average loss at step 2436000: 4.040543\n",
      "Average loss at step 2438000: 4.043240\n",
      "Average loss at step 2440000: 4.041753\n",
      "Average loss at step 2442000: 4.042937\n",
      "Average loss at step 2444000: 4.040434\n",
      "Average loss at step 2446000: 4.040795\n",
      "Average loss at step 2448000: 4.042492\n",
      "Average loss at step 2450000: 4.042458\n",
      "Average loss at step 2452000: 4.042512\n",
      "Average loss at step 2454000: 4.041097\n",
      "Average loss at step 2456000: 4.040593\n",
      "Average loss at step 2458000: 4.038783\n",
      "Average loss at step 2460000: 4.040916\n",
      "Average loss at step 2462000: 4.039126\n",
      "Average loss at step 2464000: 4.039613\n",
      "Average loss at step 2466000: 4.042692\n",
      "Average loss at step 2468000: 4.043191\n",
      "Average loss at step 2470000: 4.041603\n",
      "Average loss at step 2472000: 4.041348\n",
      "Average loss at step 2474000: 4.041046\n",
      "Average loss at step 2476000: 4.043589\n",
      "Average loss at step 2478000: 4.040940\n",
      "Average loss at step 2480000: 4.040287\n",
      "Average loss at step 2482000: 4.039543\n",
      "Average loss at step 2484000: 4.040707\n",
      "Average loss at step 2486000: 4.040566\n",
      "Average loss at step 2488000: 4.040652\n",
      "Average loss at step 2490000: 4.041581\n",
      "Average loss at step 2492000: 4.042052\n",
      "Average loss at step 2494000: 4.040834\n",
      "Average loss at step 2496000: 4.041570\n",
      "Average loss at step 2498000: 4.040719\n",
      "Average loss at step 2500000: 4.040868\n",
      "Average loss at step 2502000: 4.042986\n",
      "Average loss at step 2504000: 4.042503\n",
      "Average loss at step 2506000: 4.040135\n",
      "Average loss at step 2508000: 4.040313\n",
      "Average loss at step 2510000: 4.042881\n",
      "Average loss at step 2512000: 4.041435\n",
      "Average loss at step 2514000: 4.039071\n",
      "Average loss at step 2516000: 4.040719\n",
      "Average loss at step 2518000: 4.039004\n",
      "Average loss at step 2520000: 4.041922\n",
      "Average loss at step 2522000: 4.041120\n",
      "Average loss at step 2524000: 4.039956\n",
      "Average loss at step 2526000: 4.040450\n",
      "Average loss at step 2528000: 4.039351\n",
      "Average loss at step 2530000: 4.041130\n",
      "Average loss at step 2532000: 4.041371\n",
      "Average loss at step 2534000: 4.041481\n",
      "Average loss at step 2536000: 4.038727\n",
      "Average loss at step 2538000: 4.041445\n",
      "Average loss at step 2540000: 4.041778\n",
      "Average loss at step 2542000: 4.041449\n",
      "Average loss at step 2544000: 4.042334\n",
      "Average loss at step 2546000: 4.042304\n",
      "Average loss at step 2548000: 4.043120\n",
      "Average loss at step 2550000: 4.039831\n",
      "Average loss at step 2552000: 4.041495\n",
      "Average loss at step 2554000: 4.040646\n",
      "Average loss at step 2556000: 4.040970\n",
      "Average loss at step 2558000: 4.043231\n",
      "Average loss at step 2560000: 4.039722\n",
      "Average loss at step 2562000: 4.039953\n",
      "Average loss at step 2564000: 4.039967\n",
      "Average loss at step 2566000: 4.040091\n",
      "Average loss at step 2568000: 4.042577\n",
      "Average loss at step 2570000: 4.040658\n",
      "Average loss at step 2572000: 4.040063\n",
      "Average loss at step 2574000: 4.039551\n",
      "Average loss at step 2576000: 4.043177\n",
      "Average loss at step 2578000: 4.038312\n",
      "Average loss at step 2580000: 4.044567\n",
      "Average loss at step 2582000: 4.040759\n",
      "Average loss at step 2584000: 4.040903\n",
      "Average loss at step 2586000: 4.040710\n",
      "Average loss at step 2588000: 4.040193\n",
      "Average loss at step 2590000: 4.040522\n",
      "Average loss at step 2592000: 4.040452\n",
      "Average loss at step 2594000: 4.041150\n",
      "Average loss at step 2596000: 4.041542\n",
      "Average loss at step 2598000: 4.042399\n",
      "Average loss at step 2600000: 4.042119\n",
      "Average loss at step 2602000: 4.040539\n",
      "Average loss at step 2604000: 4.040216\n",
      "Average loss at step 2606000: 4.042069\n",
      "Average loss at step 2608000: 4.040537\n",
      "Average loss at step 2610000: 4.038904\n",
      "Average loss at step 2612000: 4.040403\n",
      "Average loss at step 2614000: 4.037898\n",
      "Average loss at step 2616000: 4.042810\n",
      "Average loss at step 2618000: 4.040101\n",
      "Average loss at step 2620000: 4.041706\n",
      "Average loss at step 2622000: 4.042300\n",
      "Average loss at step 2624000: 4.041214\n",
      "Average loss at step 2626000: 4.039566\n",
      "Average loss at step 2628000: 4.043430\n",
      "Average loss at step 2630000: 4.040486\n",
      "Average loss at step 2632000: 4.038887\n",
      "Average loss at step 2634000: 4.039020\n",
      "Average loss at step 2636000: 4.041076\n",
      "Average loss at step 2638000: 4.043696\n",
      "Average loss at step 2640000: 4.037359\n",
      "Average loss at step 2642000: 4.040544\n",
      "Average loss at step 2644000: 4.039986\n",
      "Average loss at step 2646000: 4.043412\n",
      "Average loss at step 2648000: 4.042028\n",
      "Average loss at step 2650000: 4.039656\n",
      "Average loss at step 2652000: 4.041039\n",
      "Average loss at step 2654000: 4.040302\n",
      "Average loss at step 2656000: 4.040574\n",
      "Average loss at step 2658000: 4.040713\n",
      "Average loss at step 2660000: 4.042173\n",
      "Average loss at step 2662000: 4.040721\n",
      "Average loss at step 2664000: 4.038639\n",
      "Average loss at step 2666000: 4.043379\n",
      "Average loss at step 2668000: 4.040136\n",
      "Average loss at step 2670000: 4.040811\n",
      "Average loss at step 2672000: 4.038703\n",
      "Average loss at step 2674000: 4.040377\n",
      "Average loss at step 2676000: 4.040536\n",
      "Average loss at step 2678000: 4.040629\n",
      "Average loss at step 2680000: 4.040613\n",
      "Average loss at step 2682000: 4.038908\n",
      "Average loss at step 2684000: 4.043210\n",
      "Average loss at step 2686000: 4.039482\n",
      "Average loss at step 2688000: 4.041269\n",
      "Average loss at step 2690000: 4.040526\n",
      "Average loss at step 2692000: 4.041873\n",
      "Average loss at step 2694000: 4.041269\n",
      "Average loss at step 2696000: 4.039869\n",
      "Average loss at step 2698000: 4.044927\n",
      "Average loss at step 2700000: 4.041293\n",
      "Average loss at step 2702000: 4.039931\n",
      "Average loss at step 2704000: 4.040968\n",
      "Average loss at step 2706000: 4.040540\n",
      "Average loss at step 2708000: 4.038080\n",
      "Average loss at step 2710000: 4.041793\n",
      "Average loss at step 2712000: 4.039921\n",
      "Average loss at step 2714000: 4.041553\n",
      "Average loss at step 2716000: 4.042587\n",
      "Average loss at step 2718000: 4.036359\n",
      "Average loss at step 2720000: 4.040337\n",
      "Average loss at step 2722000: 4.041425\n",
      "Average loss at step 2724000: 4.041099\n",
      "Average loss at step 2726000: 4.040110\n",
      "Average loss at step 2728000: 4.040048\n",
      "Average loss at step 2730000: 4.037848\n",
      "Average loss at step 2732000: 4.038875\n",
      "Average loss at step 2734000: 4.041532\n",
      "Average loss at step 2736000: 4.040367\n",
      "Average loss at step 2738000: 4.039177\n",
      "Average loss at step 2740000: 4.041437\n",
      "Average loss at step 2742000: 4.038810\n",
      "Average loss at step 2744000: 4.041512\n",
      "Average loss at step 2746000: 4.037932\n",
      "Average loss at step 2748000: 4.039962\n",
      "Average loss at step 2750000: 4.038844\n",
      "Average loss at step 2752000: 4.039943\n",
      "Average loss at step 2754000: 4.037954\n",
      "Average loss at step 2756000: 4.038294\n",
      "Average loss at step 2758000: 4.039456\n",
      "Average loss at step 2760000: 4.039288\n",
      "Average loss at step 2762000: 4.040086\n",
      "Average loss at step 2764000: 4.041053\n",
      "Average loss at step 2766000: 4.039460\n",
      "Average loss at step 2768000: 4.038627\n",
      "Average loss at step 2770000: 4.039298\n",
      "Average loss at step 2772000: 4.038783\n",
      "Average loss at step 2774000: 4.040236\n",
      "Average loss at step 2776000: 4.041557\n",
      "Average loss at step 2778000: 4.039211\n",
      "Average loss at step 2780000: 4.038644\n",
      "Average loss at step 2782000: 4.038361\n",
      "Average loss at step 2784000: 4.041148\n",
      "Average loss at step 2786000: 4.038439\n",
      "Average loss at step 2788000: 4.039259\n",
      "Average loss at step 2790000: 4.040637\n",
      "Average loss at step 2792000: 4.039832\n",
      "Average loss at step 2794000: 4.041493\n",
      "Average loss at step 2796000: 4.039676\n",
      "Average loss at step 2798000: 4.040394\n",
      "Average loss at step 2800000: 4.037582\n",
      "Average loss at step 2802000: 4.040153\n",
      "Average loss at step 2804000: 4.038252\n",
      "Average loss at step 2806000: 4.039751\n",
      "Average loss at step 2808000: 4.041067\n",
      "Average loss at step 2810000: 4.040989\n",
      "Average loss at step 2812000: 4.041001\n",
      "Average loss at step 2814000: 4.040082\n",
      "Average loss at step 2816000: 4.039040\n",
      "Average loss at step 2818000: 4.041758\n",
      "Average loss at step 2820000: 4.041924\n",
      "Average loss at step 2822000: 4.040917\n",
      "Average loss at step 2824000: 4.038620\n",
      "Average loss at step 2826000: 4.039366\n",
      "Average loss at step 2828000: 4.042114\n",
      "Average loss at step 2830000: 4.039948\n",
      "Average loss at step 2832000: 4.038514\n",
      "Average loss at step 2834000: 4.038842\n",
      "Average loss at step 2836000: 4.036977\n",
      "Average loss at step 2838000: 4.040279\n",
      "Average loss at step 2840000: 4.041019\n",
      "Average loss at step 2842000: 4.040121\n",
      "Average loss at step 2844000: 4.040840\n",
      "Average loss at step 2846000: 4.039433\n",
      "Average loss at step 2848000: 4.037567\n",
      "Average loss at step 2850000: 4.039865\n",
      "Average loss at step 2852000: 4.041902\n",
      "Average loss at step 2854000: 4.041757\n",
      "Average loss at step 2856000: 4.039609\n",
      "Average loss at step 2858000: 4.039258\n",
      "Average loss at step 2860000: 4.040926\n",
      "Average loss at step 2862000: 4.040852\n",
      "Average loss at step 2864000: 4.039711\n",
      "Average loss at step 2866000: 4.039641\n",
      "Average loss at step 2868000: 4.039431\n",
      "Average loss at step 2870000: 4.039202\n",
      "Average loss at step 2872000: 4.042768\n",
      "Average loss at step 2874000: 4.039270\n",
      "Average loss at step 2876000: 4.039814\n",
      "Average loss at step 2878000: 4.041763\n",
      "Average loss at step 2880000: 4.039941\n",
      "Average loss at step 2882000: 4.040408\n",
      "Average loss at step 2884000: 4.038962\n",
      "Average loss at step 2886000: 4.037382\n",
      "Average loss at step 2888000: 4.038501\n",
      "Average loss at step 2890000: 4.039715\n",
      "Average loss at step 2892000: 4.038879\n",
      "Average loss at step 2894000: 4.041387\n",
      "Average loss at step 2896000: 4.038894\n",
      "Average loss at step 2898000: 4.040703\n",
      "Average loss at step 2900000: 4.038175\n",
      "Average loss at step 2902000: 4.041458\n",
      "Average loss at step 2904000: 4.040633\n",
      "Average loss at step 2906000: 4.038896\n",
      "Average loss at step 2908000: 4.037928\n",
      "Average loss at step 2910000: 4.042990\n",
      "Average loss at step 2912000: 4.038041\n",
      "Average loss at step 2914000: 4.037285\n",
      "Average loss at step 2916000: 4.039022\n",
      "Average loss at step 2918000: 4.040263\n",
      "Average loss at step 2920000: 4.036587\n",
      "Average loss at step 2922000: 4.040750\n",
      "Average loss at step 2924000: 4.040221\n",
      "Average loss at step 2926000: 4.038688\n",
      "Average loss at step 2928000: 4.038063\n",
      "Average loss at step 2930000: 4.040137\n",
      "Average loss at step 2932000: 4.040614\n",
      "Average loss at step 2934000: 4.038486\n",
      "Average loss at step 2936000: 4.039222\n",
      "Average loss at step 2938000: 4.039627\n",
      "Average loss at step 2940000: 4.038301\n",
      "Average loss at step 2942000: 4.038554\n",
      "Average loss at step 2944000: 4.038824\n",
      "Average loss at step 2946000: 4.037545\n",
      "Average loss at step 2948000: 4.037366\n",
      "Average loss at step 2950000: 4.040204\n",
      "Average loss at step 2952000: 4.040794\n",
      "Average loss at step 2954000: 4.041864\n",
      "Average loss at step 2956000: 4.039389\n",
      "Average loss at step 2958000: 4.040899\n",
      "Average loss at step 2960000: 4.039226\n",
      "Average loss at step 2962000: 4.039266\n",
      "Average loss at step 2964000: 4.039965\n",
      "Average loss at step 2966000: 4.039374\n",
      "Average loss at step 2968000: 4.037625\n",
      "Average loss at step 2970000: 4.039454\n",
      "Average loss at step 2972000: 4.039814\n",
      "Average loss at step 2974000: 4.039924\n",
      "Average loss at step 2976000: 4.040387\n",
      "Average loss at step 2978000: 4.039580\n",
      "Average loss at step 2980000: 4.038079\n",
      "Average loss at step 2982000: 4.039460\n",
      "Average loss at step 2984000: 4.040637\n",
      "Average loss at step 2986000: 4.038622\n",
      "Average loss at step 2988000: 4.039728\n",
      "Average loss at step 2990000: 4.038095\n",
      "Average loss at step 2992000: 4.037921\n",
      "Average loss at step 2994000: 4.043262\n",
      "Average loss at step 2996000: 4.038886\n",
      "Average loss at step 2998000: 4.036801\n",
      "Average loss at step 3000000: 4.041838\n",
      "Average loss at step 3002000: 4.038174\n",
      "Average loss at step 3004000: 4.040766\n",
      "Average loss at step 3006000: 4.040386\n",
      "Average loss at step 3008000: 4.040448\n",
      "Average loss at step 3010000: 4.039100\n",
      "Average loss at step 3012000: 4.041737\n",
      "Average loss at step 3014000: 4.040455\n",
      "Average loss at step 3016000: 4.040718\n",
      "Average loss at step 3018000: 4.039552\n",
      "Average loss at step 3020000: 4.038415\n",
      "Average loss at step 3022000: 4.039400\n",
      "Average loss at step 3024000: 4.037896\n",
      "Average loss at step 3026000: 4.039080\n",
      "Average loss at step 3028000: 4.039827\n",
      "Average loss at step 3030000: 4.038214\n",
      "Average loss at step 3032000: 4.040389\n",
      "Average loss at step 3034000: 4.037188\n",
      "Average loss at step 3036000: 4.038193\n",
      "Average loss at step 3038000: 4.038134\n",
      "Average loss at step 3040000: 4.039113\n",
      "Average loss at step 3042000: 4.039293\n",
      "Average loss at step 3044000: 4.038743\n",
      "Average loss at step 3046000: 4.040513\n",
      "Average loss at step 3048000: 4.040561\n",
      "Average loss at step 3050000: 4.038638\n",
      "Average loss at step 3052000: 4.038553\n",
      "Average loss at step 3054000: 4.038597\n",
      "Average loss at step 3056000: 4.039221\n",
      "Average loss at step 3058000: 4.040322\n",
      "Average loss at step 3060000: 4.037652\n",
      "Average loss at step 3062000: 4.037061\n",
      "Average loss at step 3064000: 4.037984\n",
      "Average loss at step 3066000: 4.038657\n",
      "Average loss at step 3068000: 4.039054\n",
      "Average loss at step 3070000: 4.038131\n",
      "Average loss at step 3072000: 4.037230\n",
      "Average loss at step 3074000: 4.038768\n",
      "Average loss at step 3076000: 4.038941\n",
      "Average loss at step 3078000: 4.040175\n",
      "Average loss at step 3080000: 4.040798\n",
      "Average loss at step 3082000: 4.038249\n",
      "Average loss at step 3084000: 4.038619\n",
      "Average loss at step 3086000: 4.040170\n",
      "Average loss at step 3088000: 4.036464\n",
      "Average loss at step 3090000: 4.039372\n",
      "Average loss at step 3092000: 4.041484\n",
      "Average loss at step 3094000: 4.040534\n",
      "Average loss at step 3096000: 4.038057\n",
      "Average loss at step 3098000: 4.038624\n",
      "Average loss at step 3100000: 4.037724\n",
      "Average loss at step 3102000: 4.040990\n",
      "Average loss at step 3104000: 4.040151\n",
      "Average loss at step 3106000: 4.038457\n",
      "Average loss at step 3108000: 4.037529\n",
      "Average loss at step 3110000: 4.037725\n",
      "Average loss at step 3112000: 4.036362\n",
      "Average loss at step 3114000: 4.041211\n",
      "Average loss at step 3116000: 4.039498\n",
      "Average loss at step 3118000: 4.037483\n",
      "Average loss at step 3120000: 4.037286\n",
      "Average loss at step 3122000: 4.037410\n",
      "Average loss at step 3124000: 4.039470\n",
      "Average loss at step 3126000: 4.040279\n",
      "Average loss at step 3128000: 4.039469\n",
      "Average loss at step 3130000: 4.039582\n",
      "Average loss at step 3132000: 4.037034\n",
      "Average loss at step 3134000: 4.037520\n",
      "Average loss at step 3136000: 4.037488\n",
      "Average loss at step 3138000: 4.041637\n",
      "Average loss at step 3140000: 4.036591\n",
      "Average loss at step 3142000: 4.038134\n",
      "Average loss at step 3144000: 4.038757\n",
      "Average loss at step 3146000: 4.041339\n",
      "Average loss at step 3148000: 4.039635\n",
      "Average loss at step 3150000: 4.039691\n",
      "Average loss at step 3152000: 4.036861\n",
      "Average loss at step 3154000: 4.038518\n",
      "Average loss at step 3156000: 4.037585\n",
      "Average loss at step 3158000: 4.038298\n",
      "Average loss at step 3160000: 4.035244\n",
      "Average loss at step 3162000: 4.039677\n",
      "Average loss at step 3164000: 4.039098\n",
      "Average loss at step 3166000: 4.039607\n",
      "Average loss at step 3168000: 4.039023\n",
      "Average loss at step 3170000: 4.038272\n",
      "Average loss at step 3172000: 4.038369\n",
      "Average loss at step 3174000: 4.036560\n",
      "Average loss at step 3176000: 4.039055\n",
      "Average loss at step 3178000: 4.039127\n",
      "Average loss at step 3180000: 4.038243\n",
      "Average loss at step 3182000: 4.038628\n",
      "Average loss at step 3184000: 4.039056\n",
      "Average loss at step 3186000: 4.035903\n",
      "Average loss at step 3188000: 4.040484\n",
      "Average loss at step 3190000: 4.038856\n",
      "Average loss at step 3192000: 4.038640\n",
      "Average loss at step 3194000: 4.039961\n",
      "Average loss at step 3196000: 4.039792\n",
      "Average loss at step 3198000: 4.036737\n",
      "Average loss at step 3200000: 4.038377\n",
      "Average loss at step 3202000: 4.039787\n",
      "Average loss at step 3204000: 4.041329\n",
      "Average loss at step 3206000: 4.038856\n",
      "Average loss at step 3208000: 4.037610\n",
      "Average loss at step 3210000: 4.036706\n",
      "Average loss at step 3212000: 4.039850\n",
      "Average loss at step 3214000: 4.035967\n",
      "Average loss at step 3216000: 4.038850\n",
      "Average loss at step 3218000: 4.035988\n",
      "Average loss at step 3220000: 4.038382\n",
      "Average loss at step 3222000: 4.038190\n",
      "Average loss at step 3224000: 4.039191\n",
      "Average loss at step 3226000: 4.039755\n",
      "Average loss at step 3228000: 4.037661\n",
      "Average loss at step 3230000: 4.037929\n",
      "Average loss at step 3232000: 4.039496\n",
      "Average loss at step 3234000: 4.039131\n",
      "Average loss at step 3236000: 4.039289\n",
      "Average loss at step 3238000: 4.037786\n",
      "Average loss at step 3240000: 4.037634\n",
      "Average loss at step 3242000: 4.038832\n",
      "Average loss at step 3244000: 4.039424\n",
      "Average loss at step 3246000: 4.038452\n",
      "Average loss at step 3248000: 4.040841\n",
      "Average loss at step 3250000: 4.037542\n",
      "Average loss at step 3252000: 4.040585\n",
      "Average loss at step 3254000: 4.038277\n",
      "Average loss at step 3256000: 4.039731\n",
      "Average loss at step 3258000: 4.037410\n",
      "Average loss at step 3260000: 4.041841\n",
      "Average loss at step 3262000: 4.037994\n",
      "Average loss at step 3264000: 4.037637\n",
      "Average loss at step 3266000: 4.037840\n",
      "Average loss at step 3268000: 4.036943\n",
      "Average loss at step 3270000: 4.038866\n",
      "Average loss at step 3272000: 4.038590\n",
      "Average loss at step 3274000: 4.037416\n",
      "Average loss at step 3276000: 4.036869\n",
      "Average loss at step 3278000: 4.039101\n",
      "Average loss at step 3280000: 4.039545\n",
      "Average loss at step 3282000: 4.036587\n",
      "Average loss at step 3284000: 4.039525\n",
      "Average loss at step 3286000: 4.039733\n",
      "Average loss at step 3288000: 4.037296\n",
      "Average loss at step 3290000: 4.038514\n",
      "Average loss at step 3292000: 4.036691\n",
      "Average loss at step 3294000: 4.038913\n",
      "Average loss at step 3296000: 4.036617\n",
      "Average loss at step 3298000: 4.039478\n",
      "Average loss at step 3300000: 4.037398\n",
      "Average loss at step 3302000: 4.037942\n",
      "Average loss at step 3304000: 4.036292\n",
      "Average loss at step 3306000: 4.039816\n",
      "Average loss at step 3308000: 4.037178\n",
      "Average loss at step 3310000: 4.039926\n",
      "Average loss at step 3312000: 4.040646\n",
      "Average loss at step 3314000: 4.035886\n",
      "Average loss at step 3316000: 4.037134\n",
      "Average loss at step 3318000: 4.036703\n",
      "Average loss at step 3320000: 4.040858\n",
      "Average loss at step 3322000: 4.036268\n",
      "Average loss at step 3324000: 4.038002\n",
      "Average loss at step 3326000: 4.038256\n",
      "Average loss at step 3328000: 4.038118\n",
      "Average loss at step 3330000: 4.037709\n",
      "Average loss at step 3332000: 4.039112\n",
      "Average loss at step 3334000: 4.037243\n",
      "Average loss at step 3336000: 4.038405\n",
      "Average loss at step 3338000: 4.038412\n",
      "Average loss at step 3340000: 4.038703\n",
      "Average loss at step 3342000: 4.037749\n",
      "Average loss at step 3344000: 4.037545\n",
      "Average loss at step 3346000: 4.035911\n",
      "Average loss at step 3348000: 4.037392\n",
      "Average loss at step 3350000: 4.038243\n",
      "Average loss at step 3352000: 4.035214\n",
      "Average loss at step 3354000: 4.035671\n",
      "Average loss at step 3356000: 4.041213\n",
      "Average loss at step 3358000: 4.038628\n",
      "Average loss at step 3360000: 4.039859\n",
      "Average loss at step 3362000: 4.038885\n",
      "Average loss at step 3364000: 4.038164\n",
      "Average loss at step 3366000: 4.039071\n",
      "Average loss at step 3368000: 4.037821\n",
      "Average loss at step 3370000: 4.041383\n",
      "Average loss at step 3372000: 4.038406\n",
      "Average loss at step 3374000: 4.040186\n",
      "Average loss at step 3376000: 4.040111\n",
      "Average loss at step 3378000: 4.039755\n",
      "Average loss at step 3380000: 4.036370\n",
      "Average loss at step 3382000: 4.034814\n",
      "Average loss at step 3384000: 4.037591\n",
      "Average loss at step 3386000: 4.040083\n",
      "Average loss at step 3388000: 4.038375\n",
      "Average loss at step 3390000: 4.038202\n",
      "Average loss at step 3392000: 4.033453\n",
      "Average loss at step 3394000: 4.037730\n",
      "Average loss at step 3396000: 4.038887\n",
      "Average loss at step 3398000: 4.039605\n",
      "Average loss at step 3400000: 4.038990\n",
      "Average loss at step 3402000: 4.037171\n",
      "Average loss at step 3404000: 4.037039\n",
      "Average loss at step 3406000: 4.036920\n",
      "Average loss at step 3408000: 4.037741\n",
      "Average loss at step 3410000: 4.039520\n",
      "Average loss at step 3412000: 4.036091\n",
      "Average loss at step 3414000: 4.037893\n",
      "Average loss at step 3416000: 4.038513\n",
      "Average loss at step 3418000: 4.040121\n",
      "Average loss at step 3420000: 4.036163\n",
      "Average loss at step 3422000: 4.036449\n",
      "Average loss at step 3424000: 4.041410\n",
      "Average loss at step 3426000: 4.036710\n",
      "Average loss at step 3428000: 4.037614\n",
      "Average loss at step 3430000: 4.036987\n",
      "Average loss at step 3432000: 4.034761\n",
      "Average loss at step 3434000: 4.039362\n",
      "Average loss at step 3436000: 4.037385\n",
      "Average loss at step 3438000: 4.038528\n",
      "Average loss at step 3440000: 4.038983\n",
      "Average loss at step 3442000: 4.035331\n",
      "Average loss at step 3444000: 4.038116\n",
      "Average loss at step 3446000: 4.039652\n",
      "Average loss at step 3448000: 4.037340\n",
      "Average loss at step 3450000: 4.039522\n",
      "Average loss at step 3452000: 4.037241\n",
      "Average loss at step 3454000: 4.036646\n",
      "Average loss at step 3456000: 4.039205\n",
      "Average loss at step 3458000: 4.036394\n",
      "Average loss at step 3460000: 4.036435\n",
      "Average loss at step 3462000: 4.038180\n",
      "Average loss at step 3464000: 4.039123\n",
      "Average loss at step 3466000: 4.037366\n",
      "Average loss at step 3468000: 4.036224\n",
      "Average loss at step 3470000: 4.037917\n",
      "Average loss at step 3472000: 4.039133\n",
      "Average loss at step 3474000: 4.039863\n",
      "Average loss at step 3476000: 4.036839\n",
      "Average loss at step 3478000: 4.036661\n",
      "Average loss at step 3480000: 4.036800\n",
      "Average loss at step 3482000: 4.034782\n",
      "Average loss at step 3484000: 4.038698\n",
      "Average loss at step 3486000: 4.039630\n",
      "Average loss at step 3488000: 4.038498\n",
      "Average loss at step 3490000: 4.034667\n",
      "Average loss at step 3492000: 4.035615\n",
      "Average loss at step 3494000: 4.036378\n",
      "Average loss at step 3496000: 4.039754\n",
      "Average loss at step 3498000: 4.039254\n",
      "Average loss at step 3500000: 4.038077\n",
      "Average loss at step 3502000: 4.039991\n",
      "Average loss at step 3504000: 4.037035\n",
      "Average loss at step 3506000: 4.041059\n",
      "Average loss at step 3508000: 4.035950\n",
      "Average loss at step 3510000: 4.041081\n",
      "Average loss at step 3512000: 4.036170\n",
      "Average loss at step 3514000: 4.037560\n",
      "Average loss at step 3516000: 4.040070\n",
      "Average loss at step 3518000: 4.036737\n",
      "Average loss at step 3520000: 4.039479\n",
      "Average loss at step 3522000: 4.037949\n",
      "Average loss at step 3524000: 4.037415\n",
      "Average loss at step 3526000: 4.036776\n",
      "Average loss at step 3528000: 4.039961\n",
      "Average loss at step 3530000: 4.037871\n",
      "Average loss at step 3532000: 4.040462\n",
      "Average loss at step 3534000: 4.038205\n",
      "Average loss at step 3536000: 4.037175\n",
      "Average loss at step 3538000: 4.038607\n",
      "Average loss at step 3540000: 4.038620\n",
      "Average loss at step 3542000: 4.037465\n",
      "Average loss at step 3544000: 4.037911\n",
      "Average loss at step 3546000: 4.038393\n",
      "Average loss at step 3548000: 4.034922\n",
      "Average loss at step 3550000: 4.035925\n",
      "Average loss at step 3552000: 4.040283\n",
      "Average loss at step 3554000: 4.037238\n",
      "Average loss at step 3556000: 4.037640\n",
      "Average loss at step 3558000: 4.039180\n",
      "Average loss at step 3560000: 4.035195\n",
      "Average loss at step 3562000: 4.039904\n",
      "Average loss at step 3564000: 4.038028\n",
      "Average loss at step 3566000: 4.037064\n",
      "Average loss at step 3568000: 4.038772\n",
      "Average loss at step 3570000: 4.036135\n",
      "Average loss at step 3572000: 4.037120\n",
      "Average loss at step 3574000: 4.036890\n",
      "Average loss at step 3576000: 4.037116\n",
      "Average loss at step 3578000: 4.036682\n",
      "Average loss at step 3580000: 4.038218\n",
      "Average loss at step 3582000: 4.036204\n",
      "Average loss at step 3584000: 4.037021\n",
      "Average loss at step 3586000: 4.036693\n",
      "Average loss at step 3588000: 4.035856\n",
      "Average loss at step 3590000: 4.035778\n",
      "Average loss at step 3592000: 4.040079\n",
      "Average loss at step 3594000: 4.036677\n",
      "Average loss at step 3596000: 4.037889\n",
      "Average loss at step 3598000: 4.038170\n",
      "Average loss at step 3600000: 4.037100\n",
      "Average loss at step 3602000: 4.035939\n",
      "Average loss at step 3604000: 4.037462\n",
      "Average loss at step 3606000: 4.034637\n",
      "Average loss at step 3608000: 4.037679\n",
      "Average loss at step 3610000: 4.038443\n",
      "Average loss at step 3612000: 4.039500\n",
      "Average loss at step 3614000: 4.039551\n",
      "Average loss at step 3616000: 4.036793\n",
      "Average loss at step 3618000: 4.034470\n",
      "Average loss at step 3620000: 4.036013\n",
      "Average loss at step 3622000: 4.036169\n",
      "Average loss at step 3624000: 4.037397\n",
      "Average loss at step 3626000: 4.038486\n",
      "Average loss at step 3628000: 4.040557\n",
      "Average loss at step 3630000: 4.038237\n",
      "Average loss at step 3632000: 4.037724\n",
      "Average loss at step 3634000: 4.035504\n",
      "Average loss at step 3636000: 4.036039\n",
      "Average loss at step 3638000: 4.039748\n",
      "Average loss at step 3640000: 4.037744\n",
      "Average loss at step 3642000: 4.038322\n",
      "Average loss at step 3644000: 4.037214\n",
      "Average loss at step 3646000: 4.038398\n",
      "Average loss at step 3648000: 4.036528\n",
      "Average loss at step 3650000: 4.037202\n",
      "Average loss at step 3652000: 4.036497\n",
      "Average loss at step 3654000: 4.038162\n",
      "Average loss at step 3656000: 4.036992\n",
      "Average loss at step 3658000: 4.034971\n",
      "Average loss at step 3660000: 4.037007\n",
      "Average loss at step 3662000: 4.037310\n",
      "Average loss at step 3664000: 4.035442\n",
      "Average loss at step 3666000: 4.036088\n",
      "Average loss at step 3668000: 4.038533\n",
      "Average loss at step 3670000: 4.037406\n",
      "Average loss at step 3672000: 4.037835\n",
      "Average loss at step 3674000: 4.037374\n",
      "Average loss at step 3676000: 4.034487\n",
      "Average loss at step 3678000: 4.036351\n",
      "Average loss at step 3680000: 4.039225\n",
      "Average loss at step 3682000: 4.036509\n",
      "Average loss at step 3684000: 4.038559\n",
      "Average loss at step 3686000: 4.035730\n",
      "Average loss at step 3688000: 4.037167\n",
      "Average loss at step 3690000: 4.038741\n",
      "Average loss at step 3692000: 4.039606\n",
      "Average loss at step 3694000: 4.037088\n",
      "Average loss at step 3696000: 4.038935\n",
      "Average loss at step 3698000: 4.035327\n",
      "Average loss at step 3700000: 4.038029\n",
      "Average loss at step 3702000: 4.037872\n",
      "Average loss at step 3704000: 4.033895\n",
      "Average loss at step 3706000: 4.035858\n",
      "Average loss at step 3708000: 4.038106\n",
      "Average loss at step 3710000: 4.036547\n",
      "Average loss at step 3712000: 4.037550\n",
      "Average loss at step 3714000: 4.036564\n",
      "Average loss at step 3716000: 4.036811\n",
      "Average loss at step 3718000: 4.037321\n",
      "Average loss at step 3720000: 4.037185\n",
      "Average loss at step 3722000: 4.038511\n",
      "Average loss at step 3724000: 4.034898\n",
      "Average loss at step 3726000: 4.035383\n",
      "Average loss at step 3728000: 4.038920\n",
      "Average loss at step 3730000: 4.036722\n",
      "Average loss at step 3732000: 4.034382\n",
      "Average loss at step 3734000: 4.036253\n",
      "Average loss at step 3736000: 4.037412\n",
      "Average loss at step 3738000: 4.039204\n",
      "Average loss at step 3740000: 4.039533\n",
      "Average loss at step 3742000: 4.036519\n",
      "Average loss at step 3744000: 4.036286\n",
      "Average loss at step 3746000: 4.038531\n",
      "Average loss at step 3748000: 4.035628\n",
      "Average loss at step 3750000: 4.036065\n",
      "Average loss at step 3752000: 4.034588\n",
      "Average loss at step 3754000: 4.035478\n",
      "Average loss at step 3756000: 4.036710\n",
      "Average loss at step 3758000: 4.035358\n",
      "Average loss at step 3760000: 4.037652\n",
      "Average loss at step 3762000: 4.037632\n",
      "Average loss at step 3764000: 4.035312\n",
      "Average loss at step 3766000: 4.037122\n",
      "Average loss at step 3768000: 4.036562\n",
      "Average loss at step 3770000: 4.037017\n",
      "Average loss at step 3772000: 4.039214\n",
      "Average loss at step 3774000: 4.039095\n",
      "Average loss at step 3776000: 4.038771\n",
      "Average loss at step 3778000: 4.036853\n",
      "Average loss at step 3780000: 4.036423\n",
      "Average loss at step 3782000: 4.038554\n",
      "Average loss at step 3784000: 4.036798\n",
      "Average loss at step 3786000: 4.037155\n",
      "Average loss at step 3788000: 4.037435\n",
      "Average loss at step 3790000: 4.038881\n",
      "Average loss at step 3792000: 4.035686\n",
      "Average loss at step 3794000: 4.038817\n",
      "Average loss at step 3796000: 4.039284\n",
      "Average loss at step 3798000: 4.036223\n",
      "Average loss at step 3800000: 4.037117\n",
      "Average loss at step 3802000: 4.037880\n",
      "Average loss at step 3804000: 4.038092\n",
      "Average loss at step 3806000: 4.038409\n",
      "Average loss at step 3808000: 4.036831\n",
      "Average loss at step 3810000: 4.036660\n",
      "Average loss at step 3812000: 4.037602\n",
      "Average loss at step 3814000: 4.037061\n",
      "Average loss at step 3816000: 4.036922\n",
      "Average loss at step 3818000: 4.035284\n",
      "Average loss at step 3820000: 4.035831\n",
      "Average loss at step 3822000: 4.035348\n",
      "Average loss at step 3824000: 4.038452\n",
      "Average loss at step 3826000: 4.035465\n",
      "Average loss at step 3828000: 4.036357\n",
      "Average loss at step 3830000: 4.037950\n",
      "Average loss at step 3832000: 4.037084\n",
      "Average loss at step 3834000: 4.036030\n",
      "Average loss at step 3836000: 4.039045\n",
      "Average loss at step 3838000: 4.034082\n",
      "Average loss at step 3840000: 4.036249\n",
      "Average loss at step 3842000: 4.035498\n",
      "Average loss at step 3844000: 4.035312\n",
      "Average loss at step 3846000: 4.039171\n",
      "Average loss at step 3848000: 4.036999\n",
      "Average loss at step 3850000: 4.037502\n",
      "Average loss at step 3852000: 4.037872\n",
      "Average loss at step 3854000: 4.038854\n",
      "Average loss at step 3856000: 4.036897\n",
      "Average loss at step 3858000: 4.036288\n",
      "Average loss at step 3860000: 4.036793\n",
      "Average loss at step 3862000: 4.035333\n",
      "Average loss at step 3864000: 4.037285\n",
      "Average loss at step 3866000: 4.038124\n",
      "Average loss at step 3868000: 4.036768\n",
      "Average loss at step 3870000: 4.037236\n",
      "Average loss at step 3872000: 4.036088\n",
      "Average loss at step 3874000: 4.035013\n",
      "Average loss at step 3876000: 4.037616\n",
      "Average loss at step 3878000: 4.035506\n",
      "Average loss at step 3880000: 4.035920\n",
      "Average loss at step 3882000: 4.035021\n",
      "Average loss at step 3884000: 4.036938\n",
      "Average loss at step 3886000: 4.035770\n",
      "Average loss at step 3888000: 4.035835\n",
      "Average loss at step 3890000: 4.034437\n",
      "Average loss at step 3892000: 4.036080\n",
      "Average loss at step 3894000: 4.037509\n",
      "Average loss at step 3896000: 4.035947\n",
      "Average loss at step 3898000: 4.036668\n",
      "Average loss at step 3900000: 4.038695\n",
      "Average loss at step 3902000: 4.037878\n",
      "Average loss at step 3904000: 4.040718\n",
      "Average loss at step 3906000: 4.036730\n",
      "Average loss at step 3908000: 4.036315\n",
      "Average loss at step 3910000: 4.039335\n",
      "Average loss at step 3912000: 4.037945\n",
      "Average loss at step 3914000: 4.035309\n",
      "Average loss at step 3916000: 4.036639\n",
      "Average loss at step 3918000: 4.036847\n",
      "Average loss at step 3920000: 4.039740\n",
      "Average loss at step 3922000: 4.037800\n",
      "Average loss at step 3924000: 4.036363\n",
      "Average loss at step 3926000: 4.038007\n",
      "Average loss at step 3928000: 4.035106\n",
      "Average loss at step 3930000: 4.034229\n",
      "Average loss at step 3932000: 4.036769\n",
      "Average loss at step 3934000: 4.038559\n",
      "Average loss at step 3936000: 4.035793\n",
      "Average loss at step 3938000: 4.034854\n",
      "Average loss at step 3940000: 4.036289\n",
      "Average loss at step 3942000: 4.035582\n",
      "Average loss at step 3944000: 4.036850\n",
      "Average loss at step 3946000: 4.034596\n",
      "Average loss at step 3948000: 4.037167\n",
      "Average loss at step 3950000: 4.038107\n",
      "Average loss at step 3952000: 4.036609\n",
      "Average loss at step 3954000: 4.036558\n",
      "Average loss at step 3956000: 4.038401\n",
      "Average loss at step 3958000: 4.034816\n",
      "Average loss at step 3960000: 4.034743\n",
      "Average loss at step 3962000: 4.037836\n",
      "Average loss at step 3964000: 4.037911\n",
      "Average loss at step 3966000: 4.037567\n",
      "Average loss at step 3968000: 4.037250\n",
      "Average loss at step 3970000: 4.039311\n",
      "Average loss at step 3972000: 4.036637\n",
      "Average loss at step 3974000: 4.036524\n",
      "Average loss at step 3976000: 4.036197\n",
      "Average loss at step 3978000: 4.033334\n",
      "Average loss at step 3980000: 4.036387\n",
      "Average loss at step 3982000: 4.038129\n",
      "Average loss at step 3984000: 4.036862\n",
      "Average loss at step 3986000: 4.036670\n",
      "Average loss at step 3988000: 4.037306\n",
      "Average loss at step 3990000: 4.034256\n",
      "Average loss at step 3992000: 4.038674\n",
      "Average loss at step 3994000: 4.038404\n",
      "Average loss at step 3996000: 4.034964\n",
      "Average loss at step 3998000: 4.037748\n",
      "Average loss at step 4000000: 4.038235\n",
      "Average loss at step 4002000: 4.035863\n",
      "Average loss at step 4004000: 4.033456\n",
      "Average loss at step 4006000: 4.036408\n",
      "Average loss at step 4008000: 4.036590\n",
      "Average loss at step 4010000: 4.038061\n",
      "Average loss at step 4012000: 4.036002\n",
      "Average loss at step 4014000: 4.036799\n",
      "Average loss at step 4016000: 4.036038\n",
      "Average loss at step 4018000: 4.036004\n",
      "Average loss at step 4020000: 4.037088\n",
      "Average loss at step 4022000: 4.035126\n",
      "Average loss at step 4024000: 4.035896\n",
      "Average loss at step 4026000: 4.037945\n",
      "Average loss at step 4028000: 4.033315\n",
      "Average loss at step 4030000: 4.038742\n",
      "Average loss at step 4032000: 4.037704\n",
      "Average loss at step 4034000: 4.034984\n",
      "Average loss at step 4036000: 4.035356\n",
      "Average loss at step 4038000: 4.035624\n",
      "Average loss at step 4040000: 4.035044\n",
      "Average loss at step 4042000: 4.037905\n",
      "Average loss at step 4044000: 4.039631\n",
      "Average loss at step 4046000: 4.037172\n",
      "Average loss at step 4048000: 4.037352\n",
      "Average loss at step 4050000: 4.037292\n",
      "Average loss at step 4052000: 4.038797\n",
      "Average loss at step 4054000: 4.035477\n",
      "Average loss at step 4056000: 4.036644\n",
      "Average loss at step 4058000: 4.035238\n",
      "Average loss at step 4060000: 4.039355\n",
      "Average loss at step 4062000: 4.038168\n",
      "Average loss at step 4064000: 4.035689\n",
      "Average loss at step 4066000: 4.037360\n",
      "Average loss at step 4068000: 4.036840\n",
      "Average loss at step 4070000: 4.036752\n",
      "Average loss at step 4072000: 4.034195\n",
      "Average loss at step 4074000: 4.038644\n",
      "Average loss at step 4076000: 4.037349\n",
      "Average loss at step 4078000: 4.039561\n",
      "Average loss at step 4080000: 4.035641\n",
      "Average loss at step 4082000: 4.036981\n",
      "Average loss at step 4084000: 4.037985\n",
      "Average loss at step 4086000: 4.037228\n",
      "Average loss at step 4088000: 4.036556\n",
      "Average loss at step 4090000: 4.035134\n",
      "Average loss at step 4092000: 4.036402\n",
      "Average loss at step 4094000: 4.033575\n",
      "Average loss at step 4096000: 4.037590\n",
      "Average loss at step 4098000: 4.035527\n",
      "Average loss at step 4100000: 4.035532\n",
      "Average loss at step 4102000: 4.036449\n",
      "Average loss at step 4104000: 4.039367\n",
      "Average loss at step 4106000: 4.034920\n",
      "Average loss at step 4108000: 4.037548\n",
      "Average loss at step 4110000: 4.035066\n",
      "Average loss at step 4112000: 4.037679\n",
      "Average loss at step 4114000: 4.036034\n",
      "Average loss at step 4116000: 4.037571\n",
      "Average loss at step 4118000: 4.035353\n",
      "Average loss at step 4120000: 4.037158\n",
      "Average loss at step 4122000: 4.036101\n",
      "Average loss at step 4124000: 4.037121\n",
      "Average loss at step 4126000: 4.036220\n",
      "Average loss at step 4128000: 4.037801\n",
      "Average loss at step 4130000: 4.036881\n",
      "Average loss at step 4132000: 4.037751\n",
      "Average loss at step 4134000: 4.034807\n",
      "Average loss at step 4136000: 4.034885\n",
      "Average loss at step 4138000: 4.034222\n",
      "Average loss at step 4140000: 4.036947\n",
      "Average loss at step 4142000: 4.036355\n",
      "Average loss at step 4144000: 4.034701\n",
      "Average loss at step 4146000: 4.035977\n",
      "Average loss at step 4148000: 4.037976\n",
      "Average loss at step 4150000: 4.035247\n",
      "Average loss at step 4152000: 4.034229\n",
      "Average loss at step 4154000: 4.035706\n",
      "Average loss at step 4156000: 4.037677\n",
      "Average loss at step 4158000: 4.037755\n",
      "Average loss at step 4160000: 4.036506\n",
      "Average loss at step 4162000: 4.036707\n",
      "Average loss at step 4164000: 4.034938\n",
      "Average loss at step 4166000: 4.035432\n",
      "Average loss at step 4168000: 4.037172\n",
      "Average loss at step 4170000: 4.038438\n",
      "Average loss at step 4172000: 4.035332\n",
      "Average loss at step 4174000: 4.037291\n",
      "Average loss at step 4176000: 4.036025\n",
      "Average loss at step 4178000: 4.037898\n",
      "Average loss at step 4180000: 4.037433\n",
      "Average loss at step 4182000: 4.038217\n",
      "Average loss at step 4184000: 4.038472\n",
      "Average loss at step 4186000: 4.035881\n",
      "Average loss at step 4188000: 4.036095\n",
      "Average loss at step 4190000: 4.036652\n",
      "Average loss at step 4192000: 4.039682\n",
      "Average loss at step 4194000: 4.036082\n",
      "Average loss at step 4196000: 4.037890\n",
      "Average loss at step 4198000: 4.035342\n",
      "Average loss at step 4200000: 4.034630\n",
      "Average loss at step 4202000: 4.034629\n",
      "Average loss at step 4204000: 4.035501\n",
      "Average loss at step 4206000: 4.035404\n",
      "Average loss at step 4208000: 4.037030\n",
      "Average loss at step 4210000: 4.033215\n",
      "Average loss at step 4212000: 4.036595\n",
      "Average loss at step 4214000: 4.035902\n",
      "Average loss at step 4216000: 4.037208\n",
      "Average loss at step 4218000: 4.035457\n",
      "Average loss at step 4220000: 4.036685\n",
      "Average loss at step 4222000: 4.037069\n",
      "Average loss at step 4224000: 4.037022\n",
      "Average loss at step 4226000: 4.033320\n",
      "Average loss at step 4228000: 4.036469\n",
      "Average loss at step 4230000: 4.034758\n",
      "Average loss at step 4232000: 4.035681\n",
      "Average loss at step 4234000: 4.036702\n",
      "Average loss at step 4236000: 4.038715\n",
      "Average loss at step 4238000: 4.035514\n",
      "Average loss at step 4240000: 4.036471\n",
      "Average loss at step 4242000: 4.040448\n",
      "Average loss at step 4244000: 4.033861\n",
      "Average loss at step 4246000: 4.035910\n",
      "Average loss at step 4248000: 4.037306\n",
      "Average loss at step 4250000: 4.031634\n",
      "Average loss at step 4252000: 4.038107\n",
      "Average loss at step 4254000: 4.035946\n",
      "Average loss at step 4256000: 4.037871\n",
      "Average loss at step 4258000: 4.035674\n",
      "Average loss at step 4260000: 4.035881\n",
      "Average loss at step 4262000: 4.033189\n",
      "Average loss at step 4264000: 4.037342\n",
      "Average loss at step 4266000: 4.036950\n",
      "Average loss at step 4268000: 4.036063\n",
      "Average loss at step 4270000: 4.035302\n",
      "Average loss at step 4272000: 4.034916\n",
      "Average loss at step 4274000: 4.038404\n",
      "Average loss at step 4276000: 4.031626\n",
      "Average loss at step 4278000: 4.034870\n",
      "Average loss at step 4280000: 4.034925\n",
      "Average loss at step 4282000: 4.036280\n",
      "Average loss at step 4284000: 4.035938\n",
      "Average loss at step 4286000: 4.036161\n",
      "Average loss at step 4288000: 4.038253\n",
      "Average loss at step 4290000: 4.036120\n",
      "Average loss at step 4292000: 4.035757\n",
      "Average loss at step 4294000: 4.036252\n",
      "Average loss at step 4296000: 4.036892\n",
      "Average loss at step 4298000: 4.035964\n",
      "Average loss at step 4300000: 4.034070\n",
      "Average loss at step 4302000: 4.037501\n",
      "Average loss at step 4304000: 4.035790\n",
      "Average loss at step 4306000: 4.037787\n",
      "Average loss at step 4308000: 4.035660\n",
      "Average loss at step 4310000: 4.034359\n",
      "Average loss at step 4312000: 4.034715\n",
      "Average loss at step 4314000: 4.035149\n",
      "Average loss at step 4316000: 4.037491\n",
      "Average loss at step 4318000: 4.035064\n",
      "Average loss at step 4320000: 4.035206\n",
      "Average loss at step 4322000: 4.037051\n",
      "Average loss at step 4324000: 4.036616\n",
      "Average loss at step 4326000: 4.035156\n",
      "Average loss at step 4328000: 4.038220\n",
      "Average loss at step 4330000: 4.035471\n",
      "Average loss at step 4332000: 4.035458\n",
      "Average loss at step 4334000: 4.039027\n",
      "Average loss at step 4336000: 4.035069\n",
      "Average loss at step 4338000: 4.038434\n",
      "Average loss at step 4340000: 4.037451\n",
      "Average loss at step 4342000: 4.037987\n",
      "Average loss at step 4344000: 4.034174\n",
      "Average loss at step 4346000: 4.034687\n",
      "Average loss at step 4348000: 4.036289\n",
      "Average loss at step 4350000: 4.035879\n",
      "Average loss at step 4352000: 4.037683\n",
      "Average loss at step 4354000: 4.035973\n",
      "Average loss at step 4356000: 4.036846\n",
      "Average loss at step 4358000: 4.037648\n",
      "Average loss at step 4360000: 4.036999\n",
      "Average loss at step 4362000: 4.034234\n",
      "Average loss at step 4364000: 4.036193\n",
      "Average loss at step 4366000: 4.034535\n",
      "Average loss at step 4368000: 4.035368\n",
      "Average loss at step 4370000: 4.035348\n",
      "Average loss at step 4372000: 4.034652\n",
      "Average loss at step 4374000: 4.036218\n",
      "Average loss at step 4376000: 4.037559\n",
      "Average loss at step 4378000: 4.035911\n",
      "Average loss at step 4380000: 4.037008\n",
      "Average loss at step 4382000: 4.035731\n",
      "Average loss at step 4384000: 4.033795\n",
      "Average loss at step 4386000: 4.036069\n",
      "Average loss at step 4388000: 4.037124\n",
      "Average loss at step 4390000: 4.037058\n",
      "Average loss at step 4392000: 4.036172\n",
      "Average loss at step 4394000: 4.035501\n",
      "Average loss at step 4396000: 4.035734\n",
      "Average loss at step 4398000: 4.036476\n",
      "Average loss at step 4400000: 4.035317\n",
      "Average loss at step 4402000: 4.036524\n",
      "Average loss at step 4404000: 4.035781\n",
      "Average loss at step 4406000: 4.037277\n",
      "Average loss at step 4408000: 4.035243\n",
      "Average loss at step 4410000: 4.035638\n",
      "Average loss at step 4412000: 4.036958\n",
      "Average loss at step 4414000: 4.035842\n",
      "Average loss at step 4416000: 4.031734\n",
      "Average loss at step 4418000: 4.035918\n",
      "Average loss at step 4420000: 4.033096\n",
      "Average loss at step 4422000: 4.036107\n",
      "Average loss at step 4424000: 4.035106\n",
      "Average loss at step 4426000: 4.035088\n",
      "Average loss at step 4428000: 4.036498\n",
      "Average loss at step 4430000: 4.039076\n",
      "Average loss at step 4432000: 4.036103\n",
      "Average loss at step 4434000: 4.034068\n",
      "Average loss at step 4436000: 4.036420\n",
      "Average loss at step 4438000: 4.034650\n",
      "Average loss at step 4440000: 4.035025\n",
      "Average loss at step 4442000: 4.040423\n",
      "Average loss at step 4444000: 4.035288\n",
      "Average loss at step 4446000: 4.037277\n",
      "Average loss at step 4448000: 4.037247\n",
      "Average loss at step 4450000: 4.036475\n",
      "Average loss at step 4452000: 4.036569\n",
      "Average loss at step 4454000: 4.036659\n",
      "Average loss at step 4456000: 4.038273\n",
      "Average loss at step 4458000: 4.036804\n",
      "Average loss at step 4460000: 4.035055\n",
      "Average loss at step 4462000: 4.036242\n",
      "Average loss at step 4464000: 4.035139\n",
      "Average loss at step 4466000: 4.037041\n",
      "Average loss at step 4468000: 4.035227\n",
      "Average loss at step 4470000: 4.035252\n",
      "Average loss at step 4472000: 4.033624\n",
      "Average loss at step 4474000: 4.034002\n",
      "Average loss at step 4476000: 4.035193\n",
      "Average loss at step 4478000: 4.035982\n",
      "Average loss at step 4480000: 4.036881\n",
      "Average loss at step 4482000: 4.033504\n",
      "Average loss at step 4484000: 4.037009\n",
      "Average loss at step 4486000: 4.037746\n",
      "Average loss at step 4488000: 4.035753\n",
      "Average loss at step 4490000: 4.037258\n",
      "Average loss at step 4492000: 4.037463\n",
      "Average loss at step 4494000: 4.034281\n",
      "Average loss at step 4496000: 4.035705\n",
      "Average loss at step 4498000: 4.035306\n",
      "Average loss at step 4500000: 4.034305\n",
      "Average loss at step 4502000: 4.035780\n",
      "Average loss at step 4504000: 4.033948\n",
      "Average loss at step 4506000: 4.033744\n",
      "Average loss at step 4508000: 4.036105\n",
      "Average loss at step 4510000: 4.035503\n",
      "Average loss at step 4512000: 4.036057\n",
      "Average loss at step 4514000: 4.034540\n",
      "Average loss at step 4516000: 4.035202\n",
      "Average loss at step 4518000: 4.034178\n",
      "Average loss at step 4520000: 4.034493\n",
      "Average loss at step 4522000: 4.033866\n",
      "Average loss at step 4524000: 4.034549\n",
      "Average loss at step 4526000: 4.037835\n",
      "Average loss at step 4528000: 4.035894\n",
      "Average loss at step 4530000: 4.036139\n",
      "Average loss at step 4532000: 4.035290\n",
      "Average loss at step 4534000: 4.035172\n",
      "Average loss at step 4536000: 4.037210\n",
      "Average loss at step 4538000: 4.036236\n",
      "Average loss at step 4540000: 4.033915\n",
      "Average loss at step 4542000: 4.034349\n",
      "Average loss at step 4544000: 4.033791\n",
      "Average loss at step 4546000: 4.036768\n",
      "Average loss at step 4548000: 4.036757\n",
      "Average loss at step 4550000: 4.034849\n",
      "Average loss at step 4552000: 4.036495\n",
      "Average loss at step 4554000: 4.035019\n",
      "Average loss at step 4556000: 4.035781\n",
      "Average loss at step 4558000: 4.037830\n",
      "Average loss at step 4560000: 4.036105\n",
      "Average loss at step 4562000: 4.036465\n",
      "Average loss at step 4564000: 4.036317\n",
      "Average loss at step 4566000: 4.036310\n",
      "Average loss at step 4568000: 4.038200\n",
      "Average loss at step 4570000: 4.036115\n",
      "Average loss at step 4572000: 4.034667\n",
      "Average loss at step 4574000: 4.035054\n",
      "Average loss at step 4576000: 4.035173\n",
      "Average loss at step 4578000: 4.036583\n",
      "Average loss at step 4580000: 4.035856\n",
      "Average loss at step 4582000: 4.034864\n",
      "Average loss at step 4584000: 4.036098\n",
      "Average loss at step 4586000: 4.035910\n",
      "Average loss at step 4588000: 4.036124\n",
      "Average loss at step 4590000: 4.037271\n",
      "Average loss at step 4592000: 4.036027\n",
      "Average loss at step 4594000: 4.034513\n",
      "Average loss at step 4596000: 4.036895\n",
      "Average loss at step 4598000: 4.034880\n",
      "Average loss at step 4600000: 4.036444\n",
      "Average loss at step 4602000: 4.035844\n",
      "Average loss at step 4604000: 4.033554\n",
      "Average loss at step 4606000: 4.035998\n",
      "Average loss at step 4608000: 4.035085\n",
      "Average loss at step 4610000: 4.035440\n",
      "Average loss at step 4612000: 4.036854\n",
      "Average loss at step 4614000: 4.037325\n",
      "Average loss at step 4616000: 4.034863\n",
      "Average loss at step 4618000: 4.034491\n",
      "Average loss at step 4620000: 4.035901\n",
      "Average loss at step 4622000: 4.035270\n",
      "Average loss at step 4624000: 4.036542\n",
      "Average loss at step 4626000: 4.034879\n",
      "Average loss at step 4628000: 4.036849\n",
      "Average loss at step 4630000: 4.038079\n",
      "Average loss at step 4632000: 4.036979\n",
      "Average loss at step 4634000: 4.036974\n",
      "Average loss at step 4636000: 4.036715\n",
      "Average loss at step 4638000: 4.034463\n",
      "Average loss at step 4640000: 4.032760\n",
      "Average loss at step 4642000: 4.037574\n",
      "Average loss at step 4644000: 4.034474\n",
      "Average loss at step 4646000: 4.035330\n",
      "Average loss at step 4648000: 4.036112\n",
      "Average loss at step 4650000: 4.038021\n",
      "Average loss at step 4652000: 4.035037\n",
      "Average loss at step 4654000: 4.037693\n",
      "Average loss at step 4656000: 4.034087\n",
      "Average loss at step 4658000: 4.034950\n",
      "Average loss at step 4660000: 4.032562\n",
      "Average loss at step 4662000: 4.035773\n",
      "Average loss at step 4664000: 4.035549\n",
      "Average loss at step 4666000: 4.035120\n",
      "Average loss at step 4668000: 4.033234\n",
      "Average loss at step 4670000: 4.035950\n",
      "Average loss at step 4672000: 4.036673\n",
      "Average loss at step 4674000: 4.035835\n",
      "Average loss at step 4676000: 4.035316\n",
      "Average loss at step 4678000: 4.034682\n",
      "Average loss at step 4680000: 4.034804\n",
      "Average loss at step 4682000: 4.034913\n",
      "Average loss at step 4684000: 4.035885\n",
      "Average loss at step 4686000: 4.035780\n",
      "Average loss at step 4688000: 4.035749\n",
      "Average loss at step 4690000: 4.035828\n",
      "Average loss at step 4692000: 4.035792\n",
      "Average loss at step 4694000: 4.037710\n",
      "Average loss at step 4696000: 4.035128\n",
      "Average loss at step 4698000: 4.033712\n",
      "Average loss at step 4700000: 4.034200\n",
      "Average loss at step 4702000: 4.036333\n",
      "Average loss at step 4704000: 4.035972\n",
      "Average loss at step 4706000: 4.035527\n",
      "Average loss at step 4708000: 4.035573\n",
      "Average loss at step 4710000: 4.034838\n",
      "Average loss at step 4712000: 4.034172\n",
      "Average loss at step 4714000: 4.037754\n",
      "Average loss at step 4716000: 4.036440\n",
      "Average loss at step 4718000: 4.036159\n",
      "Average loss at step 4720000: 4.035983\n",
      "Average loss at step 4722000: 4.038145\n",
      "Average loss at step 4724000: 4.034877\n",
      "Average loss at step 4726000: 4.036122\n",
      "Average loss at step 4728000: 4.038096\n",
      "Average loss at step 4730000: 4.036307\n",
      "Average loss at step 4732000: 4.035015\n",
      "Average loss at step 4734000: 4.034745\n",
      "Average loss at step 4736000: 4.033544\n",
      "Average loss at step 4738000: 4.036344\n",
      "Average loss at step 4740000: 4.035925\n",
      "Average loss at step 4742000: 4.035204\n",
      "Average loss at step 4744000: 4.035783\n",
      "Average loss at step 4746000: 4.034191\n",
      "Average loss at step 4748000: 4.033651\n",
      "Average loss at step 4750000: 4.034243\n",
      "Average loss at step 4752000: 4.035832\n",
      "Average loss at step 4754000: 4.035289\n",
      "Average loss at step 4756000: 4.035889\n",
      "Average loss at step 4758000: 4.034334\n",
      "Average loss at step 4760000: 4.035901\n",
      "Average loss at step 4762000: 4.036064\n",
      "Average loss at step 4764000: 4.033900\n",
      "Average loss at step 4766000: 4.034305\n",
      "Average loss at step 4768000: 4.034499\n",
      "Average loss at step 4770000: 4.034293\n",
      "Average loss at step 4772000: 4.036830\n",
      "Average loss at step 4774000: 4.036421\n",
      "Average loss at step 4776000: 4.034925\n",
      "Average loss at step 4778000: 4.033460\n",
      "Average loss at step 4780000: 4.035893\n",
      "Average loss at step 4782000: 4.038617\n",
      "Average loss at step 4784000: 4.035736\n",
      "Average loss at step 4786000: 4.034592\n",
      "Average loss at step 4788000: 4.037561\n",
      "Average loss at step 4790000: 4.034921\n",
      "Average loss at step 4792000: 4.038566\n",
      "Average loss at step 4794000: 4.032917\n",
      "Average loss at step 4796000: 4.032395\n",
      "Average loss at step 4798000: 4.038523\n",
      "Average loss at step 4800000: 4.034456\n",
      "Average loss at step 4802000: 4.037058\n",
      "Average loss at step 4804000: 4.035325\n",
      "Average loss at step 4806000: 4.032951\n",
      "Average loss at step 4808000: 4.033802\n",
      "Average loss at step 4810000: 4.035540\n",
      "Average loss at step 4812000: 4.034205\n",
      "Average loss at step 4814000: 4.033287\n",
      "Average loss at step 4816000: 4.035216\n",
      "Average loss at step 4818000: 4.036182\n",
      "Average loss at step 4820000: 4.035874\n",
      "Average loss at step 4822000: 4.034926\n",
      "Average loss at step 4824000: 4.036071\n",
      "Average loss at step 4826000: 4.034092\n",
      "Average loss at step 4828000: 4.035791\n",
      "Average loss at step 4830000: 4.035582\n",
      "Average loss at step 4832000: 4.037352\n",
      "Average loss at step 4834000: 4.035181\n",
      "Average loss at step 4836000: 4.036142\n",
      "Average loss at step 4838000: 4.033647\n",
      "Average loss at step 4840000: 4.034850\n",
      "Average loss at step 4842000: 4.033161\n",
      "Average loss at step 4844000: 4.036193\n",
      "Average loss at step 4846000: 4.032969\n",
      "Average loss at step 4848000: 4.035801\n",
      "Average loss at step 4850000: 4.034560\n",
      "Average loss at step 4852000: 4.035807\n",
      "Average loss at step 4854000: 4.034511\n",
      "Average loss at step 4856000: 4.034303\n",
      "Average loss at step 4858000: 4.036455\n",
      "Average loss at step 4860000: 4.035730\n",
      "Average loss at step 4862000: 4.036888\n",
      "Average loss at step 4864000: 4.034482\n",
      "Average loss at step 4866000: 4.035154\n",
      "Average loss at step 4868000: 4.035318\n",
      "Average loss at step 4870000: 4.034344\n",
      "Average loss at step 4872000: 4.034133\n",
      "Average loss at step 4874000: 4.035594\n",
      "Average loss at step 4876000: 4.033300\n",
      "Average loss at step 4878000: 4.036019\n",
      "Average loss at step 4880000: 4.035905\n",
      "Average loss at step 4882000: 4.035130\n",
      "Average loss at step 4884000: 4.035865\n",
      "Average loss at step 4886000: 4.035522\n",
      "Average loss at step 4888000: 4.039267\n",
      "Average loss at step 4890000: 4.036619\n",
      "Average loss at step 4892000: 4.035898\n",
      "Average loss at step 4894000: 4.033716\n",
      "Average loss at step 4896000: 4.036635\n",
      "Average loss at step 4898000: 4.035785\n",
      "Average loss at step 4900000: 4.034041\n",
      "Average loss at step 4902000: 4.037622\n",
      "Average loss at step 4904000: 4.033499\n",
      "Average loss at step 4906000: 4.035596\n",
      "Average loss at step 4908000: 4.034981\n",
      "Average loss at step 4910000: 4.035189\n",
      "Average loss at step 4912000: 4.034944\n",
      "Average loss at step 4914000: 4.033134\n",
      "Average loss at step 4916000: 4.034453\n",
      "Average loss at step 4918000: 4.035714\n",
      "Average loss at step 4920000: 4.037102\n",
      "Average loss at step 4922000: 4.036057\n",
      "Average loss at step 4924000: 4.036421\n",
      "Average loss at step 4926000: 4.037724\n",
      "Average loss at step 4928000: 4.034522\n",
      "Average loss at step 4930000: 4.036310\n",
      "Average loss at step 4932000: 4.036080\n",
      "Average loss at step 4934000: 4.034936\n",
      "Average loss at step 4936000: 4.033996\n",
      "Average loss at step 4938000: 4.033304\n",
      "Average loss at step 4940000: 4.032449\n",
      "Average loss at step 4942000: 4.034719\n",
      "Average loss at step 4944000: 4.034583\n",
      "Average loss at step 4946000: 4.034364\n",
      "Average loss at step 4948000: 4.035799\n",
      "Average loss at step 4950000: 4.035156\n",
      "Average loss at step 4952000: 4.036021\n",
      "Average loss at step 4954000: 4.036547\n",
      "Average loss at step 4956000: 4.036277\n",
      "Average loss at step 4958000: 4.036772\n",
      "Average loss at step 4960000: 4.033862\n",
      "Average loss at step 4962000: 4.033094\n",
      "Average loss at step 4964000: 4.034171\n",
      "Average loss at step 4966000: 4.037041\n",
      "Average loss at step 4968000: 4.033945\n",
      "Average loss at step 4970000: 4.035053\n",
      "Average loss at step 4972000: 4.035368\n",
      "Average loss at step 4974000: 4.036724\n",
      "Average loss at step 4976000: 4.035020\n",
      "Average loss at step 4978000: 4.033139\n",
      "Average loss at step 4980000: 4.034572\n",
      "Average loss at step 4982000: 4.032828\n",
      "Average loss at step 4984000: 4.033266\n",
      "Average loss at step 4986000: 4.035181\n",
      "Average loss at step 4988000: 4.036306\n",
      "Average loss at step 4990000: 4.034426\n",
      "Average loss at step 4992000: 4.039835\n",
      "Average loss at step 4994000: 4.035469\n",
      "Average loss at step 4996000: 4.035158\n",
      "Average loss at step 4998000: 4.034967\n",
      "Average loss at step 5000000: 4.037066\n",
      "Average loss at step 5002000: 4.034947\n",
      "Average loss at step 5004000: 4.035047\n",
      "Average loss at step 5006000: 4.034735\n",
      "Average loss at step 5008000: 4.036569\n",
      "Average loss at step 5010000: 4.035672\n",
      "Average loss at step 5012000: 4.035537\n",
      "Average loss at step 5014000: 4.035324\n",
      "Average loss at step 5016000: 4.034773\n",
      "Average loss at step 5018000: 4.036244\n",
      "Average loss at step 5020000: 4.033906\n",
      "Average loss at step 5022000: 4.034403\n",
      "Average loss at step 5024000: 4.033431\n",
      "Average loss at step 5026000: 4.036324\n",
      "Average loss at step 5028000: 4.032291\n",
      "Average loss at step 5030000: 4.036314\n",
      "Average loss at step 5032000: 4.033124\n",
      "Average loss at step 5034000: 4.036113\n",
      "Average loss at step 5036000: 4.035161\n",
      "Average loss at step 5038000: 4.035986\n",
      "Average loss at step 5040000: 4.033493\n",
      "Average loss at step 5042000: 4.034896\n",
      "Average loss at step 5044000: 4.033983\n",
      "Average loss at step 5046000: 4.033709\n",
      "Average loss at step 5048000: 4.035840\n",
      "Average loss at step 5050000: 4.033754\n",
      "Average loss at step 5052000: 4.035620\n",
      "Average loss at step 5054000: 4.038263\n",
      "Average loss at step 5056000: 4.035399\n",
      "Average loss at step 5058000: 4.033972\n",
      "Average loss at step 5060000: 4.036568\n",
      "Average loss at step 5062000: 4.033853\n",
      "Average loss at step 5064000: 4.035808\n",
      "Average loss at step 5066000: 4.030961\n",
      "Average loss at step 5068000: 4.030848\n",
      "Average loss at step 5070000: 4.036741\n",
      "Average loss at step 5072000: 4.034774\n",
      "Average loss at step 5074000: 4.034551\n",
      "Average loss at step 5076000: 4.035476\n",
      "Average loss at step 5078000: 4.032840\n",
      "Average loss at step 5080000: 4.034163\n",
      "Average loss at step 5082000: 4.035714\n",
      "Average loss at step 5084000: 4.034951\n",
      "Average loss at step 5086000: 4.034377\n",
      "Average loss at step 5088000: 4.032566\n",
      "Average loss at step 5090000: 4.034575\n",
      "Average loss at step 5092000: 4.036912\n",
      "Average loss at step 5094000: 4.033612\n",
      "Average loss at step 5096000: 4.035593\n",
      "Average loss at step 5098000: 4.034917\n",
      "Average loss at step 5100000: 4.037177\n",
      "Average loss at step 5102000: 4.034505\n",
      "Average loss at step 5104000: 4.034620\n",
      "Average loss at step 5106000: 4.035053\n",
      "Average loss at step 5108000: 4.034989\n",
      "Average loss at step 5110000: 4.034806\n",
      "Average loss at step 5112000: 4.034894\n",
      "Average loss at step 5114000: 4.036177\n",
      "Average loss at step 5116000: 4.035382\n",
      "Average loss at step 5118000: 4.032813\n",
      "Average loss at step 5120000: 4.034476\n",
      "Average loss at step 5122000: 4.034566\n",
      "Average loss at step 5124000: 4.035733\n",
      "Average loss at step 5126000: 4.034567\n",
      "Average loss at step 5128000: 4.034742\n",
      "Average loss at step 5130000: 4.034953\n",
      "Average loss at step 5132000: 4.037744\n",
      "Average loss at step 5134000: 4.035759\n",
      "Average loss at step 5136000: 4.034708\n",
      "Average loss at step 5138000: 4.033651\n",
      "Average loss at step 5140000: 4.034538\n",
      "Average loss at step 5142000: 4.034651\n",
      "Average loss at step 5144000: 4.035835\n",
      "Average loss at step 5146000: 4.036608\n",
      "Average loss at step 5148000: 4.034603\n",
      "Average loss at step 5150000: 4.034149\n",
      "Average loss at step 5152000: 4.035117\n",
      "Average loss at step 5154000: 4.033723\n",
      "Average loss at step 5156000: 4.034912\n",
      "Average loss at step 5158000: 4.035451\n",
      "Average loss at step 5160000: 4.035048\n",
      "Average loss at step 5162000: 4.034974\n",
      "Average loss at step 5164000: 4.032802\n",
      "Average loss at step 5166000: 4.035922\n",
      "Average loss at step 5168000: 4.034691\n",
      "Average loss at step 5170000: 4.037981\n",
      "Average loss at step 5172000: 4.033236\n",
      "Average loss at step 5174000: 4.034328\n",
      "Average loss at step 5176000: 4.035190\n",
      "Average loss at step 5178000: 4.035090\n",
      "Average loss at step 5180000: 4.034847\n",
      "Average loss at step 5182000: 4.035299\n",
      "Average loss at step 5184000: 4.033128\n",
      "Average loss at step 5186000: 4.033819\n",
      "Average loss at step 5188000: 4.034419\n",
      "Average loss at step 5190000: 4.033862\n",
      "Average loss at step 5192000: 4.036497\n",
      "Average loss at step 5194000: 4.035571\n",
      "Average loss at step 5196000: 4.036562\n",
      "Average loss at step 5198000: 4.034391\n",
      "Average loss at step 5200000: 4.035320\n",
      "Average loss at step 5202000: 4.033149\n",
      "Average loss at step 5204000: 4.034874\n",
      "Average loss at step 5206000: 4.034425\n",
      "Average loss at step 5208000: 4.034877\n",
      "Average loss at step 5210000: 4.035082\n",
      "Average loss at step 5212000: 4.033874\n",
      "Average loss at step 5214000: 4.033916\n",
      "Average loss at step 5216000: 4.034204\n",
      "Average loss at step 5218000: 4.033812\n",
      "Average loss at step 5220000: 4.034408\n",
      "Average loss at step 5222000: 4.034143\n",
      "Average loss at step 5224000: 4.033745\n",
      "Average loss at step 5226000: 4.034430\n",
      "Average loss at step 5228000: 4.035467\n",
      "Average loss at step 5230000: 4.036844\n",
      "Average loss at step 5232000: 4.034068\n",
      "Average loss at step 5234000: 4.033098\n",
      "Average loss at step 5236000: 4.035291\n",
      "Average loss at step 5238000: 4.034502\n",
      "Average loss at step 5240000: 4.035321\n",
      "Average loss at step 5242000: 4.034091\n",
      "Average loss at step 5244000: 4.032633\n",
      "Average loss at step 5246000: 4.036405\n",
      "Average loss at step 5248000: 4.034555\n",
      "Average loss at step 5250000: 4.034430\n",
      "Average loss at step 5252000: 4.034874\n",
      "Average loss at step 5254000: 4.032010\n",
      "Average loss at step 5256000: 4.034259\n",
      "Average loss at step 5258000: 4.034899\n",
      "Average loss at step 5260000: 4.036173\n",
      "Average loss at step 5262000: 4.035857\n",
      "Average loss at step 5264000: 4.035413\n",
      "Average loss at step 5266000: 4.034940\n",
      "Average loss at step 5268000: 4.037036\n",
      "Average loss at step 5270000: 4.033453\n",
      "Average loss at step 5272000: 4.034437\n",
      "Average loss at step 5274000: 4.036058\n",
      "Average loss at step 5276000: 4.037001\n",
      "Average loss at step 5278000: 4.034974\n",
      "Average loss at step 5280000: 4.035975\n",
      "Average loss at step 5282000: 4.034265\n",
      "Average loss at step 5284000: 4.036047\n",
      "Average loss at step 5286000: 4.037137\n",
      "Average loss at step 5288000: 4.033046\n",
      "Average loss at step 5290000: 4.033237\n",
      "Average loss at step 5292000: 4.033881\n",
      "Average loss at step 5294000: 4.033940\n",
      "Average loss at step 5296000: 4.033523\n",
      "Average loss at step 5298000: 4.034427\n",
      "Average loss at step 5300000: 4.033757\n",
      "Average loss at step 5302000: 4.033917\n",
      "Average loss at step 5304000: 4.032458\n",
      "Average loss at step 5306000: 4.034036\n",
      "Average loss at step 5308000: 4.036908\n",
      "Average loss at step 5310000: 4.034574\n",
      "Average loss at step 5312000: 4.036534\n",
      "Average loss at step 5314000: 4.034799\n",
      "Average loss at step 5316000: 4.037956\n",
      "Average loss at step 5318000: 4.033922\n",
      "Average loss at step 5320000: 4.035914\n",
      "Average loss at step 5322000: 4.033302\n",
      "Average loss at step 5324000: 4.033601\n",
      "Average loss at step 5326000: 4.036279\n",
      "Average loss at step 5328000: 4.035354\n",
      "Average loss at step 5330000: 4.036428\n",
      "Average loss at step 5332000: 4.034894\n",
      "Average loss at step 5334000: 4.034240\n",
      "Average loss at step 5336000: 4.034341\n",
      "Average loss at step 5338000: 4.034352\n",
      "Average loss at step 5340000: 4.032336\n",
      "Average loss at step 5342000: 4.032543\n",
      "Average loss at step 5344000: 4.037439\n",
      "Average loss at step 5346000: 4.035382\n",
      "Average loss at step 5348000: 4.034464\n",
      "Average loss at step 5350000: 4.035042\n",
      "Average loss at step 5352000: 4.034258\n",
      "Average loss at step 5354000: 4.034876\n",
      "Average loss at step 5356000: 4.034307\n",
      "Average loss at step 5358000: 4.036090\n",
      "Average loss at step 5360000: 4.032651\n",
      "Average loss at step 5362000: 4.032659\n",
      "Average loss at step 5364000: 4.036011\n",
      "Average loss at step 5366000: 4.033982\n",
      "Average loss at step 5368000: 4.034560\n",
      "Average loss at step 5370000: 4.035988\n",
      "Average loss at step 5372000: 4.034158\n",
      "Average loss at step 5374000: 4.036508\n",
      "Average loss at step 5376000: 4.036825\n",
      "Average loss at step 5378000: 4.032925\n",
      "Average loss at step 5380000: 4.034307\n",
      "Average loss at step 5382000: 4.034346\n",
      "Average loss at step 5384000: 4.033648\n",
      "Average loss at step 5386000: 4.035847\n",
      "Average loss at step 5388000: 4.036073\n",
      "Average loss at step 5390000: 4.035128\n",
      "Average loss at step 5392000: 4.035864\n",
      "Average loss at step 5394000: 4.032898\n",
      "Average loss at step 5396000: 4.033292\n",
      "Average loss at step 5398000: 4.035193\n",
      "Average loss at step 5400000: 4.033347\n",
      "Average loss at step 5402000: 4.034431\n",
      "Average loss at step 5404000: 4.033779\n",
      "Average loss at step 5406000: 4.034982\n",
      "Average loss at step 5408000: 4.034103\n",
      "Average loss at step 5410000: 4.035112\n",
      "Average loss at step 5412000: 4.035946\n",
      "Average loss at step 5414000: 4.035301\n",
      "Average loss at step 5416000: 4.036996\n",
      "Average loss at step 5418000: 4.034257\n",
      "Average loss at step 5420000: 4.035013\n",
      "Average loss at step 5422000: 4.032551\n",
      "Average loss at step 5424000: 4.035683\n",
      "Average loss at step 5426000: 4.034342\n",
      "Average loss at step 5428000: 4.033548\n",
      "Average loss at step 5430000: 4.034581\n",
      "Average loss at step 5432000: 4.033210\n",
      "Average loss at step 5434000: 4.035409\n",
      "Average loss at step 5436000: 4.034257\n",
      "Average loss at step 5438000: 4.035520\n",
      "Average loss at step 5440000: 4.032867\n",
      "Average loss at step 5442000: 4.036283\n",
      "Average loss at step 5444000: 4.032006\n",
      "Average loss at step 5446000: 4.033520\n",
      "Average loss at step 5448000: 4.035115\n",
      "Average loss at step 5450000: 4.033059\n",
      "Average loss at step 5452000: 4.035924\n",
      "Average loss at step 5454000: 4.035792\n",
      "Average loss at step 5456000: 4.035600\n",
      "Average loss at step 5458000: 4.034556\n",
      "Average loss at step 5460000: 4.035962\n",
      "Average loss at step 5462000: 4.034827\n",
      "Average loss at step 5464000: 4.032684\n",
      "Average loss at step 5466000: 4.036344\n",
      "Average loss at step 5468000: 4.035694\n",
      "Average loss at step 5470000: 4.035870\n",
      "Average loss at step 5472000: 4.035661\n",
      "Average loss at step 5474000: 4.033166\n",
      "Average loss at step 5476000: 4.035403\n",
      "Average loss at step 5478000: 4.034421\n",
      "Average loss at step 5480000: 4.033327\n",
      "Average loss at step 5482000: 4.032996\n",
      "Average loss at step 5484000: 4.035972\n",
      "Average loss at step 5486000: 4.032096\n",
      "Average loss at step 5488000: 4.032986\n",
      "Average loss at step 5490000: 4.035313\n",
      "Average loss at step 5492000: 4.035269\n",
      "Average loss at step 5494000: 4.034684\n",
      "Average loss at step 5496000: 4.031748\n",
      "Average loss at step 5498000: 4.034101\n",
      "Average loss at step 5500000: 4.033980\n",
      "Average loss at step 5502000: 4.034580\n",
      "Average loss at step 5504000: 4.034304\n",
      "Average loss at step 5506000: 4.033678\n",
      "Average loss at step 5508000: 4.033321\n",
      "Average loss at step 5510000: 4.032564\n",
      "Average loss at step 5512000: 4.035210\n",
      "Average loss at step 5514000: 4.036472\n",
      "Average loss at step 5516000: 4.034318\n",
      "Average loss at step 5518000: 4.034527\n",
      "Average loss at step 5520000: 4.035469\n",
      "Average loss at step 5522000: 4.035047\n",
      "Average loss at step 5524000: 4.031686\n",
      "Average loss at step 5526000: 4.034079\n",
      "Average loss at step 5528000: 4.034572\n",
      "Average loss at step 5530000: 4.032659\n",
      "Average loss at step 5532000: 4.035208\n",
      "Average loss at step 5534000: 4.036325\n",
      "Average loss at step 5536000: 4.032497\n",
      "Average loss at step 5538000: 4.034639\n",
      "Average loss at step 5540000: 4.035089\n",
      "Average loss at step 5542000: 4.036021\n",
      "Average loss at step 5544000: 4.035733\n",
      "Average loss at step 5546000: 4.034630\n",
      "Average loss at step 5548000: 4.034023\n",
      "Average loss at step 5550000: 4.032977\n",
      "Average loss at step 5552000: 4.034740\n",
      "Average loss at step 5554000: 4.033997\n",
      "Average loss at step 5556000: 4.037857\n",
      "Average loss at step 5558000: 4.036340\n",
      "Average loss at step 5560000: 4.033339\n",
      "Average loss at step 5562000: 4.035813\n",
      "Average loss at step 5564000: 4.032719\n",
      "Average loss at step 5566000: 4.034610\n",
      "Average loss at step 5568000: 4.036085\n",
      "Average loss at step 5570000: 4.032670\n",
      "Average loss at step 5572000: 4.033706\n",
      "Average loss at step 5574000: 4.033052\n",
      "Average loss at step 5576000: 4.034979\n",
      "Average loss at step 5578000: 4.032628\n",
      "Average loss at step 5580000: 4.035209\n",
      "Average loss at step 5582000: 4.034361\n",
      "Average loss at step 5584000: 4.036470\n",
      "Average loss at step 5586000: 4.034833\n",
      "Average loss at step 5588000: 4.033597\n",
      "Average loss at step 5590000: 4.036029\n",
      "Average loss at step 5592000: 4.035197\n",
      "Average loss at step 5594000: 4.033793\n",
      "Average loss at step 5596000: 4.034753\n",
      "Average loss at step 5598000: 4.034947\n",
      "Average loss at step 5600000: 4.035366\n",
      "Average loss at step 5602000: 4.034286\n",
      "Average loss at step 5604000: 4.036417\n",
      "Average loss at step 5606000: 4.034263\n",
      "Average loss at step 5608000: 4.032400\n",
      "Average loss at step 5610000: 4.036494\n",
      "Average loss at step 5612000: 4.033290\n",
      "Average loss at step 5614000: 4.031470\n",
      "Average loss at step 5616000: 4.037149\n",
      "Average loss at step 5618000: 4.033702\n",
      "Average loss at step 5620000: 4.033105\n",
      "Average loss at step 5622000: 4.034589\n",
      "Average loss at step 5624000: 4.030744\n",
      "Average loss at step 5626000: 4.035562\n",
      "Average loss at step 5628000: 4.035373\n",
      "Average loss at step 5630000: 4.034536\n",
      "Average loss at step 5632000: 4.033374\n",
      "Average loss at step 5634000: 4.034162\n",
      "Average loss at step 5636000: 4.034968\n",
      "Average loss at step 5638000: 4.036590\n",
      "Average loss at step 5640000: 4.032997\n",
      "Average loss at step 5642000: 4.033932\n",
      "Average loss at step 5644000: 4.034168\n",
      "Average loss at step 5646000: 4.036641\n",
      "Average loss at step 5648000: 4.034518\n",
      "Average loss at step 5650000: 4.035353\n",
      "Average loss at step 5652000: 4.033843\n",
      "Average loss at step 5654000: 4.034019\n",
      "Average loss at step 5656000: 4.035934\n",
      "Average loss at step 5658000: 4.034768\n",
      "Average loss at step 5660000: 4.034940\n",
      "Average loss at step 5662000: 4.034820\n",
      "Average loss at step 5664000: 4.031814\n",
      "Average loss at step 5666000: 4.033786\n",
      "Average loss at step 5668000: 4.033062\n",
      "Average loss at step 5670000: 4.035716\n",
      "Average loss at step 5672000: 4.033501\n",
      "Average loss at step 5674000: 4.034369\n",
      "Average loss at step 5676000: 4.033428\n",
      "Average loss at step 5678000: 4.036907\n",
      "Average loss at step 5680000: 4.033948\n",
      "Average loss at step 5682000: 4.032381\n",
      "Average loss at step 5684000: 4.033679\n",
      "Average loss at step 5686000: 4.034430\n",
      "Average loss at step 5688000: 4.035025\n",
      "Average loss at step 5690000: 4.032812\n",
      "Average loss at step 5692000: 4.032536\n",
      "Average loss at step 5694000: 4.033891\n",
      "Average loss at step 5696000: 4.034118\n",
      "Average loss at step 5698000: 4.035201\n",
      "Average loss at step 5700000: 4.034682\n",
      "Average loss at step 5702000: 4.034197\n",
      "Average loss at step 5704000: 4.032007\n",
      "Average loss at step 5706000: 4.032866\n",
      "Average loss at step 5708000: 4.033139\n",
      "Average loss at step 5710000: 4.037806\n",
      "Average loss at step 5712000: 4.032277\n",
      "Average loss at step 5714000: 4.037055\n",
      "Average loss at step 5716000: 4.036341\n",
      "Average loss at step 5718000: 4.032672\n",
      "Average loss at step 5720000: 4.035867\n",
      "Average loss at step 5722000: 4.034218\n",
      "Average loss at step 5724000: 4.035042\n",
      "Average loss at step 5726000: 4.034182\n",
      "Average loss at step 5728000: 4.034892\n",
      "Average loss at step 5730000: 4.032719\n",
      "Average loss at step 5732000: 4.036140\n",
      "Average loss at step 5734000: 4.033812\n",
      "Average loss at step 5736000: 4.035834\n",
      "Average loss at step 5738000: 4.032980\n",
      "Average loss at step 5740000: 4.035738\n",
      "Average loss at step 5742000: 4.033582\n",
      "Average loss at step 5744000: 4.035468\n",
      "Average loss at step 5746000: 4.035486\n",
      "Average loss at step 5748000: 4.033872\n",
      "Average loss at step 5750000: 4.034812\n",
      "Average loss at step 5752000: 4.033386\n",
      "Average loss at step 5754000: 4.032439\n",
      "Average loss at step 5756000: 4.034877\n",
      "Average loss at step 5758000: 4.034329\n",
      "Average loss at step 5760000: 4.033198\n",
      "Average loss at step 5762000: 4.033862\n",
      "Average loss at step 5764000: 4.034335\n",
      "Average loss at step 5766000: 4.034346\n",
      "Average loss at step 5768000: 4.032812\n",
      "Average loss at step 5770000: 4.034952\n",
      "Average loss at step 5772000: 4.032529\n",
      "Average loss at step 5774000: 4.035974\n",
      "Average loss at step 5776000: 4.034293\n",
      "Average loss at step 5778000: 4.033058\n",
      "Average loss at step 5780000: 4.033508\n",
      "Average loss at step 5782000: 4.033052\n",
      "Average loss at step 5784000: 4.032660\n",
      "Average loss at step 5786000: 4.035793\n",
      "Average loss at step 5788000: 4.033312\n",
      "Average loss at step 5790000: 4.034170\n",
      "Average loss at step 5792000: 4.035830\n",
      "Average loss at step 5794000: 4.033641\n",
      "Average loss at step 5796000: 4.033036\n",
      "Average loss at step 5798000: 4.034005\n",
      "Average loss at step 5800000: 4.032341\n",
      "Average loss at step 5802000: 4.033736\n",
      "Average loss at step 5804000: 4.036037\n",
      "Average loss at step 5806000: 4.035468\n",
      "Average loss at step 5808000: 4.035853\n",
      "Average loss at step 5810000: 4.034692\n",
      "Average loss at step 5812000: 4.037685\n",
      "Average loss at step 5814000: 4.035556\n",
      "Average loss at step 5816000: 4.032463\n",
      "Average loss at step 5818000: 4.035688\n",
      "Average loss at step 5820000: 4.035113\n",
      "Average loss at step 5822000: 4.034710\n",
      "Average loss at step 5824000: 4.034748\n",
      "Average loss at step 5826000: 4.037140\n",
      "Average loss at step 5828000: 4.034487\n",
      "Average loss at step 5830000: 4.036237\n",
      "Average loss at step 5832000: 4.035014\n",
      "Average loss at step 5834000: 4.033534\n",
      "Average loss at step 5836000: 4.034496\n",
      "Average loss at step 5838000: 4.031521\n",
      "Average loss at step 5840000: 4.033336\n",
      "Average loss at step 5842000: 4.034551\n",
      "Average loss at step 5844000: 4.032701\n",
      "Average loss at step 5846000: 4.032846\n",
      "Average loss at step 5848000: 4.034020\n",
      "Average loss at step 5850000: 4.035981\n",
      "Average loss at step 5852000: 4.034520\n",
      "Average loss at step 5854000: 4.036730\n",
      "Average loss at step 5856000: 4.035473\n",
      "Average loss at step 5858000: 4.033907\n",
      "Average loss at step 5860000: 4.034286\n",
      "Average loss at step 5862000: 4.034301\n",
      "Average loss at step 5864000: 4.035324\n",
      "Average loss at step 5866000: 4.032201\n",
      "Average loss at step 5868000: 4.033944\n",
      "Average loss at step 5870000: 4.035387\n",
      "Average loss at step 5872000: 4.037563\n",
      "Average loss at step 5874000: 4.033858\n",
      "Average loss at step 5876000: 4.035736\n",
      "Average loss at step 5878000: 4.036776\n",
      "Average loss at step 5880000: 4.030299\n",
      "Average loss at step 5882000: 4.034389\n",
      "Average loss at step 5884000: 4.034128\n",
      "Average loss at step 5886000: 4.030712\n",
      "Average loss at step 5888000: 4.034016\n",
      "Average loss at step 5890000: 4.035974\n",
      "Average loss at step 5892000: 4.033468\n",
      "Average loss at step 5894000: 4.034486\n",
      "Average loss at step 5896000: 4.032061\n",
      "Average loss at step 5898000: 4.031584\n",
      "Average loss at step 5900000: 4.034024\n",
      "Average loss at step 5902000: 4.034204\n",
      "Average loss at step 5904000: 4.033997\n",
      "Average loss at step 5906000: 4.033872\n",
      "Average loss at step 5908000: 4.034747\n",
      "Average loss at step 5910000: 4.036542\n",
      "Average loss at step 5912000: 4.032699\n",
      "Average loss at step 5914000: 4.033097\n",
      "Average loss at step 5916000: 4.033007\n",
      "Average loss at step 5918000: 4.034409\n",
      "Average loss at step 5920000: 4.035342\n",
      "Average loss at step 5922000: 4.034251\n",
      "Average loss at step 5924000: 4.035358\n",
      "Average loss at step 5926000: 4.033109\n",
      "Average loss at step 5928000: 4.032816\n",
      "Average loss at step 5930000: 4.032379\n",
      "Average loss at step 5932000: 4.033476\n",
      "Average loss at step 5934000: 4.035636\n",
      "Average loss at step 5936000: 4.033743\n",
      "Average loss at step 5938000: 4.036203\n",
      "Average loss at step 5940000: 4.032648\n",
      "Average loss at step 5942000: 4.033695\n",
      "Average loss at step 5944000: 4.033112\n",
      "Average loss at step 5946000: 4.032048\n",
      "Average loss at step 5948000: 4.035084\n",
      "Average loss at step 5950000: 4.034785\n",
      "Average loss at step 5952000: 4.035585\n",
      "Average loss at step 5954000: 4.034441\n",
      "Average loss at step 5956000: 4.032049\n",
      "Average loss at step 5958000: 4.035626\n",
      "Average loss at step 5960000: 4.034748\n",
      "Average loss at step 5962000: 4.032965\n",
      "Average loss at step 5964000: 4.034626\n",
      "Average loss at step 5966000: 4.032728\n",
      "Average loss at step 5968000: 4.034381\n",
      "Average loss at step 5970000: 4.033899\n",
      "Average loss at step 5972000: 4.033349\n",
      "Average loss at step 5974000: 4.034740\n",
      "Average loss at step 5976000: 4.032673\n",
      "Average loss at step 5978000: 4.034865\n",
      "Average loss at step 5980000: 4.033109\n",
      "Average loss at step 5982000: 4.033672\n",
      "Average loss at step 5984000: 4.035034\n",
      "Average loss at step 5986000: 4.034752\n",
      "Average loss at step 5988000: 4.038019\n",
      "Average loss at step 5990000: 4.033214\n",
      "Average loss at step 5992000: 4.034409\n",
      "Average loss at step 5994000: 4.034853\n",
      "Average loss at step 5996000: 4.035681\n",
      "Average loss at step 5998000: 4.034242\n",
      "Average loss at step 6000000: 4.034579\n",
      "Average loss at step 6002000: 4.033454\n",
      "Average loss at step 6004000: 4.032652\n",
      "Average loss at step 6006000: 4.034321\n",
      "Average loss at step 6008000: 4.033103\n",
      "Average loss at step 6010000: 4.034303\n",
      "Average loss at step 6012000: 4.035128\n",
      "Average loss at step 6014000: 4.033955\n",
      "Average loss at step 6016000: 4.034240\n",
      "Average loss at step 6018000: 4.035284\n",
      "Average loss at step 6020000: 4.032478\n",
      "Average loss at step 6022000: 4.035634\n",
      "Average loss at step 6024000: 4.033378\n",
      "Average loss at step 6026000: 4.034706\n",
      "Average loss at step 6028000: 4.032664\n",
      "Average loss at step 6030000: 4.035561\n",
      "Average loss at step 6032000: 4.033911\n",
      "Average loss at step 6034000: 4.034083\n",
      "Average loss at step 6036000: 4.034838\n",
      "Average loss at step 6038000: 4.033443\n",
      "Average loss at step 6040000: 4.034734\n",
      "Average loss at step 6042000: 4.035220\n",
      "Average loss at step 6044000: 4.034222\n",
      "Average loss at step 6046000: 4.035227\n",
      "Average loss at step 6048000: 4.034797\n",
      "Average loss at step 6050000: 4.032478\n",
      "Average loss at step 6052000: 4.033497\n",
      "Average loss at step 6054000: 4.035866\n",
      "Average loss at step 6056000: 4.035851\n",
      "Average loss at step 6058000: 4.034045\n",
      "Average loss at step 6060000: 4.033843\n",
      "Average loss at step 6062000: 4.032574\n",
      "Average loss at step 6064000: 4.032342\n",
      "Average loss at step 6066000: 4.033238\n",
      "Average loss at step 6068000: 4.032883\n",
      "Average loss at step 6070000: 4.033373\n",
      "Average loss at step 6072000: 4.032257\n",
      "Average loss at step 6074000: 4.036006\n",
      "Average loss at step 6076000: 4.032826\n",
      "Average loss at step 6078000: 4.036293\n",
      "Average loss at step 6080000: 4.033141\n",
      "Average loss at step 6082000: 4.033863\n",
      "Average loss at step 6084000: 4.035431\n",
      "Average loss at step 6086000: 4.034270\n",
      "Average loss at step 6088000: 4.033431\n",
      "Average loss at step 6090000: 4.034525\n",
      "Average loss at step 6092000: 4.035328\n",
      "Average loss at step 6094000: 4.034171\n",
      "Average loss at step 6096000: 4.033122\n",
      "Average loss at step 6098000: 4.034141\n",
      "Average loss at step 6100000: 4.032577\n",
      "Average loss at step 6102000: 4.036084\n",
      "Average loss at step 6104000: 4.034541\n",
      "Average loss at step 6106000: 4.033378\n",
      "Average loss at step 6108000: 4.032937\n",
      "Average loss at step 6110000: 4.035283\n",
      "Average loss at step 6112000: 4.033300\n",
      "Average loss at step 6114000: 4.033161\n",
      "Average loss at step 6116000: 4.034530\n",
      "Average loss at step 6118000: 4.031084\n",
      "Average loss at step 6120000: 4.033652\n",
      "Average loss at step 6122000: 4.035306\n",
      "Average loss at step 6124000: 4.033142\n",
      "Average loss at step 6126000: 4.034772\n",
      "Average loss at step 6128000: 4.035101\n",
      "Average loss at step 6130000: 4.033413\n",
      "Average loss at step 6132000: 4.032055\n",
      "Average loss at step 6134000: 4.035066\n",
      "Average loss at step 6136000: 4.032053\n",
      "Average loss at step 6138000: 4.035768\n",
      "Average loss at step 6140000: 4.034294\n",
      "Average loss at step 6142000: 4.033275\n",
      "Average loss at step 6144000: 4.035414\n",
      "Average loss at step 6146000: 4.034611\n",
      "Average loss at step 6148000: 4.034330\n",
      "Average loss at step 6150000: 4.034616\n",
      "Average loss at step 6152000: 4.035991\n",
      "Average loss at step 6154000: 4.034192\n",
      "Average loss at step 6156000: 4.032193\n",
      "Average loss at step 6158000: 4.031954\n",
      "Average loss at step 6160000: 4.032063\n",
      "Average loss at step 6162000: 4.035870\n",
      "Average loss at step 6164000: 4.033125\n",
      "Average loss at step 6166000: 4.031522\n",
      "Average loss at step 6168000: 4.033249\n",
      "Average loss at step 6170000: 4.033022\n",
      "Average loss at step 6172000: 4.034058\n",
      "Average loss at step 6174000: 4.035252\n",
      "Average loss at step 6176000: 4.033196\n",
      "Average loss at step 6178000: 4.032441\n",
      "Average loss at step 6180000: 4.031037\n",
      "Average loss at step 6182000: 4.035459\n",
      "Average loss at step 6184000: 4.033811\n",
      "Average loss at step 6186000: 4.032010\n",
      "Average loss at step 6188000: 4.033801\n",
      "Average loss at step 6190000: 4.034093\n",
      "Average loss at step 6192000: 4.034815\n",
      "Average loss at step 6194000: 4.036390\n",
      "Average loss at step 6196000: 4.035377\n",
      "Average loss at step 6198000: 4.032145\n",
      "Average loss at step 6200000: 4.033964\n",
      "Average loss at step 6202000: 4.033651\n",
      "Average loss at step 6204000: 4.035767\n",
      "Average loss at step 6206000: 4.032333\n",
      "Average loss at step 6208000: 4.033948\n",
      "Average loss at step 6210000: 4.034822\n",
      "Average loss at step 6212000: 4.034617\n",
      "Average loss at step 6214000: 4.035524\n",
      "Average loss at step 6216000: 4.033749\n",
      "Average loss at step 6218000: 4.032396\n",
      "Average loss at step 6220000: 4.033239\n",
      "Average loss at step 6222000: 4.033924\n",
      "Average loss at step 6224000: 4.033824\n",
      "Average loss at step 6226000: 4.032357\n",
      "Average loss at step 6228000: 4.031971\n",
      "Average loss at step 6230000: 4.032537\n",
      "Average loss at step 6232000: 4.034037\n",
      "Average loss at step 6234000: 4.034615\n",
      "Average loss at step 6236000: 4.034722\n",
      "Average loss at step 6238000: 4.034957\n",
      "Average loss at step 6240000: 4.033328\n",
      "Average loss at step 6242000: 4.035700\n",
      "Average loss at step 6244000: 4.031406\n",
      "Average loss at step 6246000: 4.037208\n",
      "Average loss at step 6248000: 4.034571\n",
      "Average loss at step 6250000: 4.032732\n",
      "Average loss at step 6252000: 4.034305\n",
      "Average loss at step 6254000: 4.033524\n",
      "Average loss at step 6256000: 4.035340\n",
      "Average loss at step 6258000: 4.034461\n",
      "Average loss at step 6260000: 4.035855\n",
      "Average loss at step 6262000: 4.033334\n",
      "Average loss at step 6264000: 4.033158\n",
      "Average loss at step 6266000: 4.036256\n",
      "Average loss at step 6268000: 4.032783\n",
      "Average loss at step 6270000: 4.035672\n",
      "Average loss at step 6272000: 4.034949\n",
      "Average loss at step 6274000: 4.034103\n",
      "Average loss at step 6276000: 4.031441\n",
      "Average loss at step 6278000: 4.032265\n",
      "Average loss at step 6280000: 4.033989\n",
      "Average loss at step 6282000: 4.033316\n",
      "Average loss at step 6284000: 4.036198\n",
      "Average loss at step 6286000: 4.035889\n",
      "Average loss at step 6288000: 4.035007\n",
      "Average loss at step 6290000: 4.035753\n",
      "Average loss at step 6292000: 4.031327\n",
      "Average loss at step 6294000: 4.035372\n",
      "Average loss at step 6296000: 4.032932\n",
      "Average loss at step 6298000: 4.034162\n",
      "Average loss at step 6300000: 4.032870\n",
      "Average loss at step 6302000: 4.034254\n",
      "Average loss at step 6304000: 4.033145\n",
      "Average loss at step 6306000: 4.033187\n",
      "Average loss at step 6308000: 4.033448\n",
      "Average loss at step 6310000: 4.035272\n",
      "Average loss at step 6312000: 4.034478\n",
      "Average loss at step 6314000: 4.032031\n",
      "Average loss at step 6316000: 4.032280\n",
      "Average loss at step 6318000: 4.035155\n",
      "Average loss at step 6320000: 4.034012\n",
      "Average loss at step 6322000: 4.035624\n",
      "Average loss at step 6324000: 4.032797\n",
      "Average loss at step 6326000: 4.033934\n",
      "Average loss at step 6328000: 4.034519\n",
      "Average loss at step 6330000: 4.034698\n",
      "Average loss at step 6332000: 4.031691\n",
      "Average loss at step 6334000: 4.032637\n",
      "Average loss at step 6336000: 4.032632\n",
      "Average loss at step 6338000: 4.033209\n",
      "Average loss at step 6340000: 4.031892\n",
      "Average loss at step 6342000: 4.031818\n",
      "Average loss at step 6344000: 4.034292\n",
      "Average loss at step 6346000: 4.031642\n",
      "Average loss at step 6348000: 4.032097\n",
      "Average loss at step 6350000: 4.036939\n",
      "Average loss at step 6352000: 4.033495\n",
      "Average loss at step 6354000: 4.034235\n",
      "Average loss at step 6356000: 4.033931\n",
      "Average loss at step 6358000: 4.034450\n",
      "Average loss at step 6360000: 4.033318\n",
      "Average loss at step 6362000: 4.035583\n",
      "Average loss at step 6364000: 4.034369\n",
      "Average loss at step 6366000: 4.034677\n",
      "Average loss at step 6368000: 4.033255\n",
      "Average loss at step 6370000: 4.034294\n",
      "Average loss at step 6372000: 4.033458\n",
      "Average loss at step 6374000: 4.036658\n",
      "Average loss at step 6376000: 4.035364\n",
      "Average loss at step 6378000: 4.031900\n",
      "Average loss at step 6380000: 4.034277\n",
      "Average loss at step 6382000: 4.033550\n",
      "Average loss at step 6384000: 4.033812\n",
      "Average loss at step 6386000: 4.034195\n",
      "Average loss at step 6388000: 4.035336\n",
      "Average loss at step 6390000: 4.031263\n",
      "Average loss at step 6392000: 4.034039\n",
      "Average loss at step 6394000: 4.036797\n",
      "Average loss at step 6396000: 4.031419\n",
      "Average loss at step 6398000: 4.037200\n",
      "Average loss at step 6400000: 4.032389\n",
      "Average loss at step 6402000: 4.035086\n",
      "Average loss at step 6404000: 4.031475\n",
      "Average loss at step 6406000: 4.033841\n",
      "Average loss at step 6408000: 4.033129\n",
      "Average loss at step 6410000: 4.032552\n",
      "Average loss at step 6412000: 4.032159\n",
      "Average loss at step 6414000: 4.031349\n",
      "Average loss at step 6416000: 4.036943\n",
      "Average loss at step 6418000: 4.035778\n",
      "Average loss at step 6420000: 4.032853\n",
      "Average loss at step 6422000: 4.035114\n",
      "Average loss at step 6424000: 4.034735\n",
      "Average loss at step 6426000: 4.032154\n",
      "Average loss at step 6428000: 4.032847\n",
      "Average loss at step 6430000: 4.032740\n",
      "Average loss at step 6432000: 4.031598\n",
      "Average loss at step 6434000: 4.034110\n",
      "Average loss at step 6436000: 4.033140\n",
      "Average loss at step 6438000: 4.033455\n",
      "Average loss at step 6440000: 4.034343\n",
      "Average loss at step 6442000: 4.033198\n",
      "Average loss at step 6444000: 4.031804\n",
      "Average loss at step 6446000: 4.034995\n",
      "Average loss at step 6448000: 4.033471\n",
      "Average loss at step 6450000: 4.032838\n",
      "Average loss at step 6452000: 4.032584\n",
      "Average loss at step 6454000: 4.032223\n",
      "Average loss at step 6456000: 4.033967\n",
      "Average loss at step 6458000: 4.031228\n",
      "Average loss at step 6460000: 4.032169\n",
      "Average loss at step 6462000: 4.032883\n",
      "Average loss at step 6464000: 4.035109\n",
      "Average loss at step 6466000: 4.034029\n",
      "Average loss at step 6468000: 4.033623\n",
      "Average loss at step 6470000: 4.032830\n",
      "Average loss at step 6472000: 4.031692\n",
      "Average loss at step 6474000: 4.034290\n",
      "Average loss at step 6476000: 4.033554\n",
      "Average loss at step 6478000: 4.036193\n",
      "Average loss at step 6480000: 4.034080\n",
      "Average loss at step 6482000: 4.032774\n",
      "Average loss at step 6484000: 4.035042\n",
      "Average loss at step 6486000: 4.032578\n",
      "Average loss at step 6488000: 4.034352\n",
      "Average loss at step 6490000: 4.031681\n",
      "Average loss at step 6492000: 4.033956\n",
      "Average loss at step 6494000: 4.032263\n",
      "Average loss at step 6496000: 4.033488\n",
      "Average loss at step 6498000: 4.035449\n",
      "Average loss at step 6500000: 4.032923\n",
      "Average loss at step 6502000: 4.034860\n",
      "Average loss at step 6504000: 4.035222\n",
      "Average loss at step 6506000: 4.033122\n",
      "Average loss at step 6508000: 4.033445\n",
      "Average loss at step 6510000: 4.032843\n",
      "Average loss at step 6512000: 4.032206\n",
      "Average loss at step 6514000: 4.032873\n",
      "Average loss at step 6516000: 4.033706\n",
      "Average loss at step 6518000: 4.034102\n",
      "Average loss at step 6520000: 4.033070\n",
      "Average loss at step 6522000: 4.032112\n",
      "Average loss at step 6524000: 4.035231\n",
      "Average loss at step 6526000: 4.033304\n",
      "Average loss at step 6528000: 4.032764\n",
      "Average loss at step 6530000: 4.032320\n",
      "Average loss at step 6532000: 4.037431\n",
      "Average loss at step 6534000: 4.035003\n",
      "Average loss at step 6536000: 4.034329\n",
      "Average loss at step 6538000: 4.035034\n",
      "Average loss at step 6540000: 4.033362\n",
      "Average loss at step 6542000: 4.033311\n",
      "Average loss at step 6544000: 4.035013\n",
      "Average loss at step 6546000: 4.034520\n",
      "Average loss at step 6548000: 4.033032\n",
      "Average loss at step 6550000: 4.032372\n",
      "Average loss at step 6552000: 4.035306\n",
      "Average loss at step 6554000: 4.033189\n",
      "Average loss at step 6556000: 4.034835\n",
      "Average loss at step 6558000: 4.033456\n",
      "Average loss at step 6560000: 4.033016\n",
      "Average loss at step 6562000: 4.034031\n",
      "Average loss at step 6564000: 4.032285\n",
      "Average loss at step 6566000: 4.034494\n",
      "Average loss at step 6568000: 4.034602\n",
      "Average loss at step 6570000: 4.032530\n",
      "Average loss at step 6572000: 4.033699\n",
      "Average loss at step 6574000: 4.034370\n",
      "Average loss at step 6576000: 4.033088\n",
      "Average loss at step 6578000: 4.033806\n",
      "Average loss at step 6580000: 4.033812\n",
      "Average loss at step 6582000: 4.033954\n",
      "Average loss at step 6584000: 4.032877\n",
      "Average loss at step 6586000: 4.031123\n",
      "Average loss at step 6588000: 4.034556\n",
      "Average loss at step 6590000: 4.033612\n",
      "Average loss at step 6592000: 4.035993\n",
      "Average loss at step 6594000: 4.033898\n",
      "Average loss at step 6596000: 4.032601\n",
      "Average loss at step 6598000: 4.031642\n",
      "Average loss at step 6600000: 4.032585\n",
      "Average loss at step 6602000: 4.036215\n",
      "Average loss at step 6604000: 4.032426\n",
      "Average loss at step 6606000: 4.031575\n",
      "Average loss at step 6608000: 4.031943\n",
      "Average loss at step 6610000: 4.034480\n",
      "Average loss at step 6612000: 4.034598\n",
      "Average loss at step 6614000: 4.034099\n",
      "Average loss at step 6616000: 4.034629\n",
      "Average loss at step 6618000: 4.031575\n",
      "Average loss at step 6620000: 4.033214\n",
      "Average loss at step 6622000: 4.034420\n",
      "Average loss at step 6624000: 4.033386\n",
      "Average loss at step 6626000: 4.034777\n",
      "Average loss at step 6628000: 4.033703\n",
      "Average loss at step 6630000: 4.035275\n",
      "Average loss at step 6632000: 4.035753\n",
      "Average loss at step 6634000: 4.035873\n",
      "Average loss at step 6636000: 4.036500\n",
      "Average loss at step 6638000: 4.035750\n",
      "Average loss at step 6640000: 4.034560\n",
      "Average loss at step 6642000: 4.032047\n",
      "Average loss at step 6644000: 4.032777\n",
      "Average loss at step 6646000: 4.033005\n",
      "Average loss at step 6648000: 4.033899\n",
      "Average loss at step 6650000: 4.032412\n",
      "Average loss at step 6652000: 4.033817\n",
      "Average loss at step 6654000: 4.033919\n",
      "Average loss at step 6656000: 4.032414\n",
      "Average loss at step 6658000: 4.034094\n",
      "Average loss at step 6660000: 4.034727\n",
      "Average loss at step 6662000: 4.032347\n",
      "Average loss at step 6664000: 4.033236\n",
      "Average loss at step 6666000: 4.033824\n",
      "Average loss at step 6668000: 4.034921\n",
      "Average loss at step 6670000: 4.033518\n",
      "Average loss at step 6672000: 4.034691\n",
      "Average loss at step 6674000: 4.035421\n",
      "Average loss at step 6676000: 4.033009\n",
      "Average loss at step 6678000: 4.031909\n",
      "Average loss at step 6680000: 4.034134\n",
      "Average loss at step 6682000: 4.034720\n",
      "Average loss at step 6684000: 4.034710\n",
      "Average loss at step 6686000: 4.031041\n",
      "Average loss at step 6688000: 4.036178\n",
      "Average loss at step 6690000: 4.035480\n",
      "Average loss at step 6692000: 4.033313\n",
      "Average loss at step 6694000: 4.034157\n",
      "Average loss at step 6696000: 4.033433\n",
      "Average loss at step 6698000: 4.032416\n",
      "Average loss at step 6700000: 4.031927\n",
      "Average loss at step 6702000: 4.034660\n",
      "Average loss at step 6704000: 4.030873\n",
      "Average loss at step 6706000: 4.034157\n",
      "Average loss at step 6708000: 4.034303\n",
      "Average loss at step 6710000: 4.032727\n",
      "Average loss at step 6712000: 4.034474\n",
      "Average loss at step 6714000: 4.033666\n",
      "Average loss at step 6716000: 4.033546\n",
      "Average loss at step 6718000: 4.034620\n",
      "Average loss at step 6720000: 4.032232\n",
      "Average loss at step 6722000: 4.032539\n",
      "Average loss at step 6724000: 4.032621\n",
      "Average loss at step 6726000: 4.030457\n",
      "Average loss at step 6728000: 4.037086\n",
      "Average loss at step 6730000: 4.034132\n",
      "Average loss at step 6732000: 4.034339\n",
      "Average loss at step 6734000: 4.033420\n",
      "Average loss at step 6736000: 4.034706\n",
      "Average loss at step 6738000: 4.035010\n",
      "Average loss at step 6740000: 4.033877\n",
      "Average loss at step 6742000: 4.032702\n",
      "Average loss at step 6744000: 4.033297\n",
      "Average loss at step 6746000: 4.032636\n",
      "Average loss at step 6748000: 4.034807\n",
      "Average loss at step 6750000: 4.034537\n",
      "Average loss at step 6752000: 4.033378\n",
      "Average loss at step 6754000: 4.030602\n",
      "Average loss at step 6756000: 4.033285\n",
      "Average loss at step 6758000: 4.032148\n",
      "Average loss at step 6760000: 4.034423\n",
      "Average loss at step 6762000: 4.034491\n",
      "Average loss at step 6764000: 4.031478\n",
      "Average loss at step 6766000: 4.034282\n",
      "Average loss at step 6768000: 4.034343\n",
      "Average loss at step 6770000: 4.035648\n",
      "Average loss at step 6772000: 4.033241\n",
      "Average loss at step 6774000: 4.034656\n",
      "Average loss at step 6776000: 4.035427\n",
      "Average loss at step 6778000: 4.034331\n",
      "Average loss at step 6780000: 4.034982\n",
      "Average loss at step 6782000: 4.035002\n",
      "Average loss at step 6784000: 4.033981\n",
      "Average loss at step 6786000: 4.034225\n",
      "Average loss at step 6788000: 4.036340\n",
      "Average loss at step 6790000: 4.032465\n",
      "Average loss at step 6792000: 4.036540\n",
      "Average loss at step 6794000: 4.035074\n",
      "Average loss at step 6796000: 4.033004\n",
      "Average loss at step 6798000: 4.032795\n",
      "Average loss at step 6800000: 4.034003\n",
      "Average loss at step 6802000: 4.034086\n",
      "Average loss at step 6804000: 4.033299\n",
      "Average loss at step 6806000: 4.036420\n",
      "Average loss at step 6808000: 4.033129\n",
      "Average loss at step 6810000: 4.031911\n",
      "Average loss at step 6812000: 4.033262\n",
      "Average loss at step 6814000: 4.034115\n",
      "Average loss at step 6816000: 4.032562\n",
      "Average loss at step 6818000: 4.033642\n",
      "Average loss at step 6820000: 4.031856\n",
      "Average loss at step 6822000: 4.031575\n",
      "Average loss at step 6824000: 4.034975\n",
      "Average loss at step 6826000: 4.033108\n",
      "Average loss at step 6828000: 4.032639\n",
      "Average loss at step 6830000: 4.034658\n",
      "Average loss at step 6832000: 4.034653\n",
      "Average loss at step 6834000: 4.033688\n",
      "Average loss at step 6836000: 4.034204\n",
      "Average loss at step 6838000: 4.032805\n",
      "Average loss at step 6840000: 4.034584\n",
      "Average loss at step 6842000: 4.032117\n",
      "Average loss at step 6844000: 4.035280\n",
      "Average loss at step 6846000: 4.033500\n",
      "Average loss at step 6848000: 4.032172\n",
      "Average loss at step 6850000: 4.032926\n",
      "Average loss at step 6852000: 4.033227\n",
      "Average loss at step 6854000: 4.033750\n",
      "Average loss at step 6856000: 4.032179\n",
      "Average loss at step 6858000: 4.033013\n",
      "Average loss at step 6860000: 4.035504\n",
      "Average loss at step 6862000: 4.033744\n",
      "Average loss at step 6864000: 4.034079\n",
      "Average loss at step 6866000: 4.034695\n",
      "Average loss at step 6868000: 4.031526\n",
      "Average loss at step 6870000: 4.031739\n",
      "Average loss at step 6872000: 4.031277\n",
      "Average loss at step 6874000: 4.032885\n",
      "Average loss at step 6876000: 4.035338\n",
      "Average loss at step 6878000: 4.031963\n",
      "Average loss at step 6880000: 4.034024\n",
      "Average loss at step 6882000: 4.032421\n",
      "Average loss at step 6884000: 4.035438\n",
      "Average loss at step 6886000: 4.032519\n",
      "Average loss at step 6888000: 4.031892\n",
      "Average loss at step 6890000: 4.029659\n",
      "Average loss at step 6892000: 4.031536\n",
      "Average loss at step 6894000: 4.033734\n",
      "Average loss at step 6896000: 4.035363\n",
      "Average loss at step 6898000: 4.031507\n",
      "Average loss at step 6900000: 4.035625\n",
      "Average loss at step 6902000: 4.034144\n",
      "Average loss at step 6904000: 4.035656\n",
      "Average loss at step 6906000: 4.034987\n",
      "Average loss at step 6908000: 4.033750\n",
      "Average loss at step 6910000: 4.035419\n",
      "Average loss at step 6912000: 4.035371\n",
      "Average loss at step 6914000: 4.034610\n",
      "Average loss at step 6916000: 4.032571\n",
      "Average loss at step 6918000: 4.033844\n",
      "Average loss at step 6920000: 4.037069\n",
      "Average loss at step 6922000: 4.034946\n",
      "Average loss at step 6924000: 4.032145\n",
      "Average loss at step 6926000: 4.032664\n",
      "Average loss at step 6928000: 4.033346\n",
      "Average loss at step 6930000: 4.031656\n",
      "Average loss at step 6932000: 4.034250\n",
      "Average loss at step 6934000: 4.033311\n",
      "Average loss at step 6936000: 4.030881\n",
      "Average loss at step 6938000: 4.032979\n",
      "Average loss at step 6940000: 4.031736\n",
      "Average loss at step 6942000: 4.033718\n",
      "Average loss at step 6944000: 4.033926\n",
      "Average loss at step 6946000: 4.032403\n",
      "Average loss at step 6948000: 4.034306\n",
      "Average loss at step 6950000: 4.033473\n",
      "Average loss at step 6952000: 4.032720\n",
      "Average loss at step 6954000: 4.033004\n",
      "Average loss at step 6956000: 4.032972\n",
      "Average loss at step 6958000: 4.031399\n",
      "Average loss at step 6960000: 4.033085\n",
      "Average loss at step 6962000: 4.035328\n",
      "Average loss at step 6964000: 4.034853\n",
      "Average loss at step 6966000: 4.034055\n",
      "Average loss at step 6968000: 4.035867\n",
      "Average loss at step 6970000: 4.034050\n",
      "Average loss at step 6972000: 4.031773\n",
      "Average loss at step 6974000: 4.035458\n",
      "Average loss at step 6976000: 4.032881\n",
      "Average loss at step 6978000: 4.030415\n",
      "Average loss at step 6980000: 4.034481\n",
      "Average loss at step 6982000: 4.030156\n",
      "Average loss at step 6984000: 4.032291\n",
      "Average loss at step 6986000: 4.034436\n",
      "Average loss at step 6988000: 4.032565\n",
      "Average loss at step 6990000: 4.034147\n",
      "Average loss at step 6992000: 4.033476\n",
      "Average loss at step 6994000: 4.033430\n",
      "Average loss at step 6996000: 4.033113\n",
      "Average loss at step 6998000: 4.032256\n",
      "Average loss at step 7000000: 4.033153\n",
      "Average loss at step 7002000: 4.034648\n",
      "Average loss at step 7004000: 4.032364\n",
      "Average loss at step 7006000: 4.035201\n",
      "Average loss at step 7008000: 4.032386\n",
      "Average loss at step 7010000: 4.033309\n",
      "Average loss at step 7012000: 4.033498\n",
      "Average loss at step 7014000: 4.032510\n",
      "Average loss at step 7016000: 4.033901\n",
      "Average loss at step 7018000: 4.034679\n",
      "Average loss at step 7020000: 4.034183\n",
      "Average loss at step 7022000: 4.035908\n",
      "Average loss at step 7024000: 4.033282\n",
      "Average loss at step 7026000: 4.033797\n",
      "Average loss at step 7028000: 4.033589\n",
      "Average loss at step 7030000: 4.034144\n",
      "Average loss at step 7032000: 4.034909\n",
      "Average loss at step 7034000: 4.033930\n",
      "Average loss at step 7036000: 4.030819\n",
      "Average loss at step 7038000: 4.032661\n",
      "Average loss at step 7040000: 4.032477\n",
      "Average loss at step 7042000: 4.033056\n",
      "Average loss at step 7044000: 4.035764\n",
      "Average loss at step 7046000: 4.032725\n",
      "Average loss at step 7048000: 4.033030\n",
      "Average loss at step 7050000: 4.032885\n",
      "Average loss at step 7052000: 4.034584\n",
      "Average loss at step 7054000: 4.032597\n",
      "Average loss at step 7056000: 4.034050\n",
      "Average loss at step 7058000: 4.032195\n",
      "Average loss at step 7060000: 4.033974\n",
      "Average loss at step 7062000: 4.033488\n",
      "Average loss at step 7064000: 4.033563\n",
      "Average loss at step 7066000: 4.035573\n",
      "Average loss at step 7068000: 4.030355\n",
      "Average loss at step 7070000: 4.034025\n",
      "Average loss at step 7072000: 4.034890\n",
      "Average loss at step 7074000: 4.035078\n",
      "Average loss at step 7076000: 4.031390\n",
      "Average loss at step 7078000: 4.034497\n",
      "Average loss at step 7080000: 4.032748\n",
      "Average loss at step 7082000: 4.032984\n",
      "Average loss at step 7084000: 4.033709\n",
      "Average loss at step 7086000: 4.031441\n",
      "Average loss at step 7088000: 4.032969\n",
      "Average loss at step 7090000: 4.035008\n",
      "Average loss at step 7092000: 4.033019\n",
      "Average loss at step 7094000: 4.030133\n",
      "Average loss at step 7096000: 4.033949\n",
      "Average loss at step 7098000: 4.032415\n",
      "Average loss at step 7100000: 4.034840\n",
      "Average loss at step 7102000: 4.033098\n",
      "Average loss at step 7104000: 4.036632\n",
      "Average loss at step 7106000: 4.033773\n",
      "Average loss at step 7108000: 4.035949\n",
      "Average loss at step 7110000: 4.031378\n",
      "Average loss at step 7112000: 4.033480\n",
      "Average loss at step 7114000: 4.031727\n",
      "Average loss at step 7116000: 4.033401\n",
      "Average loss at step 7118000: 4.031061\n",
      "Average loss at step 7120000: 4.034428\n",
      "Average loss at step 7122000: 4.032077\n",
      "Average loss at step 7124000: 4.035107\n",
      "Average loss at step 7126000: 4.033549\n",
      "Average loss at step 7128000: 4.035185\n",
      "Average loss at step 7130000: 4.033727\n",
      "Average loss at step 7132000: 4.033871\n",
      "Average loss at step 7134000: 4.033011\n",
      "Average loss at step 7136000: 4.033106\n",
      "Average loss at step 7138000: 4.033078\n",
      "Average loss at step 7140000: 4.032595\n",
      "Average loss at step 7142000: 4.033029\n",
      "Average loss at step 7144000: 4.031490\n",
      "Average loss at step 7146000: 4.032257\n",
      "Average loss at step 7148000: 4.033991\n",
      "Average loss at step 7150000: 4.032310\n",
      "Average loss at step 7152000: 4.032183\n",
      "Average loss at step 7154000: 4.032289\n",
      "Average loss at step 7156000: 4.034403\n",
      "Average loss at step 7158000: 4.034520\n",
      "Average loss at step 7160000: 4.033612\n",
      "Average loss at step 7162000: 4.031888\n",
      "Average loss at step 7164000: 4.031153\n",
      "Average loss at step 7166000: 4.030999\n",
      "Average loss at step 7168000: 4.035147\n",
      "Average loss at step 7170000: 4.034116\n",
      "Average loss at step 7172000: 4.033784\n",
      "Average loss at step 7174000: 4.035710\n",
      "Average loss at step 7176000: 4.032976\n",
      "Average loss at step 7178000: 4.035232\n",
      "Average loss at step 7180000: 4.034922\n",
      "Average loss at step 7182000: 4.032807\n",
      "Average loss at step 7184000: 4.035764\n",
      "Average loss at step 7186000: 4.031709\n",
      "Average loss at step 7188000: 4.032517\n",
      "Average loss at step 7190000: 4.033996\n",
      "Average loss at step 7192000: 4.033594\n",
      "Average loss at step 7194000: 4.033832\n",
      "Average loss at step 7196000: 4.033142\n",
      "Average loss at step 7198000: 4.031797\n",
      "Average loss at step 7200000: 4.031920\n",
      "Average loss at step 7202000: 4.031287\n",
      "Average loss at step 7204000: 4.035232\n",
      "Average loss at step 7206000: 4.034999\n",
      "Average loss at step 7208000: 4.034287\n",
      "Average loss at step 7210000: 4.032371\n",
      "Average loss at step 7212000: 4.034754\n",
      "Average loss at step 7214000: 4.032766\n",
      "Average loss at step 7216000: 4.032667\n",
      "Average loss at step 7218000: 4.034472\n",
      "Average loss at step 7220000: 4.032956\n",
      "Average loss at step 7222000: 4.032277\n",
      "Average loss at step 7224000: 4.031415\n",
      "Average loss at step 7226000: 4.034169\n",
      "Average loss at step 7228000: 4.032905\n",
      "Average loss at step 7230000: 4.034485\n",
      "Average loss at step 7232000: 4.032698\n",
      "Average loss at step 7234000: 4.034012\n",
      "Average loss at step 7236000: 4.035725\n",
      "Average loss at step 7238000: 4.033350\n",
      "Average loss at step 7240000: 4.034989\n",
      "Average loss at step 7242000: 4.035252\n",
      "Average loss at step 7244000: 4.031115\n",
      "Average loss at step 7246000: 4.034696\n",
      "Average loss at step 7248000: 4.033628\n",
      "Average loss at step 7250000: 4.029352\n",
      "Average loss at step 7252000: 4.033392\n",
      "Average loss at step 7254000: 4.032086\n",
      "Average loss at step 7256000: 4.034478\n",
      "Average loss at step 7258000: 4.034682\n",
      "Average loss at step 7260000: 4.035263\n",
      "Average loss at step 7262000: 4.032554\n",
      "Average loss at step 7264000: 4.034364\n",
      "Average loss at step 7266000: 4.032404\n",
      "Average loss at step 7268000: 4.034128\n",
      "Average loss at step 7270000: 4.033522\n",
      "Average loss at step 7272000: 4.033901\n",
      "Average loss at step 7274000: 4.035861\n",
      "Average loss at step 7276000: 4.033015\n",
      "Average loss at step 7278000: 4.033888\n",
      "Average loss at step 7280000: 4.033307\n",
      "Average loss at step 7282000: 4.034989\n",
      "Average loss at step 7284000: 4.032401\n",
      "Average loss at step 7286000: 4.034265\n",
      "Average loss at step 7288000: 4.033418\n",
      "Average loss at step 7290000: 4.034274\n",
      "Average loss at step 7292000: 4.034196\n",
      "Average loss at step 7294000: 4.035118\n",
      "Average loss at step 7296000: 4.035399\n",
      "Average loss at step 7298000: 4.034980\n",
      "Average loss at step 7300000: 4.032203\n",
      "Average loss at step 7302000: 4.032846\n",
      "Average loss at step 7304000: 4.034783\n",
      "Average loss at step 7306000: 4.034074\n",
      "Average loss at step 7308000: 4.032134\n",
      "Average loss at step 7310000: 4.033419\n",
      "Average loss at step 7312000: 4.033129\n",
      "Average loss at step 7314000: 4.034739\n",
      "Average loss at step 7316000: 4.033870\n",
      "Average loss at step 7318000: 4.032317\n",
      "Average loss at step 7320000: 4.033401\n",
      "Average loss at step 7322000: 4.033018\n",
      "Average loss at step 7324000: 4.034467\n",
      "Average loss at step 7326000: 4.032650\n",
      "Average loss at step 7328000: 4.032992\n",
      "Average loss at step 7330000: 4.032816\n",
      "Average loss at step 7332000: 4.032842\n",
      "Average loss at step 7334000: 4.036251\n",
      "Average loss at step 7336000: 4.031630\n",
      "Average loss at step 7338000: 4.035911\n",
      "Average loss at step 7340000: 4.032936\n",
      "Average loss at step 7342000: 4.034949\n",
      "Average loss at step 7344000: 4.033837\n",
      "Average loss at step 7346000: 4.032628\n",
      "Average loss at step 7348000: 4.032921\n",
      "Average loss at step 7350000: 4.035206\n",
      "Average loss at step 7352000: 4.035829\n",
      "Average loss at step 7354000: 4.032068\n",
      "Average loss at step 7356000: 4.033453\n",
      "Average loss at step 7358000: 4.034204\n",
      "Average loss at step 7360000: 4.033560\n",
      "Average loss at step 7362000: 4.033309\n",
      "Average loss at step 7364000: 4.032468\n",
      "Average loss at step 7366000: 4.030656\n",
      "Average loss at step 7368000: 4.032898\n",
      "Average loss at step 7370000: 4.033502\n",
      "Average loss at step 7372000: 4.031073\n",
      "Average loss at step 7374000: 4.032605\n",
      "Average loss at step 7376000: 4.035772\n",
      "Average loss at step 7378000: 4.034364\n",
      "Average loss at step 7380000: 4.034753\n",
      "Average loss at step 7382000: 4.032373\n",
      "Average loss at step 7384000: 4.032838\n",
      "Average loss at step 7386000: 4.032243\n",
      "Average loss at step 7388000: 4.031321\n",
      "Average loss at step 7390000: 4.032166\n",
      "Average loss at step 7392000: 4.035259\n",
      "Average loss at step 7394000: 4.033257\n",
      "Average loss at step 7396000: 4.032518\n",
      "Average loss at step 7398000: 4.034456\n",
      "Average loss at step 7400000: 4.032673\n",
      "Average loss at step 7402000: 4.034344\n",
      "Average loss at step 7404000: 4.033019\n",
      "Average loss at step 7406000: 4.032287\n",
      "Average loss at step 7408000: 4.032635\n",
      "Average loss at step 7410000: 4.034231\n",
      "Average loss at step 7412000: 4.035004\n",
      "Average loss at step 7414000: 4.034060\n",
      "Average loss at step 7416000: 4.031436\n",
      "Average loss at step 7418000: 4.033164\n",
      "Average loss at step 7420000: 4.035250\n",
      "Average loss at step 7422000: 4.033348\n",
      "Average loss at step 7424000: 4.031491\n",
      "Average loss at step 7426000: 4.030835\n",
      "Average loss at step 7428000: 4.034641\n",
      "Average loss at step 7430000: 4.034506\n",
      "Average loss at step 7432000: 4.033023\n",
      "Average loss at step 7434000: 4.032549\n",
      "Average loss at step 7436000: 4.030930\n",
      "Average loss at step 7438000: 4.032998\n",
      "Average loss at step 7440000: 4.033489\n",
      "Average loss at step 7442000: 4.034105\n",
      "Average loss at step 7444000: 4.033171\n",
      "Average loss at step 7446000: 4.035555\n",
      "Average loss at step 7448000: 4.032827\n",
      "Average loss at step 7450000: 4.035102\n",
      "Average loss at step 7452000: 4.031899\n",
      "Average loss at step 7454000: 4.034776\n",
      "Average loss at step 7456000: 4.035011\n",
      "Average loss at step 7458000: 4.032890\n",
      "Average loss at step 7460000: 4.032382\n",
      "Average loss at step 7462000: 4.032529\n",
      "Average loss at step 7464000: 4.034564\n",
      "Average loss at step 7466000: 4.032089\n",
      "Average loss at step 7468000: 4.032589\n",
      "Average loss at step 7470000: 4.034003\n",
      "Average loss at step 7472000: 4.032318\n",
      "Average loss at step 7474000: 4.030865\n",
      "Average loss at step 7476000: 4.032699\n",
      "Average loss at step 7478000: 4.033782\n",
      "Average loss at step 7480000: 4.034308\n",
      "Average loss at step 7482000: 4.032736\n",
      "Average loss at step 7484000: 4.031484\n",
      "Average loss at step 7486000: 4.032738\n",
      "Average loss at step 7488000: 4.033102\n",
      "Average loss at step 7490000: 4.034153\n",
      "Average loss at step 7492000: 4.031739\n",
      "Average loss at step 7494000: 4.031697\n",
      "Average loss at step 7496000: 4.033115\n",
      "Average loss at step 7498000: 4.032980\n",
      "Average loss at step 7500000: 4.033506\n",
      "Average loss at step 7502000: 4.032009\n",
      "Average loss at step 7504000: 4.032483\n",
      "Average loss at step 7506000: 4.032598\n",
      "Average loss at step 7508000: 4.034517\n",
      "Average loss at step 7510000: 4.033670\n",
      "Average loss at step 7512000: 4.034081\n",
      "Average loss at step 7514000: 4.034509\n",
      "Average loss at step 7516000: 4.030615\n",
      "Average loss at step 7518000: 4.033491\n",
      "Average loss at step 7520000: 4.032879\n",
      "Average loss at step 7522000: 4.031094\n",
      "Average loss at step 7524000: 4.033153\n",
      "Average loss at step 7526000: 4.032415\n",
      "Average loss at step 7528000: 4.032049\n",
      "Average loss at step 7530000: 4.034447\n",
      "Average loss at step 7532000: 4.035093\n",
      "Average loss at step 7534000: 4.033744\n",
      "Average loss at step 7536000: 4.033318\n",
      "Average loss at step 7538000: 4.034916\n",
      "Average loss at step 7540000: 4.030703\n",
      "Average loss at step 7542000: 4.031339\n",
      "Average loss at step 7544000: 4.029937\n",
      "Average loss at step 7546000: 4.036131\n",
      "Average loss at step 7548000: 4.031581\n",
      "Average loss at step 7550000: 4.029382\n",
      "Average loss at step 7552000: 4.034434\n",
      "Average loss at step 7554000: 4.034231\n",
      "Average loss at step 7556000: 4.035029\n",
      "Average loss at step 7558000: 4.034389\n",
      "Average loss at step 7560000: 4.033306\n",
      "Average loss at step 7562000: 4.032577\n",
      "Average loss at step 7564000: 4.034298\n",
      "Average loss at step 7566000: 4.035488\n",
      "Average loss at step 7568000: 4.035388\n",
      "Average loss at step 7570000: 4.034810\n",
      "Average loss at step 7572000: 4.032161\n",
      "Average loss at step 7574000: 4.034673\n",
      "Average loss at step 7576000: 4.033554\n",
      "Average loss at step 7578000: 4.032579\n",
      "Average loss at step 7580000: 4.033626\n",
      "Average loss at step 7582000: 4.031639\n",
      "Average loss at step 7584000: 4.032978\n",
      "Average loss at step 7586000: 4.034298\n",
      "Average loss at step 7588000: 4.032370\n",
      "Average loss at step 7590000: 4.036290\n",
      "Average loss at step 7592000: 4.034315\n",
      "Average loss at step 7594000: 4.033928\n",
      "Average loss at step 7596000: 4.035496\n",
      "Average loss at step 7598000: 4.033004\n",
      "Average loss at step 7600000: 4.032828\n",
      "Average loss at step 7602000: 4.032917\n",
      "Average loss at step 7604000: 4.034831\n",
      "Average loss at step 7606000: 4.032723\n",
      "Average loss at step 7608000: 4.033491\n",
      "Average loss at step 7610000: 4.032167\n",
      "Average loss at step 7612000: 4.033349\n",
      "Average loss at step 7614000: 4.034249\n",
      "Average loss at step 7616000: 4.032703\n",
      "Average loss at step 7618000: 4.033938\n",
      "Average loss at step 7620000: 4.034743\n",
      "Average loss at step 7622000: 4.034710\n",
      "Average loss at step 7624000: 4.035253\n",
      "Average loss at step 7626000: 4.032304\n",
      "Average loss at step 7628000: 4.031612\n",
      "Average loss at step 7630000: 4.033718\n",
      "Average loss at step 7632000: 4.032250\n",
      "Average loss at step 7634000: 4.032310\n",
      "Average loss at step 7636000: 4.034152\n",
      "Average loss at step 7638000: 4.032962\n",
      "Average loss at step 7640000: 4.032962\n",
      "Average loss at step 7642000: 4.032927\n",
      "Average loss at step 7644000: 4.032766\n",
      "Average loss at step 7646000: 4.031367\n",
      "Average loss at step 7648000: 4.034420\n",
      "Average loss at step 7650000: 4.034396\n",
      "Average loss at step 7652000: 4.034573\n",
      "Average loss at step 7654000: 4.034471\n",
      "Average loss at step 7656000: 4.033862\n",
      "Average loss at step 7658000: 4.032740\n",
      "Average loss at step 7660000: 4.033183\n",
      "Average loss at step 7662000: 4.031829\n",
      "Average loss at step 7664000: 4.035489\n",
      "Average loss at step 7666000: 4.034381\n",
      "Average loss at step 7668000: 4.032067\n",
      "Average loss at step 7670000: 4.033271\n",
      "Average loss at step 7672000: 4.033826\n",
      "Average loss at step 7674000: 4.031729\n",
      "Average loss at step 7676000: 4.032431\n",
      "Average loss at step 7678000: 4.034895\n",
      "Average loss at step 7680000: 4.033189\n",
      "Average loss at step 7682000: 4.033423\n",
      "Average loss at step 7684000: 4.034681\n",
      "Average loss at step 7686000: 4.031234\n",
      "Average loss at step 7688000: 4.032507\n",
      "Average loss at step 7690000: 4.035374\n",
      "Average loss at step 7692000: 4.032768\n",
      "Average loss at step 7694000: 4.031891\n",
      "Average loss at step 7696000: 4.032884\n",
      "Average loss at step 7698000: 4.032426\n",
      "Average loss at step 7700000: 4.034601\n",
      "Average loss at step 7702000: 4.034784\n",
      "Average loss at step 7704000: 4.031581\n",
      "Average loss at step 7706000: 4.031875\n",
      "Average loss at step 7708000: 4.032342\n",
      "Average loss at step 7710000: 4.031712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-398-7c9b75ba9e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         batch_data, batch_labels = generate_batch(\n\u001b[0;32m----> 9\u001b[0;31m             batch_size, num_skips, skip_window)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-330-c48ae5c78893>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(batch_size, num_skips, skip_window)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_skips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets_to_avoid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtargets_to_avoid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_skips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskip_window\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \"\"\"\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF,\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int, _maxwidth)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# This code is a bit messy to make it fast for the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# common case while still doing adequate error checking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mistart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"non-integer arg 1 for randrange()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings_iter%d.txt' %step\n",
    "            not_normal_embeddings = embeddings.eval()\n",
    "            ordered_embeddings = [not_normal_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "            np.savetxt(embedding_filename, ordered_embeddings)\n",
    "#             sim = similarity.eval()\n",
    "#             for i in range(valid_size):\n",
    "#                 valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#                 top_k = 8 # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "#                 log = 'Nearest to %s:' % valid_word\n",
    "#                 for k in range(top_k):\n",
    "#                     close_word = reverse_dictionary[nearest[k]]\n",
    "#                     log = '%s %s,' % (log, close_word)\n",
    "#                 print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    not_normal_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings_1.txt'\n",
    "ordered_embeddings = [final_embeddings[dictionary[str(node)]] for node in range(len(dictionary))]\n",
    "np.savetxt(embedding_filename, ordered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58.47768402, -14.06970406, -11.84331131,   3.77727461,\n",
       "        27.18597221, -25.0004673 ,  65.72541809, -65.32319641,\n",
       "        46.84730148, -17.94046402,  -9.88634205,   4.96879911,\n",
       "       -39.52791977, -41.69575119,  74.56922913,   7.61631155,\n",
       "        23.03376389,  50.97651291, -33.59208679,  31.2266407 ,\n",
       "        81.93914032,  15.72993279,  -5.28370953,  17.18107605,\n",
       "       -75.29068756,  14.2514925 ,   7.94441748,  -4.89663982,\n",
       "       -51.30142212, -33.08482361, -32.42542267,   1.93974102,\n",
       "        46.80765533,   8.50759125,  10.44501019, -40.15472794,\n",
       "        -5.02595329, -36.50287247, -45.88523102, -79.72190094,\n",
       "       -17.26217461,   8.80545712, -52.35463333, -16.01053047,\n",
       "       -43.30438232, -39.79665756, -16.72794151,  34.34871674,\n",
       "        57.16449738,  -4.06061411, -72.57341003,  -7.9510541 ,\n",
       "        63.76834869,  43.05852509,   2.076653  , -98.20155334,\n",
       "        16.07836723,  46.57059479,  68.2883606 ,  -1.08950758,\n",
       "         5.00839376,   9.26826859,   8.00768375,  18.40788651,\n",
       "        57.93119049,  69.93483734, -30.36079407, -24.08821869,\n",
       "       -98.82420349,  56.28701782,  -6.49470615, -29.10713005,\n",
       "         9.16787815,  29.59499741,  82.98940277,  83.68479919,\n",
       "       -54.79275894,  25.96276855,  22.00436592, -61.77494812,\n",
       "       -33.84917831, -51.7205925 , -47.96949768, -18.86766052,\n",
       "       -10.83108044,   3.68754077,  65.11179352,  28.03749084,\n",
       "       -43.74489975,  14.33013916,  11.00716972,  48.05020142,\n",
       "       -17.2221508 ,  46.67128754,   9.17617607, -66.37744141,\n",
       "        28.96920204, -23.5886116 ,  48.79130173,   3.59684467,\n",
       "       -10.80480194,  -7.6785965 ,  23.84983635, -46.33642578,\n",
       "       -20.64024162,   0.37871927,  22.73223305,  52.55372238,\n",
       "        10.19163036, -11.5577383 ,  -4.63438082,   3.70495296,\n",
       "         7.59680653,  27.99442673,  29.33081055, -51.17831421,\n",
       "        59.33683395,  11.71917534,   9.37813663, -17.67382622,\n",
       "       -19.4407177 , -19.11290359, -16.30101204, -26.50016785,\n",
       "        37.48277664,   3.48921919,  24.05833054,  27.21180725], dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_embeddings[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(embedding_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58.47768402, -14.06970406, -11.84331131,   3.77727461,\n",
       "        27.18597221, -25.0004673 ,  65.72541809, -65.32319641,\n",
       "        46.84730148, -17.94046402,  -9.88634205,   4.96879911,\n",
       "       -39.52791977, -41.69575119,  74.56922913,   7.61631155,\n",
       "        23.03376389,  50.97651291, -33.59208679,  31.2266407 ,\n",
       "        81.93914032,  15.72993279,  -5.28370953,  17.18107605,\n",
       "       -75.29068756,  14.2514925 ,   7.94441748,  -4.89663982,\n",
       "       -51.30142212, -33.08482361, -32.42542267,   1.93974102,\n",
       "        46.80765533,   8.50759125,  10.44501019, -40.15472794,\n",
       "        -5.02595329, -36.50287247, -45.88523102, -79.72190094,\n",
       "       -17.26217461,   8.80545712, -52.35463333, -16.01053047,\n",
       "       -43.30438232, -39.79665756, -16.72794151,  34.34871674,\n",
       "        57.16449738,  -4.06061411, -72.57341003,  -7.9510541 ,\n",
       "        63.76834869,  43.05852509,   2.076653  , -98.20155334,\n",
       "        16.07836723,  46.57059479,  68.2883606 ,  -1.08950758,\n",
       "         5.00839376,   9.26826859,   8.00768375,  18.40788651,\n",
       "        57.93119049,  69.93483734, -30.36079407, -24.08821869,\n",
       "       -98.82420349,  56.28701782,  -6.49470615, -29.10713005,\n",
       "         9.16787815,  29.59499741,  82.98940277,  83.68479919,\n",
       "       -54.79275894,  25.96276855,  22.00436592, -61.77494812,\n",
       "       -33.84917831, -51.7205925 , -47.96949768, -18.86766052,\n",
       "       -10.83108044,   3.68754077,  65.11179352,  28.03749084,\n",
       "       -43.74489975,  14.33013916,  11.00716972,  48.05020142,\n",
       "       -17.2221508 ,  46.67128754,   9.17617607, -66.37744141,\n",
       "        28.96920204, -23.5886116 ,  48.79130173,   3.59684467,\n",
       "       -10.80480194,  -7.6785965 ,  23.84983635, -46.33642578,\n",
       "       -20.64024162,   0.37871927,  22.73223305,  52.55372238,\n",
       "        10.19163036, -11.5577383 ,  -4.63438082,   3.70495296,\n",
       "         7.59680653,  27.99442673,  29.33081055, -51.17831421,\n",
       "        59.33683395,  11.71917534,   9.37813663, -17.67382622,\n",
       "       -19.4407177 , -19.11290359, -16.30101204, -26.50016785,\n",
       "        37.48277664,   3.48921919,  24.05833054,  27.21180725])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[1209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 10308, 10309, 10310])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00, ...,\n",
       "         1.03090000e+04,   1.03100000e+04,   1.03110000e+04])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10312"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10311"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocabulary_size)\n",
    "couples, labels = skipgrams(data, vocabulary_size, window_size=10, sampling_table=sampling_table, negative_samples=5)\n",
    "# word_target, word_context = zip(*couples)\n",
    "# word_target = np.array(word_target, dtype=\"int32\")\n",
    "# word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "# print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824960\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in walks_corpus:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = Skipgram(sentences=walks_corpus, vocabulary_counts=vocabulary_size, size=128,\n",
    "#                  window=10, min_count=0, trim_rule=None, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse2graph(x):\n",
    "    G = defaultdict(lambda: set())\n",
    "    cx = x.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "        G[i].add(j)\n",
    "    return {str(k): [str(x) for x in v] for k,v in iteritems(G)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert X.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scoring(emb_filename, matfile):\n",
    "    # 0. Files\n",
    "    embeddings_file = emb_filename\n",
    "\n",
    "    # 1. Load Embeddings\n",
    "#     embeddings = np.loadtxt(embeddings_file)\n",
    "    \n",
    "    ## for original deepwalk\n",
    "    #model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)\n",
    "    \n",
    "    ## for external word2vec lib\n",
    "    model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')\n",
    "\n",
    "    # 2. Load labels\n",
    "    mat = sio.loadmat(matfile)\n",
    "    A = mat['network']\n",
    "    graph = sparse2graph(A)\n",
    "    labels_matrix = mat['group']\n",
    "    labels_count = labels_matrix.shape[1]\n",
    "    mlb = MultiLabelBinarizer(range(labels_count))\n",
    "\n",
    "    # Map nodes to their features (note:  assumes nodes are labeled as integers 1:N)\n",
    "#     features_matrix = embeddings\n",
    "\n",
    "#     features_matrix = np.asarray([model[str(node)] for node in range(len(graph))])\n",
    "\n",
    "    # use other word2vec lib\n",
    "    features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])\n",
    "    \n",
    "    # 2. Shuffle, to create train/test groups\n",
    "    shuffles = []\n",
    "    for x in range(1):\n",
    "        shuffles.append(skshuffle(features_matrix, labels_matrix))\n",
    "\n",
    "    # 3. to score each train/test group\n",
    "    all_results = defaultdict(list)\n",
    "\n",
    "#     if args.all:\n",
    "#         training_percents = numpy.asarray(range(1, 10)) * .1\n",
    "#     else:\n",
    "#         training_percents = [0.1, 0.5, 0.9]\n",
    "    training_percents = [0.1]\n",
    "    for train_percent in training_percents:\n",
    "        for shuf in shuffles:\n",
    "            \n",
    "            X, y = shuf\n",
    "\n",
    "            training_size = int(train_percent * X.shape[0])\n",
    "\n",
    "            X_train = X[:training_size, :]\n",
    "            y_train_ = y[:training_size]\n",
    "\n",
    "            y_train = [[] for x in range(y_train_.shape[0])]\n",
    "\n",
    "\n",
    "            cy =  y_train_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_train[i].append(j)\n",
    "\n",
    "            assert sum(len(l) for l in y_train) == y_train_.nnz\n",
    "\n",
    "            X_test = X[training_size:, :]\n",
    "            y_test_ = y[training_size:]\n",
    "\n",
    "            y_test = [[] for _ in range(y_test_.shape[0])]\n",
    "\n",
    "            cy =  y_test_.tocoo()\n",
    "            for i, j in zip(cy.row, cy.col):\n",
    "                y_test[i].append(j)\n",
    "\n",
    "            clf = TopKRanker(LogisticRegression())\n",
    "            clf.fit(X_train, y_train_)\n",
    "\n",
    "            # find out how many labels should be predicted\n",
    "            top_k_list = [len(l) for l in y_test]\n",
    "            preds = clf.predict(X_test, top_k_list)\n",
    "\n",
    "            results = {}\n",
    "            averages = [\"micro\", \"macro\"]\n",
    "            for average in averages:\n",
    "                results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
    "\n",
    "            all_results[train_percent].append(results)\n",
    "\n",
    "    print ('Results, using embeddings of dimensionality', X.shape[1])\n",
    "    print ('-------------------')\n",
    "    for train_percent in sorted(all_results.keys()):\n",
    "        print ('Train percent:', train_percent)\n",
    "    for index, result in enumerate(all_results[train_percent]):\n",
    "        print ('Shuffle #%d:   ' % (index + 1), result)\n",
    "    avg_score = defaultdict(float)\n",
    "    for score_dict in all_results[train_percent]:\n",
    "        for metric, score in iteritems(score_dict):\n",
    "            avg_score[metric] += score\n",
    "    for metric in avg_score:\n",
    "        avg_score[metric] /= len(all_results[train_percent])\n",
    "    print ('Average score:', dict(avg_score))\n",
    "    print ('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8496e862a2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_filename_other_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-e951e3ee7ec9>\u001b[0m in \u001b[0;36mscoring\u001b[0;34m(emb_filename, matfile)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m## for external word2vec lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 2. Load labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "matfile = '/hdd2/graph_embedding/deepwalk/example_graphs/blogcatalog.mat'\n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "embedding_filename_other_lib = '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin'\n",
    "\n",
    "scoring('', matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_filename_original = '/hdd2/graph_embedding/customized/model_ns5_iter1.output'\n",
    "model = KeyedVectors.load_word2vec_format(embedding_filename_original, binary=False)\n",
    "                                          \n",
    "embedding_filename = '/hdd2/graph_embedding/customized/blog_embeddings.iter2.txt'\n",
    "embeddings = np.loadtxt(embedding_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.110529  ,  0.19354101,  0.3969    ,  0.16305301, -0.198276  ,\n",
       "       -0.219273  ,  0.045498  ,  0.138824  , -0.174495  ,  0.20618799,\n",
       "        0.48984301,  0.302856  , -0.46013999,  0.227097  ,  0.071323  ,\n",
       "       -0.43010399,  0.015921  , -0.021136  ,  0.28175899, -0.26225701,\n",
       "        0.102745  , -0.31963301,  0.058964  ,  0.132149  ,  0.176946  ,\n",
       "       -0.155433  ,  0.160973  , -0.282166  ,  0.030017  , -0.165079  ,\n",
       "        0.25168899, -0.52189702,  0.005448  ,  0.18592501, -0.013992  ,\n",
       "       -0.070675  ,  0.033961  ,  0.117675  , -0.073083  ,  0.068748  ,\n",
       "       -0.10755   ,  0.068676  ,  0.17162   , -0.136898  ,  0.17979699,\n",
       "       -0.106551  , -0.212037  ,  0.103523  , -0.242975  , -0.46731299,\n",
       "        0.182107  ,  0.092075  , -0.141946  ,  0.051342  ,  0.31653801,\n",
       "        0.28085399, -0.029812  ,  0.19881   ,  0.31846499, -0.12293   ,\n",
       "        0.22415499, -0.21315201, -0.220193  , -0.15813901,  0.104372  ,\n",
       "       -0.038969  ,  0.020534  , -0.18385699, -0.049377  ,  0.241363  ,\n",
       "        0.15504301,  0.27251899, -0.20202599,  0.074011  ,  0.248133  ,\n",
       "       -0.015073  ,  0.046193  , -0.124894  , -0.06814   , -0.128296  ,\n",
       "        0.020906  , -0.07755   , -0.172672  ,  0.032093  ,  0.156846  ,\n",
       "       -0.019415  ,  0.051345  , -0.13377801, -0.195548  ,  0.128029  ,\n",
       "        0.066909  ,  0.13171799, -0.214425  ,  0.097262  , -0.20645601,\n",
       "       -0.114667  , -0.193297  , -0.218493  , -0.14356001, -0.06486   ,\n",
       "        0.078362  ,  0.043282  , -0.195365  ,  0.035029  , -0.100952  ,\n",
       "        0.028167  , -0.254026  ,  0.080769  ,  0.138097  , -0.118832  ,\n",
       "        0.22276799, -0.099504  ,  0.045412  ,  0.141414  ,  0.123148  ,\n",
       "        0.13963801, -0.004271  ,  0.172041  ,  0.12623601,  0.24216899,\n",
       "       -0.219863  ,  0.27274099,  0.026154  ,  0.10464   , -0.023742  ,\n",
       "       -0.206168  , -0.33293399, -0.034799  ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.47322476e-01,   2.67052501e-02,   1.52672976e-02,\n",
       "        -1.37635857e-01,  -6.47586957e-02,  -2.46349778e-02,\n",
       "        -8.18009302e-02,   6.91710338e-02,  -9.66271833e-02,\n",
       "        -9.89247933e-02,   5.70955575e-02,   1.53366596e-01,\n",
       "         8.30798447e-02,  -1.44452751e-01,   9.11711424e-04,\n",
       "         1.01996966e-01,   4.97803837e-02,  -4.91250493e-02,\n",
       "         1.03872299e-01,  -1.11238003e-01,   3.35200056e-02,\n",
       "         6.37169033e-02,  -3.92291024e-02,  -2.92820130e-02,\n",
       "        -2.85616820e-03,  -3.53677608e-02,  -5.77375665e-02,\n",
       "        -3.55093554e-02,   4.43368331e-02,  -1.63600937e-01,\n",
       "        -5.06901741e-02,   1.09737791e-01,  -6.03695633e-03,\n",
       "        -9.09792027e-04,  -9.44678709e-02,  -5.35581484e-02,\n",
       "         5.67527264e-02,  -2.60556079e-02,   1.89456977e-02,\n",
       "         2.38609854e-02,   4.84210551e-02,   1.34210438e-01,\n",
       "        -1.30895942e-01,   6.51589632e-02,   6.92558140e-02,\n",
       "        -5.88307390e-03,  -1.87402926e-02,   1.52439341e-01,\n",
       "        -7.65966475e-02,  -1.14347152e-02,  -5.93073331e-02,\n",
       "         3.97681519e-02,  -2.26105414e-02,  -1.00840345e-01,\n",
       "        -2.73720361e-02,  -1.35795742e-01,  -7.55931512e-02,\n",
       "         6.89051822e-02,  -9.89682674e-02,   8.19836278e-03,\n",
       "         9.56752300e-02,   1.11364149e-01,  -5.73285446e-02,\n",
       "        -7.95298442e-02,  -6.94709942e-02,   1.97762456e-02,\n",
       "        -7.93395117e-02,   5.48537821e-02,  -7.36792460e-02,\n",
       "         4.13507484e-02,   1.48582190e-01,  -1.37633294e-01,\n",
       "        -1.10649310e-01,   5.96768968e-02,   9.31203440e-02,\n",
       "         1.78780377e-01,  -1.70841515e-01,   9.95152742e-02,\n",
       "         9.34500918e-02,   1.38932645e-01,  -1.15846768e-01,\n",
       "         3.72944437e-02,  -1.21906027e-01,  -1.49959236e-01,\n",
       "        -6.60825595e-02,   8.33349824e-02,   2.63328534e-02,\n",
       "         3.53457741e-02,   1.41411439e-01,  -7.07744882e-02,\n",
       "         1.21387757e-01,   5.68825640e-02,  -1.21080384e-01,\n",
       "         1.42978668e-01,  -1.67644396e-01,   1.68275498e-02,\n",
       "        -9.96617302e-02,   4.47709225e-02,   2.08472624e-01,\n",
       "        -1.08954571e-01,  -8.83026272e-02,  -1.23706110e-01,\n",
       "        -5.51782846e-02,   1.30027205e-01,   5.61708212e-02,\n",
       "         7.12516978e-02,   3.37933712e-02,   8.94594342e-02,\n",
       "         1.06588854e-02,  -2.62339506e-02,   4.57158871e-02,\n",
       "        -2.36465354e-02,   1.83984548e-01,  -5.11433706e-02,\n",
       "         1.30957412e-02,   1.15885787e-01,   1.39660118e-02,\n",
       "         2.78853234e-02,  -1.25376284e-01,  -4.51841056e-02,\n",
       "         2.44418718e-02,  -9.92230922e-02,   7.17640370e-02,\n",
       "        -5.39226495e-02,   9.22740158e-03,   1.02272414e-01,\n",
       "        -8.26397240e-02,   1.59677104e-04])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sio.loadmat(matfile)\n",
    "A = mat['network']\n",
    "labels_matrix = mat['group']\n",
    "labels_count = labels_matrix.shape[1]\n",
    "mlb = MultiLabelBinarizer(range(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MultiLabelBinarizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fd4e0751c0ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MultiLabelBinarizer' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-13 21:07:11,853 : INFO : collecting all words and their counts\n",
      "2018-03-13 21:07:11,855 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-13 21:07:11,857 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-03-13 21:07:11,858 : INFO : Loading a fresh vocabulary\n",
      "2018-03-13 21:07:11,860 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-03-13 21:07:11,863 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-03-13 21:07:11,865 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-03-13 21:07:11,868 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-03-13 21:07:11,870 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-03-13 21:07:11,872 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-03-13 21:07:11,874 : INFO : resetting layer weights\n",
      "2018-03-13 21:07:11,876 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-03-13 21:07:11,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,883 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,892 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,900 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 183 effective words/s\n",
      "2018-03-13 21:07:11,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,907 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-13 21:07:11,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-13 21:07:11,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-13 21:07:11,916 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-13 21:07:11,917 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 25 effective words/s\n",
      "2018-03-13 21:07:11,918 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_iter_label = 10\n",
    "init_iter_graph = 7\n",
    "inst_generator = gen_train_inst()\n",
    "graph_generator = gen_graph()\n",
    "# Generates pairs with the same label (1) or different labels (-1)\n",
    "label_generator = gen_label_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1\n",
    "iter_graph = 0\n",
    "iter_inst = 5\n",
    "iter_label = 0\n",
    "use_reweight = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     gx, gy = next(label_generator)\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     feed_dict={g_sym: gx[:, 0], gy_sym: gx[:, 1].reshape(gx.shape[0],1)}\n",
    "#     _, loss = session.run([g_optimizer, g_loss], feed_dict=feed_dict)\n",
    "#     print 'iter label', i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00497301], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_reweight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "# #     tf.global_variables_initializer().run()\n",
    "#     feed_dict={x_sym: xs, y_sym: ys}\n",
    "#     ll, l = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8\n",
      "Words processed: 17000K     Vocab size: 4399K  \n",
      "Vocab size (unigrams + bigrams): 2419827\n",
      "Words in train file: 17005206\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('/hdd2/graph_embedding/tmp/text8', '/hdd2/graph_embedding/tmp/text8-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /hdd2/graph_embedding/tmp/text8-phrases\n",
      "Vocab size: 98331\n",
      "Words in train file: 15857306\n",
      "Alpha: 0.000002  Progress: 100.04%  Words/thread/sec: 467.25k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('/hdd2/graph_embedding/tmp/text8-phrases', '/hdd2/graph_embedding/tmp/text8.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 7] Argument list too long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-bd9adf4b8805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word2vec.word2vec(data,\n\u001b[1;32m      2\u001b[0m                   \u001b[0;34m'/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.1.bin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                   size=128, verbose=True)\n\u001b[0m",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/word2vec/scripts_interface.pyc\u001b[0m in \u001b[0;36mword2vec\u001b[0;34m(train, output, size, window, sample, hs, negative, threads, iter_, min_count, alpha, debug, binary, cbow, save_vocab, read_vocab, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/site-packages/word2vec/scripts_interface.pyc\u001b[0m in \u001b[0;36mrun_cmd\u001b[0;34m(command, verbose)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     proc = subprocess.Popen(command, stdout=subprocess.PIPE,\n\u001b[0;32m--> 142\u001b[0;31m                             stderr=subprocess.PIPE)\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mingzeng/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 7] Argument list too long"
     ]
    }
   ],
   "source": [
    "word2vec.word2vec(data,\n",
    "                  '/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.1.bin', \n",
    "                  size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.load('/hdd2/graph_embedding/tmp/blogcatalog.embeddings.walks.0.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_matrix = np.asarray([model[str(node)] for node in xrange(len(model.vocab)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'4838', u'175', ..., u'9756', u'1678', u'2931'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03300432, -0.07565996, -0.16463131, -0.10145742,  0.00168844,\n",
       "       -0.09007662, -0.03259023, -0.22714335, -0.00282227, -0.05083675,\n",
       "       -0.07032426, -0.15832779, -0.13998848,  0.07574239,  0.01739285,\n",
       "        0.05170207, -0.01704509, -0.04309488,  0.13984987, -0.07098495,\n",
       "       -0.08131668, -0.06108802,  0.03238546,  0.03199002, -0.14047897,\n",
       "        0.1192169 , -0.11983237,  0.17141342, -0.04238614, -0.07682881,\n",
       "       -0.09226497, -0.08641661, -0.05119943,  0.04880232,  0.05202007,\n",
       "        0.11911941,  0.02494194,  0.0460702 , -0.02544389,  0.02227429,\n",
       "       -0.02918422,  0.08963519, -0.01449389,  0.13147458,  0.04316   ,\n",
       "        0.05062373,  0.01030556,  0.14064927,  0.05300813,  0.09054667,\n",
       "        0.16458763,  0.00496632,  0.09853401, -0.09157445,  0.03466847,\n",
       "       -0.192109  , -0.04286476,  0.0337339 ,  0.09414463,  0.02883872,\n",
       "       -0.12935385,  0.05580147,  0.04444493,  0.12305645, -0.06843153,\n",
       "       -0.05650309, -0.06930542, -0.0933516 , -0.08945961,  0.0355445 ,\n",
       "        0.05217047, -0.09187157,  0.0095493 , -0.16591462,  0.02874937,\n",
       "       -0.0271637 , -0.1071742 , -0.07731676,  0.17057821, -0.08997511,\n",
       "       -0.15251309,  0.0335523 ,  0.08237864,  0.05653743,  0.05472478,\n",
       "       -0.16204691,  0.09632239, -0.0988512 , -0.12040844, -0.13067709,\n",
       "       -0.03478862, -0.07479616,  0.07388658,  0.08031294, -0.12074082,\n",
       "       -0.04398282, -0.04163745,  0.09059986,  0.1189674 ,  0.05980834,\n",
       "       -0.03435113, -0.12926504,  0.06568849, -0.04446449, -0.093913  ,\n",
       "       -0.02966127, -0.00469951, -0.15404062,  0.07520716, -0.02498432,\n",
       "        0.12759562, -0.08096215, -0.07254944,  0.04480873, -0.19941515,\n",
       "       -0.0785991 ,  0.00944981, -0.00052774,  0.031948  ,  0.05943104,\n",
       "       -0.03682529,  0.02468612, -0.00828248,  0.08574598, -0.08077673,\n",
       "        0.09135576,  0.03673941,  0.01044743])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03359349, -0.01812873, -0.00173358,  0.01964512,  0.10055595,\n",
       "        0.02292307,  0.0607472 ,  0.04138128, -0.12003173, -0.09744611,\n",
       "        0.07093195, -0.01193107,  0.07130294,  0.08676811,  0.11344255,\n",
       "        0.05344105, -0.02284033,  0.05260049, -0.01932774, -0.09711266,\n",
       "       -0.19951601,  0.00738851, -0.07712144, -0.07025649, -0.03221126,\n",
       "       -0.13667656,  0.08364721, -0.04125704, -0.03211425,  0.06983157,\n",
       "       -0.00799658, -0.01177289, -0.16706869,  0.07463736, -0.09091619,\n",
       "       -0.11201481,  0.08337475,  0.05930549, -0.07940312, -0.05631904,\n",
       "       -0.01991395, -0.02349721,  0.1824768 ,  0.01132163, -0.04610476,\n",
       "        0.01006832,  0.04507252, -0.08544502,  0.02825145,  0.02216966,\n",
       "        0.03565323, -0.06984831,  0.03623525,  0.1129318 , -0.09319748,\n",
       "       -0.04915803,  0.11094059,  0.0029751 , -0.08399934,  0.21738739,\n",
       "        0.06743176, -0.09108514,  0.01798326, -0.08503798,  0.0290214 ,\n",
       "        0.07596923, -0.02369601,  0.11986063, -0.04430954, -0.07094733,\n",
       "        0.01175736,  0.01439442, -0.04382178, -0.00226002,  0.21220601,\n",
       "       -0.06764342,  0.07890032,  0.05381876,  0.00717588,  0.07492647,\n",
       "        0.03855799,  0.0598171 ,  0.15265729, -0.07431357, -0.01416465,\n",
       "        0.01648333, -0.12150514,  0.03880982,  0.19457227, -0.07679147,\n",
       "        0.07640341, -0.11388411, -0.06228767, -0.19465441,  0.15413325,\n",
       "        0.15040557,  0.12038716, -0.00415648,  0.08566573,  0.00507924,\n",
       "        0.02487304,  0.1539935 , -0.11243668,  0.14525574,  0.16143647,\n",
       "        0.15734923, -0.06363664, -0.1864682 , -0.01665902,  0.03511102,\n",
       "        0.1189222 , -0.02687126, -0.02499661, -0.04100073,  0.0366022 ,\n",
       "       -0.10856557,  0.00770556,  0.1559227 , -0.02330417,  0.04375458,\n",
       "       -0.13448504,  0.01144326, -0.03192285, -0.04401572,  0.12403052,\n",
       "        0.07985658,  0.04823248, -0.1284066 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
